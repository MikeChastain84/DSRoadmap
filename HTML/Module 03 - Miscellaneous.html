<h2><a id="_39zg2z9t4icf"></a><strong>More Notes on Probability</strong></h2><p>A FEW YEARS AGO a man won the Spanish national lottery with a ticket that ended in the number 48. Proud of his “accomplishment,” he revealed the theory that brought him the riches. “I dreamed of the number 7 for seven straight nights,” he said, “and 7 times 7 is 48.” Those of us with a better command of our multiplication tables might chuckle at the man’s error, but we all create our own view of the world and then employ it to filter and process our perceptions, extracting meaning from the ocean of data that washes over us in daily life. And we often make errors that, though less obvious, are just as significant as his.</p><p>A Drunkard's Walk Prologue Paragraph 1</p><p><strong>I. Introduction</strong></p><ul><li><strong>A man's lottery win and his faulty logic highlight how people can make significant errors in their understanding of probability.</strong></li><li><strong>The chapter introduces the concept of probability, emphasizing its role in decision-making under uncertainty.</strong></li></ul><p><strong>II. Notes</strong></p><ul><li><strong>Deterministic programs focus on definite inputs and outputs, while probabilistic programs deal with decisions under uncertainty.</strong></li><li><strong>Random variables represent the uncertain outcomes of random events, and their probabilities can be measured.</strong></li><li><strong>In finance, risk involves an unknown outcome with a known probability distribution, while uncertainty involves both an unknown outcome and an unknown probability distribution.</strong></li><li><strong>Variance measures the dispersion of a random variable, indicating how far a set of numbers is spread out from their average value.</strong></li></ul><p><strong>III. Data Does Not Speak for Itself</strong></p><ul><li><strong>Numbers need to be interpreted and given meaning, as they don't have inherent meaning themselves.</strong></li></ul><p><strong>IV. Probability</strong></p><ul><li><strong>The probability of an event is measured on a scale from 0 to 1, with 0 indicating that the event will not happen and 1 indicating that the event will always happen.</strong></li><li><strong>It is calculated as the number of favorable outcomes divided by the total number of possible outcomes.</strong></li><li><strong>The terms &quot;events&quot; and &quot;sample space&quot; are important in probability.</strong></li><li><strong>Combinations, permutations, and factorials are used to calculate probabilities in different situations.</strong></li><li><strong>Intersections, unions, and conditional probability are also important concepts in probability.</strong></li></ul><p><strong>V. Bayes Theorem and Bayes Inference</strong></p><ul><li><strong>Bayes' Theorem is a way to update the probability of an event based on new evidence.</strong></li><li><strong>Bayesian inference is a method of statistical inference that uses Bayes' Theorem to update the probability of a hypothesis as more evidence or information becomes available.</strong></li></ul><p><strong>VI. Types of Probability</strong></p><ul><li><strong>There are several types of probability, including classical probability, enumerative probability, long-run frequency probability, propensity or chance probability, and subjective probability.</strong></li></ul><p><strong>VII. Rolling One Die</strong></p><ul><li><strong>The example of rolling a die is used to illustrate the difference between frequentist and Bayesian approaches to probability.</strong></li></ul><p><strong>VIII. Rolling Two Dice</strong></p><ul><li><strong>The probability distribution for the sum of two dice is shown.</strong></li></ul><p><strong>IX. Gambling</strong></p><ul><li><strong>The history of probability is intertwined with the history of gambling.</strong></li><li><strong>The ancient Greeks and Romans had gambling games, but they didn't develop a systematic understanding of probability.</strong></li><li><strong>The development of probability theory began in the 16th century, with the work of mathematicians such as Gerolamo Cardano and Galileo Galilei.</strong></li><li><strong>Blaise Pascal and Pierre de Fermat made significant contributions to probability theory in the 17th century.</strong></li></ul><p><strong>X. Fraternal Twins</strong></p><ul><li><strong>The example of fraternal twins is used to illustrate the concept of conditional probability.</strong></li></ul><p><strong>XI. Probability Foundations</strong></p><ul><li><strong>The basic rules of probability are explained, including the addition rule and the multiplication rule.</strong></li></ul><p><strong>The addition rule and the multiplication rule are fundamental concepts in probability that help calculate the probability of complex events.</strong></p><p><strong>Addition Rule</strong></p><ul><li><strong>Used to calculate the probability of either one event or another occurring.</strong></li><li><strong>For mutually exclusive events (events that cannot occur at the same time), the probability of either event occurring is the sum of their individual probabilities.</strong></li><li><strong>For non-mutually exclusive events (events that can occur at the same time), the probability of either event occurring is the sum of their individual probabilities minus the probability of both events occurring together.</strong></li></ul><p><strong>Example:</strong></p><p><strong>What is the probability of rolling a 1 or a 6 on a standard six-sided die?</strong></p><ul><li><strong>Since these events are mutually exclusive, we can use the addition rule for mutually exclusive events.</strong></li><li><strong>The probability of rolling a 1 is 1/6.</strong></li><li><strong>The probability of rolling a 6 is 1/6.</strong></li><li><strong>Therefore, the probability of rolling a 1 or a 6 is 1/6 + 1/6 = 1/3.</strong></li></ul><p><strong>Multiplication Rule</strong></p><ul><li><strong>Used to calculate the probability of two events occurring together.</strong></li><li><strong>For independent events (events where the occurrence of one does not affect the occurrence of the other), the probability of both events occurring is the product of their individual probabilities.</strong></li><li><strong>For dependent events (events where the occurrence of one affects the occurrence of the other), the probability of both events occurring is the product of the probability of the first event and the conditional probability of the second event given that the first event has occurred.</strong></li></ul><p><strong>Example:</strong></p><p><strong>What is the probability of drawing two aces in a row from a standard deck of 52 cards, if the first card is not replaced?</strong></p><ul><li><strong>Since the events are dependent (the first card not being replaced affects the probability of drawing an ace on the second draw), we use the multiplication rule for dependent events.</strong></li><li><strong>The probability of drawing an ace on the first draw is 4/52 (there are 4 aces in a deck of 52 cards).</strong></li><li><strong>The probability of drawing another ace on the second draw, given that an ace was drawn on the first and not replaced, is 3/51 (there are 3 aces left in a deck of 51 cards).</strong></li><li><strong>Therefore, the probability of drawing two aces in a row is (4/52) * (3/51) = 1/221.</strong></li></ul><p><strong>XII. Adding or Multiplying Probabilities</strong></p><ul><li><strong>See the definitions above</strong></li></ul><p><strong>XIII. The Conjunction Fallacy</strong></p><ul><li><strong>The conjunction fallacy is a common error in decision-making where people judge that a conjunction of two possible events is more likely than one or both of the conjuncts.</strong></li></ul><p><strong>XIV. The Prosecutor's Fallacy</strong></p><ul><li><strong>The prosecutor's fallacy is a common error in legal reasoning that involves misinterpreting statistical evidence.</strong></li><li><strong>The Prosecutor's Fallacy is a common error in legal reasoning that involves misinterpreting conditional probabilities. It occurs when the probability of evidence being present given innocence is confused with the probability of innocence given the presence of that evidence.</strong></li><li><strong>In simpler terms: It's mistakenly thinking that just because a piece of evidence is more likely to be found if someone is guilty, finding that evidence automatically means they are highly likely to be guilty.</strong></li><li><strong>Example:</strong></li><li><strong>Imagine a rare blood type is found at a crime scene, and this blood type is present in only 1% of the population. The prosecutor might argue that since the defendant has this rare blood type, there's a 99% chance they are guilty. This is the Prosecutor's Fallacy.</strong></li><li><strong>Why is it a fallacy?</strong></li><li><strong>It ignores the prior probability of the defendant being guilty <em>before</em> the blood type evidence was considered. If there was little other evidence linking the defendant to the crime, the prior probability of guilt would be low. The blood type evidence increases the likelihood of guilt, but it doesn't automatically jump to 99%.</strong></li><li><strong>Key takeaway: The Prosecutor's Fallacy highlights the importance of considering all relevant evidence and probabilities, including prior probabilities, when assessing guilt or innocence.</strong></li></ul><p><strong>XV. O.J. Simpson: The Other Side of Probability</strong></p><ul><li><strong>The O.J. Simpson trial is used to illustrate the prosecutor's fallacy.</strong></li><li><strong>Most abusers to do not kill their spouse</strong></li><li><strong>Spouses that are killed by the other spouse are usually abusers</strong></li></ul><p><strong>XVI. Monty Hall Problem</strong></p><ul><li><strong>The Monty Hall Problem is a classic brain teaser that illustrates how our intuition about probability can be wrong.</strong></li></ul><p><strong>XVII. Formulas</strong></p><ul><li><strong>Formulas for factorials, permutations, and combinations are provided.</strong></li></ul><p><strong>XVIII. Intersections</strong></p><ul><li><strong>The concept of intersections in probability is explained.</strong></li></ul><p><strong>XIX. Unions</strong></p><ul><li><strong>The concept of unions in probability is explained.</strong></li></ul><p><strong>XX. Compliment</strong></p><ul><li><strong>The concept of complements in probability is explained.</strong></li></ul><p><strong>XXI. Conditional Probability</strong></p><ul><li><strong>The concept of conditional probability is explained.</strong></li></ul><p><strong>XXII. Bayes' Theorem</strong></p><ul><li><strong>Bayes' Theorem is a way to update the probability of an event based on new evidence.</strong></li></ul><p><strong>XXIII. Example Problems</strong></p><ul><li><strong>Several example problems are provided to illustrate the application of Bayes' Theorem.</strong></li></ul><p><strong>Frequentists vs Bayesians</strong></p><p><strong>1. Interpretation of Probability</strong></p><ul><li><strong>Frequentists:</strong> View probability as the long-run frequency of an event occurring. They estimate probabilities based on repeated observations or experiments.</li><li><strong>Bayesians:</strong> View probability as a degree of belief or confidence in an event occurring. They incorporate prior knowledge and update their beliefs based on new evidence using Bayes' theorem.</li></ul><p><strong>2. Use of Prior Information</strong></p><ul><li><strong>Frequentists:</strong> Generally avoid using prior information or subjective beliefs in their analysis. They rely solely on the observed data to draw conclusions.</li><li><strong>Bayesians:</strong> Explicitly incorporate prior information into their analysis. They use Bayes' theorem to combine prior beliefs with observed data to update their understanding of an event's probability.</li></ul><p><strong>3. Statistical Inference</strong></p><ul><li><strong>Frequentists:</strong> Use hypothesis testing and confidence intervals to draw inferences about population parameters. They focus on rejecting or failing to reject a null hypothesis based on the observed data.</li><li><strong>Bayesians:</strong> Use posterior distributions and credible intervals to draw inferences about population parameters. They update their prior beliefs based on the observed data to obtain a posterior distribution that reflects their updated knowledge.</li></ul><p><strong>Examples</strong></p><p>Let's consider a scenario where you're trying to determine the probability of a coin landing heads up.</p><ul><li><strong>Frequentist approach:</strong> You would flip the coin a large number of times (e.g., 1000) and count the number of times it lands heads up. The proportion of heads in your experiment would be your estimate of the probability of the coin landing heads up.</li><li><strong>Bayesian approach:</strong> You might start with a prior belief that the coin is fair (50% chance of heads). Then, you would flip the coin a few times and update your belief based on the observed outcomes. If you get a few heads in a row, your posterior probability of the coin being biased towards heads would increase.</li></ul><p><strong>In Summary</strong></p><table><thead><tr><th><p>Approach</p></th><th><p>Probability Interpretation</p></th><th><p>Use of Prior Information</p></th><th><p>Statistical Inference</p></th></tr><tr><th><p>Frequentist</p></th><th><p>Long-run frequency</p></th><th><p>Avoided</p></th><th><p>Hypothesis testing, confidence intervals</p></th></tr><tr><th><p>Bayesian</p></th><th><p>Degree of belief</p></th><th><p>Incorporated</p></th><th><p>Posterior distributions, credible intervals</p></th></tr></thead></table>