<h3><a id="_xav3hwwtulf4"></a><strong>Terms and Concepts</strong></h3><ol><li><strong>Bessel's Correction<br /></strong><ul><li>Bessel's correction is the use of nâˆ’1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample.</li><li>This method corrects the bias in the estimation of the population variance.</li><li>It also partially corrects the bias in the estimation of the population standard deviation.</li></ul></li><li><strong>Bias<br /></strong><ul><li>Bias is a systematic error that can occur in a statistical analysis, leading to inaccurate or misleading results.</li><li>It can be caused by various factors, such as sampling bias, measurement bias, or human bias.</li></ul></li><li><strong>Degrees of Freedom<br /></strong><ul><li>Degrees of freedom are the number of independent values that a statistical analysis can estimate.</li><li>You can also think of it as the number of values that are free to vary as you estimate parameters.</li><li>Degrees of freedom is a combination of how much data you have and how many parameters you need to estimate.</li></ul></li><li><strong>Standard Deviation<br /></strong><ul><li>The standard deviation is a measure of the spread or dispersion of data points around the mean.</li><li>It is calculated as the square root of the variance.</li><li>A higher standard deviation indicates that the data is more spread out, while a lower standard deviation indicates that the data is more clustered around the mean.</li></ul></li><li><strong>Boosting<br /></strong><ul><li>In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.</li><li>Boosting is based on the question posed by Kearns and Valiant (1988, 1989): &quot;Can a set of weak learners create a single strong learner?&quot;.</li><li>A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing).</li></ul></li><li><strong>Bootstrapping<br /></strong><ul><li>Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples.</li><li>This process allows you to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics.</li><li>These resamples are known as sampling distributions of estimates.</li></ul></li><li><strong>Central Limit Theorem<br /></strong><ul><li>The central limit theorem (CLT) is a statistical concept that states that the distribution of sample means approximates a normal distribution as the sample size gets larger, regardless of the population's distribution.</li><li>Sample means will be normally distributed, even if the population isn't normally distributed.</li><li>The central limit theorem is important in hypothesis testing and confidence intervals.</li></ul></li><li><strong>Law of Large Numbers<br /></strong><ul><li>In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times.</li><li>According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.</li><li>Guarantees stable long-term results for the averages of some random events.</li></ul></li><li><strong>Distribution of Sample Means</strong><ul><li>Imagine taking many random samples from a population and calculating the mean of each sample. The distribution of these sample means is called the &quot;distribution of sample means.&quot;</li><li>Crucially, even if the original population isn't normally distributed, the distribution of sample means tends towards a normal distribution as the sample size increases. This is the foundation of the Central Limit Theorem.</li><li>This distribution has its own mean (close to the population mean) and standard deviation (called the standard error).</li></ul></li><li><strong>Confidence Intervals</strong><ul><li>A confidence interval is a range of values around a sample statistic (like the mean) that is likely to contain the true population parameter with a certain level of confidence (e.g., 95%).</li><li>It essentially gives you a plausible range for where the true population value lies, based on your sample data.</li><li>Confidence intervals are affected by sample size and variability. Larger samples and lower variability lead to narrower confidence intervals, indicating more precise estimates.</li></ul></li><li><strong>Uncertainty</strong><ul><li>In the statistical sense, uncertainty refers to the lack of complete knowledge about a population or phenomenon. We use samples to estimate population characteristics, but there's always some level of uncertainty because we haven't measured the entire population.</li><li>Uncertainty can be quantified using concepts like confidence intervals, standard errors, and margins of error.</li></ul></li><li><strong>Margin of Error</strong><ul><li>The margin of error is the range of values above and below the sample statistic in a confidence interval.</li><li>It essentially tells you how much your sample estimate might differ from the true population value due to random sampling error.</li><li>A larger margin of error indicates greater uncertainty in the estimate.</li></ul></li><li><strong>Standard Error</strong><ul><li>The standard error is the standard deviation of the distribution of sample means.</li><li>It measures how much the sample means vary from the true population mean.</li><li>A smaller standard error indicates that the sample means are clustered more closely around the population mean, suggesting a more precise estimate.</li></ul></li><li><strong>Relationships and Comparisons</strong></li></ol><ul><li>The distribution of sample means is the foundation for understanding confidence intervals and standard errors.</li><li>Confidence intervals are constructed using the margin of error, which is calculated using the standard error.</li><li>All these concepts help quantify and manage uncertainty in statistical analysis.</li><li>By understanding these concepts and their relationships, you can better interpret statistical results, estimate population parameters, and make informed decisions based on data analysis.</li></ul><ol><li><strong>Natural Language Processing (NLP)</strong><ul><li>Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language.</li><li>It involves developing algorithms and models that can analyze text and speech, extract meaning, and perform tasks such as translation, summarization, and question answering.</li><li>NLP is used in various applications, including chatbots, machine translation, sentiment analysis, and text classification.</li></ul></li><li><strong>One-Hot Encoding</strong><ul><li>In machine learning, one-hot encoding is a frequently used method to deal with categorical data.</li><li>Because many machine learning models need their input variables to be numeric, categorical variables need to be transformed in the pre-processing part.</li><li>One-hot encoding creates new binary columns for each unique value in a categorical variable, indicating the presence or absence of that value for each observation. </li></ul></li></ol><p>Natural Language Processing (NLP) is a branch of artificial intelligence that deals with enabling computers to understand, interpret, and generate human language.</p><ol><li><strong>Word Tokens</strong><ul><li>Tokens are the total number of words in a corpus regardless of whether they are repeated.</li><li>Word tokenization splits text into words.</li><li>Tokens are the basic building blocks of natural language understanding.</li></ul></li><li><strong>CountVectorizer</strong><ul><li>CountVectorizer is a tool used in NLP to convert a collection of text documents into numerical feature vectors.</li><li>It works by counting the occurrences of each word in each document and creating a matrix representation of the data.</li><li>This matrix can then be used as input for machine learning algorithms.</li></ul></li><li><strong>Bag of Words</strong><ul><li>The bag-of-words model is a simplifying representation used in NLP and information retrieval (IR).</li><li>In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.</li><li>Keeping multiplicity means that the number of times a word appears in the text is recorded.</li></ul></li><li><strong>Stemming</strong><ul><li>Stemming is the process of reducing a word to its base or root form, called the stem.</li><li>This is done by removing suffixes and prefixes from the word.</li><li>Stemming is used in NLP to normalize words and reduce the number of unique words in a corpus.</li></ul></li><li><strong>Lemmatization</strong><ul><li>Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item.</li><li>This is done by using a dictionary or a set of rules to identify the base form of a word, called the lemma.</li><li>Lemmatization is used in NLP to improve the accuracy of text analysis.</li></ul></li><li><strong>TF-IDF (Term Frequency Inverse Document Frequency)</strong><ul><li>TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus.</li><li>It is used to score words in the context of the document as well as in the context of the corpus.</li><li>The higher the TF-IDF score, the more useful the word is in the analysis of the document.</li></ul></li><li><strong>Stop Words</strong><ul><li>Stop words are commonly used words (such as &quot;the&quot;, &quot;a&quot;, &quot;is&quot;, &quot;to&quot;) that are often removed from text during NLP tasks.</li><li>This is because stop words typically do not carry much meaning and can clutter the analysis.</li><li>Removing stop words can improve the efficiency and accuracy of NLP tasks.</li></ul></li><li><strong>spaCy</strong><ul><li>spaCy is an open-source library for advanced NLP tasks.</li><li>It provides tools for various NLP tasks, such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.</li><li>It is designed to be fast, efficient, and easy to use.</li></ul></li><li><strong>Language Models</strong><ul><li>A language model is a probability distribution over sequences of words.</li><li>It assigns a probability to the whole sequence.</li><li>Language models are used in various NLP tasks, such as machine translation, speech recognition, and text generation.</li></ul></li><li><strong>N-grams</strong><ul><li>N-grams are contiguous sequences of n items from a given sample of text or speech.</li><li>They are used to model the probability of a sequence of words, which is useful in various NLP tasks.</li><li>Examples of n-grams include unigrams (single words), bigrams (two words), and trigrams (three words).</li></ul></li><li><strong>Markov Chains</strong><ul><li>A Markov chain is a mathematical system that undergoes transitions from one state to another, between a finite or countable number of possible states.</li><li>It is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</li><li>Markov chains are used in NLP to model the probability of a sequence of words, which is useful in various NLP tasks.</li></ul></li><li><strong>Word Embeddings</strong><ul><li>In natural language processing (NLP), a word embedding is a learned representation for text where words that have the same meaning have a similar representation.</li><li>Word embeddings are used to capture the semantic relationships between words.</li><li>They are used in various NLP tasks, such as machine translation, sentiment analysis, and text classification.</li></ul></li><li><strong>Similarity Measures</strong><ul><li>Similarity measures are used to quantify the similarity between two pieces of text.</li><li>There are various similarity measures, such as cosine similarity, Jaccard distance, and Euclidean distance.</li><li>Similarity measures are used in NLP tasks such as information retrieval, document clustering, and text classification.</li></ul></li><li><strong>Part of Speech (POS)</strong><ul><li>Part-of-speech (POS) tagging is the process of assigning a part-of-speech tag to each word in a text.</li><li>POS tags indicate the grammatical role of a word in a sentence, such as noun, verb, adjective, or adverb.</li><li>POS tagging is used in various NLP tasks, such as parsing, machine translation, and information retrieval.</li></ul></li><li><strong>Named Entity Recognition (NER)</strong><ul><li>Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.</li><li>NER is used in various NLP applications, such as information retrieval, question answering, and machine translation.</li></ul></li><li><strong>Sentence Segmentation</strong><ul><li>Sentence segmentation, also known as sentence boundary disambiguation, is the problem in natural language processing of deciding where sentences begin and end.</li><li>Sentence segmentation is used to divide a text into individual sentences, which is a necessary step for many NLP tasks.</li></ul></li><li><strong>Topic Modeling</strong><ul><li>Topic modeling is a statistical method for discovering abstract &quot;topics&quot; that occur in a collection of documents.</li><li>It is used to uncover hidden themes or topics in a corpus of text data.</li><li>Topic modeling is used in various NLP applications, such as document summarization, information retrieval, and text classification.</li></ul></li><li><strong>Latent Dirichlet Allocation (LDA)</strong><ul><li>Latent Dirichlet Allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.</li></ul></li><li><strong>Non-Negative Matrix Factorization</strong><ul><li>Non-Negative Matrix Factorization is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence.</li><li><a href="https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/">https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/</a> </li><li>Performs dimensionality reduction and clustering</li><li>Used with TF-IDF</li></ul></li><li><strong>Similarity Measures</strong><ul><li><a href="https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55">https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55</a></li><li>Longest Common Substring<ol><li>India and Indiana would return 5</li></ol></li><li>Levenshtein Edit Distance<ol><li>Finds the minimum number of single-character edits such as replacement, deletion, and insertion, needed to convert 1 text into another</li><li>India and Indiana would return 2</li></ol></li><li>Hamming Distance<ol><li>Finds the number replacements needed to change one text into another of equal size</li><li>Indians and Indiana returns 2</li></ol></li><li>Jaccard Distance<ol><li>Finds how disimilar two words are by distance and the lower the distance, the more similar</li></ol></li><li>Euclidean Distance<ol><li>Finds the length between two points</li></ol></li><li>l2 norm</li><li>Dot Product<ol><li>Considers orientation, the direction, that Euclidean lacks</li><li>Uses magnitude with orientation</li></ol></li><li>Cosine Similarity<ol><li>In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval -1, 1. For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. For example, in information retrieval and text mining, each word is assigned a different coordinate and a document is represented by the vector of the numbers of occurrences of each word in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be, in terms of their subject matter, and independently of the length of the documents. The technique is also used to measure cohesion within clusters in the field of data mining.</li><li>https://en.wikipedia.org/wiki/Cosine_similarity</li></ol></li></ul></li></ol>