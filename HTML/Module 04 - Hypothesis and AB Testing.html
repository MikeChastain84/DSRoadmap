<p>Here are brief definitions and explanations for the terms and concepts presented in the document:</p><p><strong>Hypothesis Testing</strong></p><ul><li><strong>A/B testing</strong> is a randomized experimentation process wherein two or more versions of a variable are shown to different segments of website visitors at the same time to determine which version drives business metrics best.</li><li><strong>Population</strong> is the entire group of individuals or objects that a study is interested in.</li><li><strong>Sample</strong> is a subset of the population that is selected for study.</li><li><strong>Sample mean</strong> is the average of the values in a sample.</li><li><strong>Sample variability</strong> is the degree to which the values in a sample differ from each other.</li><li><strong>Null hypothesis</strong> is a statement that there is no difference between two groups or that there is no relationship between two variables.</li><li><strong>Confidence interval</strong> is a range of values that is likely to contain the true population parameter.</li><li><strong>Type I error</strong> is the rejection of a true null hypothesis.</li><li><strong>Type II error</strong> is the failure to reject a false null hypothesis.</li><li><strong>P-value</strong> is the probability of obtaining a test statistic as extreme as or more extreme than the one observed, assuming that the null hypothesis is true.</li><li><strong>Statistical significance</strong> is a measure of how likely it is that an observed effect is due to chance.</li><li><strong>Statistical power</strong> is the probability that a test will correctly reject a false null hypothesis.</li><li><strong>Minimum detectable effect</strong> is the smallest effect size that a study is designed to detect.</li><li><strong>Practical significance</strong> is a measure of whether an effect is large enough to be meaningful in the real world.</li></ul><p><strong>Probability Density Function (PDF)</strong></p><ul><li>The PDF is a function that describes the probability of a random variable taking on a given value.</li></ul><p><strong>Cumulative Density Function (CDF)</strong></p><ul><li>The CDF is a function that describes the probability that a random variable will take on a value less than or equal to a given value.</li></ul><p><strong>Percent Point Function (PPF)</strong></p><ul><li>The PPF is the inverse of the CDF. It gives the value of the random variable for which the CDF has a given value.</li></ul><p><strong>Parametric and Non-Parametric Tests</strong></p><ul><li><strong>Parametric tests</strong> are statistical tests that make assumptions about the distribution of the data.</li><li><strong>Non-parametric tests</strong> are statistical tests that do not make assumptions about the distribution of the data.</li></ul><p><strong>t-Test</strong></p><ul><li><strong>Student's t-test</strong> is a statistical test used to determine if there is a significant difference between the means of two groups.</li><li><strong>One-sample t-test</strong> is used to test whether the mean of a single sample is equal to a specified value.</li><li><strong>Independent two-sample t-test</strong> is used to test whether the means of two independent samples are equal.</li></ul><p><strong>z-Test</strong></p><ul><li>A statistical test used to determine if there is a significant difference between the means of two groups when the variances are known.</li></ul><p><strong>Chi-Square Test</strong></p><ul><li>A statistical test used to determine if there is a significant association between two categorical variables.</li></ul><p><strong>ANOVA (Analysis of Variance)</strong></p><ul><li>A statistical test used to compare the means of two or more groups.</li></ul><p><strong>Kruskal-Wallis Test</strong></p><ul><li>A non-parametric test used to compare the distributions of two or more groups.</li></ul><p><strong>A/A Testing</strong></p><ul><li>A type of A/B testing where both groups are given the same treatment to validate the test setup.</li></ul><p><strong>AB Testing</strong></p><ul><li>A statistical method used to compare two or more versions of a webpage, app, or other product to see which one performs better.</li></ul><p><strong>Overall Evaluation Criterion (OEC)</strong></p><ul><li>A quantitative measure of the experiment's objective.</li></ul><p><strong>Gaurdrail Metrics</strong></p><ul><li>Business metrics designed to indirectly measure business value and provide alerts about any potentially misleading or erroneous results and analysis.</li></ul><p><strong>Randomization Unit</strong></p><ul><li>A who or what randomly assigned to a group.</li></ul><p><strong>Data Leakage (Interference)</strong></p><ul><li>The behavior of the control group is influenced by the treatment given to the test group.</li></ul><p><strong>SUTVA Assumptions</strong></p><ul><li>The Stable Unit Treatment Value Assumption (SUTVA) is a key assumption that is usually made in causal inference.</li></ul><p><strong>Minimum Detectable Effect (MDE)</strong></p><ul><li>The smallest improvement you are willing to be able to detect.</li></ul><p><strong>Practical, or Substantive, Significance</strong></p><ul><li>The importance, or meaningfulness, of a statistically significant result.</li></ul><p><strong>Significance Level</strong></p><ul><li>The probability of rejecting a null hypothesis when it is true.</li></ul><p><strong>Statistical Power</strong></p><ul><li>The probability of correctly identifying the effect when there is one.</li></ul><p><strong>Effect Size</strong></p><ul><li>A quantitative measure of the magnitude of a phenomenon.</li></ul><p><strong>Pooled Variance</strong></p><ul><li>A method for estimating the variance of a population when the variances of different samples from that population are assumed to be equal.</li></ul><p><strong>Conversion Rates</strong></p><ul><li>The percentage of users who take a desired action.</li></ul><p><strong>Lift</strong></p><ul><li>The percentage change in a metric between a test group and a control group.</li></ul><p><strong>Discrete Metrics</strong></p><ul><li>Metrics that can only take on a finite number of values.</li></ul><p><strong>Continuous Metrics</strong></p><ul><li>Metrics that can take on any value within a given range. </li></ul><p>You're absolutely correct! I apologize for missing those definitions in my previous response. Here are the definitions and explanations for the terms and concepts you pointed out:</p><p><strong>Kolmogorov-Smirnov Test for Normality</strong></p><ul><li>A statistical test used to determine if a dataset follows a normal distribution.</li></ul><p><strong>Chi-Square Test for Independence</strong></p><ul><li>A statistical test used to determine if there is a significant association between two categorical variables.</li></ul><p><strong>Sample Ratio Mismatch (SRM)</strong></p><ul><li>A phenomenon that can occur in A/B testing when the ratio of users in the control group to users in the treatment group is not what was intended.</li></ul><p><strong>Fisher's Exact Test</strong></p><ul><li>A statistical test used to determine if there is a significant association between two categorical variables when the sample size is small.</li></ul><p><strong>Welch's t-test</strong></p><ul><li>A statistical test used to compare the means of two groups when the variances of the two groups are not assumed to be equal.</li></ul><p><strong>OOP (Object-Oriented Programming)</strong></p><ul><li>A programming paradigm that is based on the concept of objects, which can contain data and code.</li></ul><p><strong>Python Builtins</strong></p><ul><li>Functions and constants that are available in Python without the need to import any modules. </li></ul>