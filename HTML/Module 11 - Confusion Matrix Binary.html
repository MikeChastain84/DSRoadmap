<ol><li><strong>Confusion Matrix</strong>: A table used to evaluate the performance of a classification model by comparing predicted class labels to actual class labels. It provides insights into the types of errors made by the model.<br /></li><li><strong>Error Matrix</strong>: Another name for a confusion matrix.<br /></li><li><strong>Contingency Table</strong>: A table that displays the frequencies of different combinations of two categorical variables. In the case of a confusion matrix, the variables are the actual and predicted classes.<br /></li><li><strong>True Positive (TP)</strong>: An instance where the model correctly predicts the positive class.<br /></li><li><strong>False Positive (FP)</strong>: An instance where the model incorrectly predicts the positive class when the actual class is negative.<br /></li><li><strong>False Negative (FN)</strong>: An instance where the model incorrectly predicts the negative class when the actual class is positive.<br /></li><li><strong>True Negative (TN)</strong>: An instance where the model correctly predicts the negative class.<br /></li><li><strong>Accuracy</strong>: The proportion of correctly classified instances out of the total instances. It's calculated as (TP + TN) / (TP + FP + TN + FN).<br /></li><li><strong>Precision</strong>: The proportion of true positive predictions out of all positive predictions. It's calculated as TP / (TP + FP).<br /></li><li><strong>Recall</strong>: The proportion of true positive predictions out of all actual positive instances. It's calculated as TP / (TP + FN). Also known as sensitivity.<br /></li><li><strong>Type I Error</strong>: Incorrectly rejecting the null hypothesis when it is actually true. Equivalent to a false positive.<br /></li><li><strong>Type II Error</strong>: Incorrectly failing to reject the null hypothesis when it is actually false. Equivalent to a false negative.<br /></li><li><strong>Underfitting</strong>: Occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data.<br /></li><li><strong>Overfitting</strong>: Occurs when a model is too complex and learns the training data too well, including noise. This leads to good performance on training data but poor generalization to new, unseen data.<br /></li><li><strong>Bias-Variance Tradeoff</strong>: The inherent tradeoff in machine learning between bias (error due to oversimplification) and variance (error due to overcomplexity). The goal is to find the right balance for optimal model generalization.<br /></li><li><strong>Precision-Recall Tradeoff</strong>: The tradeoff between precision and recall. Increasing precision often decreases recall, and vice versa. The optimal balance depends on the specific application and the relative costs of false positives and false negatives.<br /></li><li><strong>F1 Score</strong>: The harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.<br /></li><li><strong>ROC Curve</strong>: A graphical representation of the tradeoff between true positive rate (sensitivity) and false positive rate (1-specificity) for different classification thresholds.<br /></li><li><strong>AUC (Area Under the ROC Curve)</strong>: A measure of the overall performance of a classification model. A higher AUC indicates better discrimination ability.<br /></li></ol>