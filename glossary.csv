term,module,definition
** torchaudio,Module 12 - Gradient Descent and Backpropagation.docx,"** A library for audio processing, offering tools for loading, transforming, and analyzing audio data."
** torchtext,Module 12 - Gradient Descent and Backpropagation.docx,"** A library for natural language processing, providing tools for text preprocessing, tokenization, and building NLP models."
** torchvision,Module 12 - Gradient Descent and Backpropagation.docx,"** A dedicated library for computer vision tasks, providing datasets, transformations, and pre-trained models."
95% confidence interval,Module 08 - Coefficients and Metrics.docx,the lower and upper values
"A logistic function is often used in logistic regression to model how the probability of an event may be affected by one or more explanatory variables. An example would be to have the model p=f(a+bx), where x is the explanatory variable, a and b are model parameters to be fitted, and f is the standard logistic function",Module 08 - Logistic Regression.docx,p=1+e−(β0​+β1​x)1​
ACF,Module 13 - Statsmodels Time Series Complete.docx,"ACF stands for Autocorrelation Function. It's a tool used in time series analysis to measure the correlation between a time series and lagged versions of itself. In simpler terms, it helps you understand how a data point at a particular time is related to data points that came before it."
AIC,Module 08 - Coefficients and Metrics.docx,"Akaike Information Criterion is used to compare models, a lower score is better (doesn't address features, just the overall model)"
AIC (Akaike Information Criterion),Module 10 - GLM vs OLS.docx,An estimator of prediction error for a statistical model. It's used to compare different models and select the one that is most likely to make good predictions.
ANN,Module 12 - ANN.docx,"Weights Weights are values that determine the strength of the connections between neurons in the network. During training, weights are adjusted to map input features to the desired output. Biases Each neuron has a bias, which is an offset that allows the neuron to activate even when the weighted sum of its inputs is zero. Biases provide flexibility, enhance representation capabilities, and improve learning efficiency. Learning Rate The learning rate controls how much the model's parameters (weights and biases) are adjusted during each iteration of the training process. Choosing the right learning rate is critical for successful training. Forward Propagation Forward propagation is the process of passing the input data through the neural network to get the output. The output of one layer becomes the input to the next layer. Sigmoid Function The sigmoid function is a mathematical function with an ""S""-shaped curve, used as an activation function in neural networks to introduce non-linearity. The output of the sigmoid function is always between 0 and 1, making it useful for representing probabilities. Backpropagation Backpropagation is the algorithm used to train neural networks. It calculates how much each weight and bias contributes to the overall error and updates those parameters to improve the network's accuracy. Sigmoid Derivative The sigmoid derivative plays a crucial role in the backpropagation algorithm. It tells us how much the output of the sigmoid function changes with respect to a small change in its input. Gradient Descent Update Formula The gradient descent update formula represents the gradient descent update rule. It describes how the parameters (weights and biases) are adjusted based on the gradient of the error function. Input Data (x) The input data (x) contains the features that the neural network will use to learn and make predictions. Each row in x represents a different sample or data point, and each column represents a different feature. Target Output (y) The target output (y) contains the corresponding target output or labels for each sample in x. These labels represent the desired outcome or prediction that the neural network should learn to produce. Training the Neural Network The purpose of x and y is to train the neural network to learn a mapping between the input features and the target output. Prediction After training, the neural network can be used to predict the output for new input data. CPUs, T4 GPUs, and TPU v2-8s These are different types of processors with varying strengths and weaknesses. CPUs are general-purpose processors, T4 GPUs are parallel processors well-suited for deep learning, and TPU v2-8s are custom-designed processors specifically for machine learning and AI workloads. "
ANOVA (Analysis of Variance),Module 04 - Hypothesis Testing.docx,A statistical test used to compare the means of three or more groups.
APIs (Application Programming Interfaces),Module 02 - Data.docx,Allow different software systems to communicate and exchange data.
ARIMA,Module 13 - Statsmodels Time Series Complete.docx,ARIMA stands for Autoregressive Integrated Moving Average. It's a powerful statistical model used for analyzing and forecasting time series data. The AR part of the model uses past values to predict future values. The I part deals with making the time series stationary. The MA part of the model uses past forecast errors to predict future values.
AUC (Area Under the ROC Curve),Module 11 - Confusion Matrix Binary.docx,A measure of the overall performance of a classification model. A higher AUC indicates better discrimination ability.
Abraham de Moivre (1667-1754),Module 04 - Hypothesis Testing.docx,"Developed the Doctrine of Chances, contributed to the Central Limit Theorem, and introduced the normal distribution."
Abstraction,Module 12 - Gradient Descent and Backpropagation.docx,"As the information flows through the layers, the representations become more abstract. Early layers might focus on individual words or simple patterns, while deeper layers might capture more complex relationships and long-term dependencies."
Accuracy,Module 11 - Confusion Matrix Binary.docx,The proportion of correctly classified instances out of the total instances. It's calculated as (TP + TN) / (TP + FP + TN + FN).
Accuracy,Module 11 - Precision Recall.docx,The proportion of correctly classified instances out of the total instances. It's not always the best metric for imbalanced datasets.
Act,Module 12 - Gradient Descent and Backpropagation.docx,The agent chooses an action based on its policy.
Activation,Module 12 - Gradient Descent and Backpropagation.docx,"The activation function mimics the ""firing"" of a biological neuron when the input exceeds a certain threshold."
Activation Function,Module 12 - Gradient Descent and Backpropagation.docx,"The activation function introduces non-linearity, enabling the network to learn complex relationships in the data. Introduces non-linearity and produces the final output. y = σ(z) = 1 / (1 + exp(-z)) (where σ represents the sigmoid function)"
Activation Function,Module 12 - Basic Artificial Neural Net (ANN).docx,"Applying an activation function (sigmoid in this case) to introduce non-linearity and produce the activations of the neurons in each layer. In your neural network code, you used the sigmoid function as the activation function for the neurons."
Activation Function (φ),Module 12 - Gradient Descent and Backpropagation.docx,"This is a mathematical function that introduces non-linearity into the neuron's output. It determines whether the neuron should ""fire"" (activate) based on the weighted sum and bias. Common activation functions include sigmoid, ReLU, and tanh."
Activation of Previous Layer,Module 12 - Basic Artificial Neural Net (ANN).docx,a_j^{l-1} is the activation of the j-th neuron in the previous layer (l-1). This term represents the input to the weight θ_j.
Activation within the model,Module 12 - Gradient Descent and Backpropagation.docx,"The activation functions (tanh in the RNN layers) are already applied within the model during the forward pass. When you call model(input_tensor, hidden1, hidden2), the input goes through the RNN layers and the linear layer, where the activations are applied."
Active Community,Module 12 - Gradient Descent and Backpropagation.docx,"A large and active community of users and developers contribute to PyTorch's growth, providing support, resources, and a wealth of online tutorials and examples."
Adaptability,Module 12 - Gradient Descent and Backpropagation.docx,"It adapts the learning rate for each parameter, making it suitable for problems with sparse data or parameters with varying scales."
Adaptive Learning Rates,Module 12 - Gradient Descent and Backpropagation.docx,"Adam also calculates an exponentially decaying average of past squared gradients. This is used to scale the learning rate for each parameter individually. Parameters with large, frequent updates will have their learning rates scaled down, while parameters with small, infrequent updates will have their learning rates scaled up."
Adaptive learning rates,Module 12 - Basic Artificial Neural Net (ANN).docx,Algorithms like Adam or RMSprop automatically adjust the learning rate during training based on the observed gradients.
Additive Models,Module 13 - Statsmodels Time Series Complete.docx,Used when trend is linear and/or seasonal variations are constant.
Adj R-Squared,Module 08 - Coefficients and Metrics.docx,R-squared adjustment based on number of parameters and df residuals
Adjust the RNN Input,Module 12 - Gradient Descent and Backpropagation.docx,The RNN would need to be adapted to handle sequences of words as input instead of single words.
Adjusted Closing Price,Module 13 - Pandas Time Series Complete.docx,"The closing price of a stock or asset on a particular trading day, but adjusted for any corporate actions like stock splits or dividends. This provides a more accurate representation of the asset's value over time."
Adjusted R-squared,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx",A modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of unnecessary variables that don't improve the model significantly.
Adjusting Work,Module 12 - Basic Artificial Neural Net (ANN).docx,"Based on this analysis, each team adjusts their work to reduce their contribution to the error."
Adolph Quetelet (1796-1874),Module 04 - Hypothesis Testing.docx,"Introduced the concept of the ""Average Man"" and applied statistical methods to social sciences."
Alternative Hypothesis,Module 04 - Hypothesis Testing.docx,"The hypothesis that contradicts the null hypothesis, suggesting a significant difference or effect."
Analogy,Module 12 - Gradient Descent and Backpropagation.docx,"Imagine you have three ingredients for a recipe (inputs): flour, sugar, and eggs. Each ingredient has a specific importance (weight) in the recipe. Flour might have a higher weight because you need more of it, while eggs might have a lower weight. The weighted sum is like combining the ingredients in the right proportions based on their weights. Continuing with the recipe analogy, the output is like the final dish you get after baking the ingredients. The baking process (activation function) transforms the combined ingredients into something new."
Analyze results,Module 09 - Feature Selection.docx,"Aggregate and analyze the collected outputs to estimate desired metrics, calculate probabilities, or make predictions."
Associated with,Module 12 - Basic Artificial Neural Net (ANN).docx,The first hidden layer. The second hidden layer. The output layer.
Assume a null hypothesis,Module 09 - Feature Selection.docx,"Define a null hypothesis about the population parameter (e.g., the population mean is equal to a specific value)."
Assumptions,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Linear regression models have certain assumptions that need to be met for the results to be valid. These assumptions include linearity, independence of errors, homoscedasticity, and normality of errors."
Assumptions,Module 08 - Assumptions.docx,"Linear Regression Assumptions There is a linear relationship between the dependent variable and the independent variable(s). The residuals are normally distributed. The variance of the residuals is constant over all values of the independent variable(s) (homoscedasticity). The residuals are independent. These assumptions are important because they ensure that the results of the regression analysis are valid. If the assumptions are not met, the results of the analysis may be misleading. Logistic Regression Assumptions Assumptions for Binary Logistic Regression It's important to note that logistic regression is more robust to violations of some of these assumptions compared to linear regression. However, checking for and addressing potential violations can improve the accuracy and reliability of your results. Terms General Linear Model A statistical model that assumes a linear relationship between the dependent variable and one or more independent variables. y is the dependent variable. X is the independent variable. β0​ is the intercept. β1​ is the slope. e is the error term or residual. Residuals The differences between the actual and predicted values of the dependent variable in a regression model. Normally Distributed A probability distribution that is symmetric and bell-shaped. Homoscedasticity The variance of the residuals is constant across all levels of the independent variable. Independent The residuals are not correlated with each other. Skewed A distribution that is not symmetric. Linearity A linear relationship between the dependent and independent variables. Multi-Collinearity Independent variables in a regression model are highly correlated with each other. Hypothesis Testing A statistical method used to determine whether there is enough evidence to support a claim about a population parameter. Correlation A statistical measure that describes the strength and direction of the linear relationship between two variables. Python A high-level, interpreted, general-purpose programming language. Pandas A Python library used for data manipulation and analysis. Seaborn A Python library used for data visualization. Scikit-learn (sklearn) A Python library used for machine learning. Train-test Split A technique used to split a dataset into training and testing sets. Quantiles Values that divide a distribution into equal-sized groups. Theoretical Quantiles Quantiles that are calculated from a theoretical distribution. Sample Quantiles Quantiles that are calculated from a sample of data. Effective Rank The number of independent variables in a regression model that are actually contributing to the prediction of the dependent variable. Pairplot A Seaborn function that creates a grid of scatterplots showing the relationship between all pairs of variables in a dataset. "
Assumptions,Module 08 - Assumptions.docx,The assumptions for linear regression are The basic form is y=β0​+β1​X+e where
Astronomy and Statistics,Module 06 - Astronomy and Statistics.docx,"Astronomy and Statistics  Online Interactive Maps  Antiquity were lunisolar, based on moon phase and position of the sun Julius Caesar made reforms (Julian calendar) based on an algorithm of introducing a leap day every four years but added an extra day every 128 years causing seasonal equinoxes to fall at wrong time of year Gregorian (Pope Gregory XIII 1582) is now defacto Astronomical Collections Legendre (1805 - Least Squares) New Methods for the Orbits of Comets (Least Squares) E = a + bx + cy + fz + &c., A, b, c, f, &c known coefficients; the rest are unknown and determined by E (the error) Multiply terms by coefficient of the unknowns and add it all up Cotes Rule Errors decrease with aggregation rather than increase Tobias Mayer and Euler Method of averages - combination of different observations under same equations The inequalities of the motions of Jupiter and Saturn The librations (the moon’s face varies, wobbles) of the Moon Latitude measures angular elevation above horizon Longitude based on Moon features and position with stars Created a table of equations of condition Uses the symbol plus/minus x (margin of error?) Let error be the limit of accuracy for the mean Post 1750 mathematical astronomers averaged simple measurements, combining several days of observations into a single number, as well as doing the same with equations Laplace The Mechanics of the Planets Analytical Theory of Probability Mayer’s 27 equations of conditions of moon crater observations To resolve inconsistencies he reduced the 24 equations by Sum of equations The difference of the sums Linear combinations of equations Probability density for errors - Laplace distribution (two-sided exponential distribution) Roger Boscovich Least Absolute Deviation - the combination of different observations under different conditions Legendre (Least Squares Method) The first clear and concise exposition of the method of least squares was published by Legendre in 1805. The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for the shape of the Earth. Jacob Bernoulli Ars Conjectandi - the formalization of the mathematical theory of probability Law of large numbers - the greater the number of observations the less the uncertainty in the result De Moivre - stated and proved the normal approximation to the Binomial distribution Simpson Development of quantified uncertainty and mathematical theory of inference A new problem was to combine discordant observations, if five observers record five different times for the passage of a star past a crosshair in a telescope, how are these numbers reconciled? Introduces annuity tables (insurance) Wrote a letter on the advantage of taking the mean of a number of observation (astronomical) Distribution of errors accounts for small and large errors Simpson’s Paradox - a trend that appears in multiple groups may disappear when the groups are combined Inverse Probability (Bayesian Inference, inferential statistics), probability distribution of an unobserved variable Fisher - fundamental paradox of inverse probability is the source of confusion between statistical terms  that refer to the true value to be estimated with the actual value arrived at by the estimation method, which is subject to error Choice of Means Laplace (Central Limit Theorem) Gauss The motion of planetoids are disturbed by the larger planets The Gaussian distribution came about from Laplace’s distribution of errors when sampling the mean, Gauss’ observation of measurement error and de Moivre’s attempt to approximate the binomial distribution with largeaN "
Automatic Transfers,Module 12 - Basic Artificial Neural Net (ANN).docx,"PyTorch can automatically transfer tensors between the CPU and GPU as needed, simplifying the process of utilizing GPU resources."
Autonomous Driving,Module 12 - Gradient Descent and Backpropagation.docx,Self-driving cars
Autoregression,Module 13 - Statsmodels Time Series Complete.docx,"The nutshell - Uses observations from previous time steps as input to a regression equation to predict the value at the next time step. Autoregression2 is a time series modeling technique that predicts future values based on past values of the same series. It's like saying, ""knowing what happened in the past can help us understand what might happen in the future."""
Average Man,Module 01 - Introduction.docx,"A concept introduced by Adolphe Quetelet, representing the average characteristics of a population."
Average performance,Module 09 - Feature Selection.docx,The average performance across all folds is used to estimate the model’s overall performance.
Axon,Module 12 - Gradient Descent and Backpropagation.docx,This is a long fiber that transmits the output signal to other neurons.
BIC,Module 08 - Coefficients and Metrics.docx,Bayesian Information Criterion is similar to AIC but uses a higher penalty
BIC (Bayesian Information Criterion),Module 10 - GLM vs OLS.docx,"Similar to AIC, but it places a larger penalty on models with more parameters."
Backpropagation,Module 12 - Gradient Descent and Backpropagation.docx,"Backpropagation is an algorithm specific to artificial neural networks. It involves calculating gradients of the error with respect to the weights and biases and propagating these gradients back through the network. There's no direct equivalent of this process in biological neurons. Biological neurons don't have a ""backward pass"" in the same way. The Learning Engine of Neural Networks"
Backpropagation,Module 12 - Basic Artificial Neural Net (ANN).docx,"During backpropagation, you need to calculate the gradients of the error with respect to the weights and biases. This involves calculating the gradients of the error with respect to the activations of each layer."
Backward Pass,Module 12 - Gradient Descent and Backpropagation.docx,"This is where the magic of backpropagation happens. The error is propagated back through the network, layer by layer, in reverse order."
Backward Pass,Module 12 - Basic Artificial Neural Net (ANN).docx,The optimizer calculates the gradients of the loss with respect to the parameters.
Basic Artificial Neural Net (ANN),Module 12 - Basic Artificial Neural Net (ANN).docx,"Basic Artificial Neural Net (ANN)  Initializing the Weights  In this code, self.weights1, self.weights2, and self.weights3 represent the weight matrices for the connections between different layers of the neural network. These weights play a crucial role in determining the network's behavior and learning process. self.weights1 self.weights2 self.weights3 Why are weights important? In the code, the weights are initialized with random values using np.random.randn. This random initialization is crucial for breaking symmetry and allowing the network to learn effectively.  Initialize the Biases  In this code, self.bias1, self.bias2, and self.bias3 represent the bias vectors for each layer of the neural network. These biases, along with the weights, are essential parameters that the network learns during training. self.bias1 self.bias2 self.bias3 Why are biases important? In the code, the biases are initialized with zeros using np.zeros. While zero initialization is common for biases, other initialization strategies can also be used depending on the specific network architecture and activation functions. Biases are important parameters in neural networks that provide flexibility, enhance representation capabilities, and improve learning efficiency. They act as offsets for the activation functions, allowing neurons to activate even with zero or negative input sums.  The Learning Rate   The learning rate, often denoted as α (alpha), is a crucial hyperparameter in the training of neural networks. It controls how much the model's parameters (weights and biases) are adjusted during each iteration of the optimization algorithm (e.g., gradient descent). How it Works During training, the neural network calculates the gradients of the loss function with respect to the weights and biases. These gradients indicate the direction of the steepest ascent of the loss function. To minimize the loss, we want to move the parameters in the opposite direction (steepest descent). weight = weight - learning_rate * gradient Choosing the Right Learning Rate In the Code In the provided code, the learning_rate is passed as an argument to the NeuralNetwork class and used in the backward method to scale the gradients before updating the weights and biases. This controls how much the parameters are adjusted in each iteration of the training loop. Key Takeaways The learning rate is a crucial hyperparameter that controls the step size in parameter updates during training. Choosing an appropriate learning rate is essential for successful training and convergence. Experimentation and tuning are often required to find the optimal learning rate for a specific problem and network architecture.  Forward Propagation  Forward propagation is the process of passing the input data through the neural network to get the output. It's like a chain reaction, where the output of one layer becomes the input to the next layer. First Hidden Layer Second Hidden Layer Output Layer This process continues until the final output is generated. The output represents the network's prediction based on the input data and the learned parameters (weights and biases).  The Sigmoid Function  The sigmoid function is a mathematical function that has a characteristic ""S""-shaped curve. It's often used as an activation function in neural networks due to its properties that make it well-suited for introducing non-linearity and representing probabilities. Mathematical Definition sigmoid(x) = 1 / (1 + exp(-x)) x is the input to the function. exp(-x) is the exponential of the negative input. Properties How it Works in Neural Networks In neural networks, the sigmoid function is typically used as an activation function. It takes the weighted sum of inputs and biases for a neuron and produces an output between 0 and 1. This output represents the activation level of the neuron. Advantages Limitations Alternatives Due to its limitations, other activation functions like ReLU (Rectified Linear Unit) and tanh (hyperbolic tangent) are often preferred in modern deep learning architectures. In Summary The sigmoid function is a non-linear activation function commonly used in neural networks. It squashes the input to a range between 0 and 1, making it suitable for representing probabilities and introducing non-linearity. However, it has limitations like vanishing gradients and not being zero-centered, which have led to the adoption of alternative activation functions in many cases.  Backward Propagation   Backpropagation is the heart of how neural networks learn. It's the algorithm that calculates how much each weight and bias in the network contributes to the overall error, and then updates those parameters accordingly to improve the network's accuracy. Calculate the Error Propagate the Error Backwards The error is propagated back through the network, layer by layer, in reverse order. This is done by calculating the gradients of the error with respect to the weights and biases of each layer. Calculate Gradients for the Output Layer Calculate Gradients for the Hidden Layers Similar calculations are performed for the first hidden layer. Update Weights and Biases The weights and biases of each layer are updated using the calculated gradients and the learning rate. The learning rate scales the gradients, controlling the size of the updates. Key Ideas in Backpropagation Calculating the error between the predicted output and the target output. Propagating the error back through the network, layer by layer. Calculating the gradients of the error with respect to the weights and biases. Updating the weights and biases using the gradients and the learning rate. This process allows the neural network to learn from the data and improve its accuracy over time.  ""Propagating the error backwards"" is the core idea behind backpropagation. It refers to the process of moving the error signal from the output layer back through the hidden layers to the input layer. This is done to figure out how much each weight and bias in the network contributed to the overall error. Analogy In the Code In the backward method, the lines like hidden_error2 = d_output.dot(self.weights3.T) and hidden_error1 = d_hidden2.dot(self.weights2.T) are performing this error propagation. They calculate how much of the error from the subsequent layer is attributed to the activations of the current layer. Key Takeaway Propagating the error backwards is essential for training neural networks. It allows the network to identify how each parameter contributes to the error and adjust those parameters accordingly to improve its performance. This process is what enables the network to learn from the data and make more accurate predictions.  Sigmoid Derivative  The sigmoid derivative plays a crucial role in the backpropagation algorithm used to train neural networks. It tells us how much the output of the sigmoid function changes with respect to a small change in its input. This information is essential for calculating the gradients of the error with respect to the weights and biases in the network. Mathematical Definition sigmoid_derivative(x) = sigmoid(x) * (1 - sigmoid(x)) where sigmoid(x) is the output of the sigmoid function for input x. Why is it important in backpropagation? During backpropagation, we need to calculate how much the error at the output layer is affected by the activations of the neurons in the previous layers. This involves calculating the gradients of the error with respect to the activations. Since the sigmoid function is used as the activation function in this neural network, its derivative tells us how much a small change in the activation of a neuron will affect the output of that neuron. This information is then used to calculate how much that change in activation will affect the overall error. Chain Rule The sigmoid derivative is used in conjunction with the chain rule from calculus to calculate the gradients of the error with respect to the weights and biases. The chain rule allows us to break down the calculation of the gradient into smaller, manageable steps. In the Code d_output = output_error * sigmoid_derivative(output) d_hidden2 = hidden_error2 * sigmoid_derivative(self.hidden_layer2) d_hidden1 = hidden_error1 * sigmoid_derivative(self.hidden_layer1) are using the sigmoid_derivative function to calculate the gradients of the error with respect to the activations of each layer. Key Takeaways The sigmoid derivative tells us how much the output of the sigmoid function changes with respect to its input. It's crucial for calculating gradients during backpropagation in neural networks that use the sigmoid activation function. The sigmoid derivative, along with the chain rule, allows us to determine how much each weight and bias contributes to the overall error. This information is then used to update the parameters and improve the accuracy of the neural network.  The Role of the Sigmoid Derivative Connecting to the gradient descent update forumula In Summary While the sigmoid derivative is not explicitly present in the gradient descent update rule, it's an essential part of the process that calculates the gradient used in that update rule. The sigmoid derivative is used in the backpropagation algorithm to determine how much each weight and bias contributes to the overall error, and this information is then used to update the parameters and improve the accuracy of the neural network. Key Takeaway The sigmoid derivative plays a crucial behind-the-scenes role in enabling the gradient descent update rule to work effectively in neural networks that use the sigmoid activation function.  To explicitly show the sigmoid derivative in the gradient descent update rule, we need to expand the gradient term (∂/∂θ) J(θ₀, θ₁) using the chain rule. Explanation Simplified for a Single Example Key Takeaway This expanded form of the gradient descent update rule explicitly shows the role of the sigmoid derivative in calculating the gradient. It highlights how the error is propagated backward through the network, considering the influence of the activation function at each layer. This detailed representation provides a clearer understanding of how the sigmoid derivative contributes to the learning process in neural networks.  Build the Neural Net  X (Input Data) y (Target Output) The purpose of X and y is to train the neural network to learn a mapping between the input features and the target output. By feeding the network with X and y during training, you're essentially teaching it to recognize patterns and relationships in the data so that it can make accurate predictions on new, unseen data. Prediction After training, the neural network can be used to predict the output for new input data. Given a new set of features (X_new), the network will process the input through its layers and produce a prediction (ŷ) that represents the probability of belonging to class 1. Example Scenario Let's say you're building a neural network to predict whether a customer will click on an ad (1 for click, 0 for no click). X could contain features like the customer's age, gender, location, and browsing history. y would contain the corresponding labels indicating whether each customer clicked on the ad or not. By training the network on X and y, you aim to create a model that can predict the likelihood of a new customer clicking on an ad based on their features. X and y are the training data used to teach the neural network to make predictions. X provides the input features, and y provides the corresponding target outputs. The goal is to learn a mapping between the features and the labels so that the network can accurately predict the output for new, unseen data.  Train the Neural Net  Make Predictions  Given the provided X and y arrays, the goal is to train a neural network that can predict the probability of an input belonging to class 0 or class 1. Training Process During training, the neural network will learn to map the input features (X) to the corresponding target labels (y). Since this is a binary classification problem, the network will output a probability between 0 and 1 for each input sample. Prediction After training, if you feed the network a new input sample (with four features), it will predict the probability of that sample belonging to class 1. Example X_new = np.array([[0.3, 0.4, 0.5, 0.6]]) The network might output a probability like 0.8. This indicates that the network predicts an 80% chance that this new input belongs to class 1. Key Takeaway The goal is to train a neural network that can effectively learn the relationship between the input features and the binary output labels, enabling it to make accurate predictions on new, unseen data.  Summary Initialization (__init__) Forward Propagation (forward) Calculates the weighted sum of inputs and biases for each layer. Applies the sigmoid activation function to the result of each layer to introduce non-linearity. Returns the output from the output layer. Backpropagation (backward) Calculates the error between the predicted output and the target output. Calculates the gradients of the error with respect to the weights and biases using the chain rule and sigmoid derivative. Updates the weights and biases using the calculated gradients. Training Creates an instance of the NeuralNetwork class. Defines sample input data (X) and target output (y). Iterates for a specified number of epochs, performing forward and backward passes to train the network. Prediction Uses the trained network to make predictions on the input data. This code demonstrates a basic implementation of a neural network with two hidden layers using only NumPy. It showcases the fundamental concepts of forward propagation, backpropagation, and gradient descent for training a neural network. You can modify this code to experiment with different network architectures, activation functions, and datasets.  Runtime Processing CPU (Central Processing Unit) T4 GPU (Graphics Processing Unit) TPU v2-8 (Tensor Processing Unit) Key Differences When to Use Each In Summary CPUs, T4 GPUs, and TPU v2-8s are all processors designed for different purposes and with varying strengths. CPUs are general-purpose workhorses, GPUs are parallel powerhouses, and TPUs are specialized for AI. Choosing the right processor depends on the specific workload and requirements.  In PyTorch, a torch.Tensor is a multi-dimensional array that is the fundamental building block for all operations and models. You can think of it as the PyTorch equivalent of a NumPy array, but with some key advantages for deep learning. 1. GPU Support 2. Automatic Differentiation 3. Optimized Operations 4. Neural Network Building Blocks 5. Dynamic Computation Graph Creating Tensors Key Takeaways torch.Tensor is the fundamental data structure in PyTorch for numerical computation. It offers GPU support, automatic differentiation, optimized operations, and integration with neural network modules. PyTorch tensors are essential for building and training deep learning models efficiently.  criterion = nn.MSELoss()  # Mean Squared Error loss optimizer = optim.SGD(net.parameters(), lr=0.1)  # Stochastic Gradient Descent These two lines of code are essential for training your neural network in PyTorch. They define the loss function and the optimizer that will be used to update the network's parameters during training. criterion = nn.MSELoss() optimizer = optim.SGD(net.parameters(), lr=0.1) In Summary This iterative process continues for a specified number of epochs, gradually improving the network's accuracy by minimizing the loss function.   "
Basic Artificial Neural Net (ANN),Module 12 - Basic Artificial Neural Net (ANN).docx,"Here's a breakdown of each weight matrix Initialization Here's a breakdown of each bias vector Initialization In summary Think of it like this The learning rate scales these gradients, determining the step size taken in that direction. The update rule for a parameter (e.g., a weight) typically looks like this The choice of learning rate is critical for successful training Finding an appropriate learning rate often involves experimentation and tuning. Common techniques include In essence, forward propagation involves The sigmoid function is defined as where In essence, backpropagation involves Imagine a team working on a project where the final outcome has an error. To improve, they need to figure out where things went wrong The derivative of the sigmoid function is In the backward method, the lines like The sigmoid derivative indirectly relates to the gradient descent update forumula, which represents the gradient descent update rule. Here's how Let's assume θ_j represents a weight connecting a neuron in layer l-1 to a neuron in layer l. The update rule with the sigmoid derivative would look like this where If we consider a single training example and omit the summation, the update rule becomes In summary Let's say after training, you provide the network with a new input Here's a breakdown of the differences between CPUs, T4 GPUs, and TPU v2-8s Here's a breakdown of what makes torch.Tensor special You can create PyTorch tensors in various ways These two components work together during the training loop"
Basis Functions,Module 10 - Logarithms.docx,"A set of functions used to represent or approximate other functions. Examples include polynomials, splines, and wavelets."
Basis Functions,Module 10 - GLMs.docx,"Basis functions are a set of functions used to represent or approximate other functions. Examples include polynomials, splines, and wavelets."
Bayes (Binomial) - https,Module 06 - Astronomy and Statistics.docx,//en.wikipedia.org/wiki/Binomial_distribution
Bayes' Theorem,Module 03 - Probability.docx,A formula for updating beliefs based on new evidence.
Bayesian Approach,Module 10 - Logarithms.docx,An alternative to MLE that incorporates prior knowledge or beliefs about the parameters being estimated.
Bayesian Approach,Module 10 - GLMs.docx,An alternative to MLE that incorporates prior knowledge or beliefs about the parameters being estimated.
Bayesian Statistics,Module 03 - Distributions.docx,Modeling prior beliefs about probabilities or proportions.
Bayesian approach,Module 03 - Miscellaneous.docx,"You might start with a prior belief that the coin is fair (50% chance of heads). Then, you would flip the coin a few times and update your belief based on the observed outcomes. If you get a few heads in a row, your posterior probability of the coin being biased towards heads would increase."
Bayesian statistics,Module 03 - Distributions.docx,Used as a prior distribution for various parameters.
Bayesians,Module 03 - Miscellaneous.docx,View probability as a degree of belief or confidence in an event occurring. They incorporate prior knowledge and update their beliefs based on new evidence using Bayes' theorem. Explicitly incorporate prior information into their analysis. They use Bayes' theorem to combine prior beliefs with observed data to update their understanding of an event's probability. Use posterior distributions and credible intervals to draw inferences about population parameters. They update their prior beliefs based on the observed data to obtain a posterior distribution that reflects their updated knowledge.
Bias,Module 12 - Gradient Descent and Backpropagation.docx,"The bias shifts the activation function, allowing the neuron to activate even with zero input."
Bias,Module 11 - Bias Variance Tradeoff.docx,"In the context of machine learning, bias refers to the error introduced by approximating a real-world problem, which can be complex, with a simplified model. High bias can lead to underfitting, where the model oversimplifies the relationship in the data, missing the underlying patterns."
Bias (b),Module 12 - Gradient Descent and Backpropagation.docx,This is an additional parameter that acts as an offset or threshold. It allows the neuron to activate even when the weighted sum of inputs is zero.
Bias Correction,Module 12 - Gradient Descent and Backpropagation.docx,Adam includes a bias correction mechanism to address the fact that the initial estimates of the first and second moments are biased towards zero.
Bias Variance Tradeoff,Module 11 - Bias Variance Tradeoff.docx,"The document primarily explores the Bias-Variance Tradeoff principle in machine learning, elucidating how striking the right balance between bias and variance is crucial for building effective predictive models. The document emphasizes the importance of understanding and managing the bias-variance tradeoff to develop effective machine learning models. It highlights that while simple models might suffer from high bias, overly complex models can become too sensitive to the training data, leading to high variance and overfitting. The optimal model complexity lies at the point where both bias and variance are adequately controlled, ensuring that the model captures the underlying patterns in the data without overfitting to the noise or specificities of the training set. "
Bias Variance Tradeoff,Module 11 - Bias Variance Tradeoff.docx,Here's a breakdown of the key terms and concepts involved Deep Dive
Bias-Variance Tradeoff,Module 11 - Confusion Matrix Binary.docx,The inherent tradeoff in machine learning between bias (error due to oversimplification) and variance (error due to overcomplexity). The goal is to find the right balance for optimal model generalization.
Bias-Variance Tradeoff,Module 11 - Bias Variance Tradeoff.docx,The tradeoff arises from the fact that decreasing one (bias or variance) often leads to an increase in the other. The goal is to find the sweet spot that minimizes the total error.
Biases,Module 03 - Probability.docx,"Common errors in probability reasoning, such as availability bias and the conjunction fallacy."
Bigrams,Module 12 - Gradient Descent and Backpropagation.docx,"Two-word sequences (e.g., ""the quick"", ""quick brown"")."
Binary Classification,Module 12 - Basic Artificial Neural Net (ANN).docx,"In this case, y has binary values (0 or 1), indicating that you're dealing with a binary classification problem. The network is being trained to classify the input samples into one of two categories."
Binary Outcome,Module 08 - Assumptions.docx,"The dependent variable should be binary (i.e., having only two possible outcomes)."
Binomial,Module 10 - GLMs.docx,"Used for binary data (e.g., success/failure, presence/absence) or count data where the number of trials is fixed."
Binomial Distribution,Module 10 - Logarithms.docx,Used for counting events with two possible outcomes (success or failure).
Binomial Theorem,Module 01 - Introduction.docx,"A mathematical formula used to expand powers of binomials. It has applications in probability, statistics, and other fields."
Black Box,Module 12 - Gradient Descent and Backpropagation.docx,"Neural networks, especially deep ones, are often considered ""black boxes"" because it's difficult to fully understand how they arrive at their predictions."
Blind Experiment,Module 03 - Probability.docx,An experiment where participants don't know if they are in the treatment or control group.
"Boosting is based on the question posed by Kearns and Valiant (1988, 1989)",Module 06 - Terms.docx,"""Can a set of weak learners create a single strong learner?""."
Box plots,Module 05 - EDA.docx,To visualize the distribution of a variable across different groups or categories.
Boxplot,Module 13 - Pandas Time Series Complete.docx,"A way to visually represent the distribution of data, showing the median, quartiles, and potential outliers. In the document, it's used to analyze the distribution of the DOW Jones Industrial Average values in 2020, grouped by month."
Branch,Module 11  - Trees.docx,A section of a decision tree connecting nodes.
Bring the exponent n down in front,Module 12 - Gradient Descent and Backpropagation.docx,2 * x²
Builds credibility and trust,Module 05 - EDA.docx,"A well-told story with data can help build credibility and trust with the audience. By presenting data in a transparent and engaging way, people are more likely to believe the insights and trust the analysis."
Built-in Functions,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch offers a wide range of built-in functions for tensor manipulation, linear algebra, and neural network operations, making it easier to build and train models."
Bulls and Bears,Module 13 - Pandas Time Series Complete.docx,"Terms used to describe general market trends. A ""bull market"" is characterized by rising prices and optimism, while a ""bear market"" indicates falling prices and pessimism."
Business,Module 05 - EDA.docx,"A data analyst might use storytelling to present the results of an A/B test to the marketing team, explaining the impact of different versions of a website on conversion rates."
CPU,Module 12 - Basic Artificial Neural Net (ANN).docx,"For general-purpose tasks, running applications, and tasks that don't require massive parallel computation."
CUDA Integration,Module 12 - Basic Artificial Neural Net (ANN).docx,"One of the primary benefits of PyTorch tensors is their seamless integration with NVIDIA GPUs. This allows you to perform computations on the GPU, significantly accelerating training and inference of deep learning models."
CUDA Integration,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch has excellent support for NVIDIA GPUs, leveraging their parallel processing power to significantly accelerate training and inference of deep learning models."
CUDA Support,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch uses CUDA, a parallel computing platform and API, to efficiently utilize GPU resources."
Calculate the Error,Module 12 - Gradient Descent and Backpropagation.docx,"At the output layer, the network's prediction is compared to the actual target value. This difference is the error, and it's usually measured using a cost function like Mean Squared Error (MSE)."
Calculate the Gradient,Module 12 - Gradient Descent and Backpropagation.docx,"The gradient is a vector that points in the direction of the steepest ascent of the cost function. We want to move in the opposite direction (steepest descent), so we subtract a portion of the gradient from our current theta values."
Calculate the p-value,Module 09 - Feature Selection.docx,Divide the count of extreme values by the total number of bootstrapped samples. This gives you an approximate p-value.
Calculate the statistic,Module 09 - Feature Selection.docx,"For each resampled dataset, calculate the statistic of interest (e.g., mean, median, standard deviation)."
Calculate the statistic for each resampled dataset,Module 09 - Feature Selection.docx,"As described earlier, calculate the statistic of interest for each bootstrapped sample. Calculate the same statistic for each bootstrapped sample."
Calculate the statistic for the original dataset,Module 09 - Feature Selection.docx,Calculate the statistic of interest for the original dataset.
Calculating Gradients,Module 12 - Basic Artificial Neural Net (ANN).docx,"By propagating the error backward and applying the chain rule, we calculate the gradients of the error with respect to each weight and bias in the network. These gradients indicate the direction and magnitude of the influence of each parameter on the error."
Calculating statistics,Module 09 - Feature Selection.docx,"Generate random data from a known distribution (e.g., normal, uniform) to estimate statistics like mean, variance, or percentiles."
Calendars - https,Module 06 - Astronomy and Statistics.docx,//en.wikipedia.org/wiki/History_of_calendars
"Cardinal, Interval, and Ratio",Module 02 - Data.docx,Further classifications of numerical data with increasing levels of measurement properties.
Carl Friedrich Gauss (1777-1855),Module 04 - Hypothesis Testing.docx,Contributed to astronomy and developed the Gaussian (Normal) Distribution.
Categorical,Module 02 - Data.docx,Data that represents groups or categories.
Causality,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Correlation does not imply causation. Even if there's a strong linear relationship between two variables, it doesn't necessarily mean that one causes the other."
Chain Reaction,Module 12 - Basic Artificial Neural Net (ANN).docx,"The error is then propagated back through the network, layer by layer, like a chain reaction. At each layer, we calculate how much of the error from the previous layer is attributed to the current layer's activations."
Chain Rule,Module 12 - Gradient Descent and Backpropagation.docx,The chain rule from calculus is used to calculate how much each weight and bias in the network contributed to the overall error. This involves calculating the partial derivative of the cost function with respect to each weight and bias.
Chain Rule,Module 12 - Basic Artificial Neural Net (ANN).docx,"Backpropagation utilizes the chain rule from calculus to calculate the gradients. The chain rule allows us to break down the calculation of the gradient of the overall error into smaller, manageable steps. To calculate these gradients, you use the chain rule from calculus. The chain rule requires the derivative of the activation function, which in this case is the sigmoid derivative. The product of these terms (y_i - ŷ_i) * σ'(z_i^l) * a_j^{l-1} reflects the chain rule. It calculates how much a small change in the weight θ_j affects the final error by considering the intermediate activations and the sigmoid function."
Chemistry,Module 10 - Logarithms.docx,"The pH scale, a logarithmic scale, measures the acidity or alkalinity of solutions. This is crucial in chemical reactions, biological processes, and environmental monitoring."
Classification Report,Module 11 - Confusion Matrix Multilabel.docx,"A comprehensive evaluation metric that includes precision, recall, F1-score, and support for each class, as well as overall accuracy and averages."
Close to 0,Module 12 - Gradient Descent and Backpropagation.docx,"This means the neuron is not activated or has a very low activation. It's like the neuron is ""silent"" or not contributing much to the overall output of the network. The weighted sum was likely a large negative value. This means the neuron is not activated."
Close to 1,Module 12 - Gradient Descent and Backpropagation.docx,The weighted sum was likely a large positive value. This means the neuron is highly activated.
Close to 1 (or -1 for tanh),Module 12 - Gradient Descent and Backpropagation.docx,"This means the neuron is highly activated. It's like the neuron is ""firing"" strongly and contributing significantly to the network's output."
Coefficient,Module 13 - Statsmodels Time Series Complete.docx,Each lagged value is multiplied by a coefficient that determines its influence on the prediction. These coefficients are estimated from the data.
Coefficients,Module 08 - Coefficients and Metrics.docx,"These are the 'weights' assigned to each feature variable (x1, x2, etc.) in your model. They determine the strength and direction of the relationship between a feature and the outcome."
Coefficients and Metrics,Module 08 - Coefficients and Metrics.docx,"In a linear regression model, the outcome 'y' is predicted using an equation like this Let's imagine a model predicting house prices ('y') based on features like size (x1) and location (x2) This means that"
Coefficients and Metrics,Module 08 - Coefficients and Metrics.docx,"A glossary of terms and concepts from the document you provided is presented below. Mean Squared Error (MSE) A measure of the average squared difference between the predicted values and the actual values in a regression model. R-squared The proportion of the variation in the dependent variable predictable from the independent variable(s). It's a statistical measure that indicates how well a regression model fits the data. Adjusted R-squared A modified version of R-squared that adjusts for the number of predictors in a regression model. It's used to compare models with different numbers of predictors. Least Squares A standard approach in regression analysis for approximating the solution of overdetermined systems by minimizing the sum of the squares of the residuals. It's used to find the line of best fit in linear regression. Statsmodels A Python module that provides tools for estimating different statistical models, conducting statistical tests, and statistical data exploration. It's used for in-depth statistical analysis. Ordinary Least Squares (OLS) A type of linear least squares method for estimating the unknown parameters in a linear regression model. It's a common method for fitting linear regression models. Sklearn Linear Regression Model A machine learning library that provides tools for building and training linear regression models. It's used for creating linear regression models in Python. Standard Error of the Estimate The average distance that the observed values fall from the regression line. It's a measure of the accuracy of the regression model. OLS Regression Results Explanation Model Info Goodness of Fit Coefficients Statistical Tests Understanding the Basics How They Contribute to Predicting 'y' y = β0 + β1*x1 + β2*x2 + ... + βn*xn β0 is the intercept (the value of y when all features are 0) β1, β2, ... βn are the coefficients for each feature x1, x2, ... xn are the values of your features The coefficients, guided by their significance (t-value, p-value) and confidence intervals, determine how much each feature contributes to the final prediction of 'y'. Example Coefficient for size (β1) = 1000 Coefficient for location (β2) = 50000 For every 1 unit increase in size, the house price is predicted to increase by 1000 units (holding location constant). For a 1 unit increase in the location's desirability score, the house price is predicted to increase by 50000 units (holding size constant). The standard error, t-value, p-value, and confidence intervals would further refine these estimates, indicating the precision and significance of these relationships.  "
Colab (Google Colaboratory),Module 13 - Pandas Time Series Complete.docx,A free cloud-based platform provided by Google for working with Jupyter notebooks. It's a popular choice for data analysis and machine learning tasks.
Collections,Module 02 - Data.docx,"Ways to organize and store data in Python, including lists, tuples, sets, dictionaries, and matrices."
Combinations and Permutations,Module 03 - Probability.docx,Methods for counting the number of possible outcomes in different scenarios.
Combining these,Module 12 - Gradient Descent and Backpropagation.docx,∂J/∂w = (y - t) * σ(z) * (1 - σ(z)) * x ∂J/∂b = (y - t) * σ(z) * (1 - σ(z))
Common Use,Module 10 - Link Functions.docx,"Used for continuous response variables with a normal distribution, essentially making the GLM equivalent to ordinary least squares regression. Often used for count data (Poisson regression) or continuous positive data with right-skewed distributions. The standard link function for binary response variables (logistic regression). Also used for binary response variables, often when there's an underlying latent variable with a normal distribution. Suitable for binary response data, particularly when the probability of an event is very low or very high. Also used in survival analysis. Often used for response variables with a gamma distribution. Can be used for count data with a Poisson distribution when variance increases faster than the mean."
Community Support,Module 12 - Gradient Descent and Backpropagation.docx,"A large and active community of PyTorch users and developers provides ample resources, tutorials, and support, making it easier to learn and troubleshoot issues."
Comparing Disparate Quantities,Module 10 - Logarithms.docx,"Logarithms enable comparisons between vastly different scales. For example, the Richter scale, which measures earthquake magnitude, and the pH scale, which measures acidity, are both logarithmic scales. This allows us to quantify and compare earthquakes of vastly different strengths or solutions with vastly different acidities."
Comparison,Module 02 - Data.docx,Data should enable comparisons between groups or conditions to identify differences or effects.
Comprehensive Collection,Module 02 - Data.docx,"In some cases, it's necessary to collect a wide range of data to explore potential patterns and relationships."
Computational Cost,Module 12 - Gradient Descent and Backpropagation.docx,Adding more layers increases the computational cost of training and inference.
Computational Efficiency,Module 12 - Gradient Descent and Backpropagation.docx,"Training deep learning models can be computationally intensive. PyTorch seamlessly integrates with NVIDIA GPUs, allowing you to offload the heavy computations to the GPU and drastically speed up training. Vanilla Python doesn't have this built-in capability."
Computer Science,Module 10 - Logarithms.docx,"Logarithms are fundamental to many algorithms, including those used in search engines, data compression, and cryptography."
Computer Science Perspective,Module 02 - Data.docx,"Data types as seen by the computer, including integers, floating-point numbers, and strings."
Conceptual Representation,Module 02 - Data.docx,"Creating a simplified, abstract view of the data and how different elements relate to each other."
Cond No,Module 08 - Coefficients and Metrics.docx,A test for multicollinearity
Conditional Probability,Module 03 - Probability.docx,The probability of an event occurring given that another event has already occurred.
Confidence Intervals,Module 04 - Hypothesis Testing.docx,A range of values within which the true population parameter is likely to fall with a certain level of confidence.
Confidence Intervals,Module 08 - Coefficients and Metrics.docx,"These provide a range within which the true population coefficient is likely to fall (with a certain level of confidence, e.g., 95%)."
Confidence Intervals,Module 03 - Distributions.docx,To construct confidence intervals for the variance and standard deviation of a normal distribution.
Confidence intervals,Module 09 - Feature Selection.docx,Construct confidence intervals for population parameters by repeatedly sampling from a dataset and calculating the statistic of interest.
Conflicting Tendencies,Module 12 - Gradient Descent and Backpropagation.docx,"The numerator (x² - 4) is approaching zero, suggesting the limit might be zero."
Confounding Variables,Module 03 - Probability.docx,Factors that can distort the relationship between variables in an experiment.
Confusion Matrix,Module 11 - Confusion Matrix Binary.docx,A table used to evaluate the performance of a classification model by comparing predicted class labels to actual class labels. It provides insights into the types of errors made by the model.
Confusion Matrix,Module 11 - Precision Recall.docx,A table used to evaluate the performance of a classification model by comparing actual and predicted class labels.
Confusion Matrix,Module 11 - Confusion Matrix Multilabel.docx,"A table used to evaluate the performance of a classification model. For multinomial models, it shows the counts of true and predicted classes for each class."
Confusion Matrix Binary,Module 11 - Confusion Matrix Binary.docx,
Confusion Matrix Multilabel,Module 11 - Confusion Matrix Multilabel.docx,
Conjugate Prior,Module 03 - Distributions.docx,"The Beta distribution is often used as a conjugate prior distribution in Bayesian statistics, particularly for binomial likelihoods. This means that if the prior distribution for a binomial parameter is a Beta distribution, then the posterior distribution will also be a Beta distribution after observing data."
Connects,Module 12 - Basic Artificial Neural Net (ANN).docx,Input layer to the first hidden layer. First hidden layer to the second hidden layer. Second hidden layer to the output layer.
Constant features,Module 02 - Miscellaneous.docx,Have only one unique value in the entire dataset.
Context,Module 12 - Gradient Descent and Backpropagation.docx,"N-grams capture the local context of words, which is crucial for accurate word prediction. Higher-order n-grams capture more context."
Context and Data,Module 12 - Gradient Descent and Backpropagation.docx,The specific features learned by the hidden layers depend heavily on the training data and the task.
Contingency Table,Module 11 - Confusion Matrix Binary.docx,"A table that displays the frequencies of different combinations of two categorical variables. In the case of a confusion matrix, the variables are the actual and predicted classes."
Control Flow,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch integrates seamlessly with Python's control flow statements (like if, for, while), making it easy to implement complex logic and dynamic models."
Convergence,Module 12 - Gradient Descent and Backpropagation.docx,"We want the cost function to converge to a minimum value as we iterate through the gradient descent process. This means that as the number of iterations approaches infinity, the cost function should approach a specific value (the minimum)."
Convergence Point,Module 12 - Gradient Descent and Backpropagation.docx,Convergence occurs when the algorithm reaches a point where further iterations don't significantly change the parameter values or the loss. This point is often a local minimum of the cost function.
Convexity,Module 12 - Gradient Descent and Backpropagation.docx,"For linear regression, the MSE cost function is convex. This means it has a single global minimum, ensuring that gradient descent will converge to the optimal solution."
Correlation,Module 01 - Introduction.docx,"A standardized measure of the relationship between two variables, ranging from -1 to +1."
Count the number of extreme values,Module 09 - Feature Selection.docx,Count how many times the calculated statistic from the resampled datasets is as extreme or more extreme than the statistic from the original dataset.
Covariance,Module 01 - Introduction.docx,A measure of how two variables change together.
Covariance,Module 02 - Descriptive Statistics.docx,A measure of how two variables change together.
Covariance Type,Module 08 - Coefficients and Metrics.docx,deals with violations of assumptions
Create multiple samples,Module 09 - Feature Selection.docx,Randomly draw samples (with replacement) from the original dataset. These samples are typically the same size as the original dataset.
Critical Value,Module 04 - Hypothesis Testing.docx,A value that separates the rejection region from the non-rejection region in a hypothesis test.
Cross Validation,Module 11 - Cross Validation.docx,"A technique used to evaluate the performance of a machine learning model by partitioning data into training and testing sets multiple times. This helps assess how well the model generalizes to new, unseen data and avoids overfitting. "
Cumulative Distribution Function (CDF),Module 10 - Logarithms.docx,A function that gives the probability that a random variable will take a value less than or equal to a given value.
DF Model,Module 08 - Coefficients and Metrics.docx,number of parameters in the model excluding the constant if present
DF Residuals,Module 08 - Coefficients and Metrics.docx,degrees of freedom of the residuals
Data,Module 02 - Data.docx,Here's a detailed outline and summary of key elements and terms Basic Data Types
Data,Module 02 - Data.docx,"The document ""Data Complete.ipynb - Colab.pdf"" appears to be a comprehensive guide to understanding and working with data in the context of data science. It covers a wide range of topics, from basic definitions to handling various data sources and formats. Introduction Data Defined A collection of discrete values that convey information, describing quantity, quality, facts, or other units of meaning. Data Science, Data Analysis, and Data Mining Data Life Cycle Data Types  Data Structures Data Modeling Data Models and Data Structures in Python Sources of Data Data Points and Research Questions Statistical Models and Inference Data Perspectives and Requirements Getting Data "
Data Analysis,Module 02 - Data.docx,"Involves examining, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making."
Data Exploration,Module 03 - Distributions Fitter.docx,"When you have a new dataset, fitter can help you understand the underlying distribution of your data."
Data Mining,Module 02 - Data.docx,"A specific process of uncovering patterns and information from large databases, commonly used in market analysis, fraud detection, and financial analysis."
Data Organization,Module 02 - Data.docx,Data is typically organized into structures like tables to provide context and meaning.
Data Science,Module 02 - Data.docx,"A broad field focused on extracting knowledge and insights from data, often with scientific applications."
Data Science Process,Module 02 - Data.docx,"This section introduces the typical steps involved in a data science project, including asking questions, obtaining data, understanding and cleaning the data, and using it to draw inferences and make decisions."
Data Streams,Module 02 - Data.docx,"Continuous flows of data from sources like sensor networks, GPS devices, and social media feeds."
Data vs Datum,Module 01 - Introduction.docx,The distinction between data (plural) and datum (singular).
Databases,Module 02 - Data.docx,"Organized collections of data, including SQL (relational) and NoSQL (non-relational) databases."
DateTime Index,Module 13 - Pandas Time Series Complete.docx,"In Pandas, the DateTime Index is a way to store and access time series data where the index of the DataFrame is a series of dates or timestamps. This allows for easy and efficient date-based operations and analysis."
Datum,Module 02 - Data.docx,A single value within a dataset.
Decision Boundaries,Module 12 - Gradient Descent and Backpropagation.docx,"In classification tasks, the activation levels of neurons in the output layer determine the predicted class."
Decision Node,Module 11  - Trees.docx,A node in a decision tree where a decision is made based on a feature.
Decision Tree Classifier,Module 11 - Cross Validation.docx,A machine learning model that uses a tree-like structure to make decisions based on a series of rules inferred from the data. It's used as an example in the document to demonstrate cross-validation.
Deep Learning Applications,Module 12 - Basic Artificial Neural Net (ANN).docx,"This parallel processing power makes GPUs well-suited for deep learning tasks like image recognition, natural language processing, and scientific simulations."
Deeper Representation,Module 12 - Gradient Descent and Backpropagation.docx,The second hidden layer allows the network to create a deeper representation of the input sequence. This means it can capture more intricate patterns and context that might be missed with a single hidden layer.
Define the problem,Module 09 - Feature Selection.docx,Clearly specify the problem or system you want to analyze.
Define-by-Run,Module 12 - Basic Artificial Neural Net (ANN).docx,"PyTorch uses a dynamic computation graph, which means the graph is constructed as you execute operations. This allows for flexibility in defining and modifying models during runtime, which is particularly useful for research and experimentation."
Define-by-run,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch allows you to define your neural network's behavior on the fly, modifying the computation graph as you go. This dynamic nature is incredibly valuable for research and experimentation, enabling rapid prototyping and trying out new ideas."
Definition,Module 04 - The Tests.docx,"A statistical test used to compare the means of two groups. It determines if the difference between the means is statistically significant or likely due to random variation. A statistical test used to assess the equality of variances across different groups or samples. It helps determine if the assumption of equal variances is met for tests like ANOVA. A statistical test used to assess if a dataset follows a normal distribution. It helps determine if the normality assumption is met for tests that require normally distributed data. A non-parametric test used to compare a sample with a reference probability distribution (one-sample K-S test) or to compare two samples (two-sample K-S test). It assesses if the samples come from the same distribution. A statistical test used to determine if there is a significant association between two categorical variables. It analyzes the frequencies in a contingency table to assess if the observed differences between groups are likely due to chance. A statistical test used to compare the means of three or more groups. It assesses if there are statistically significant differences between the means of the groups or if the observed differences are due to random variation. A non-parametric test used to compare the distributions of three or more groups. It is an alternative to ANOVA when the data does not meet the assumptions of normality or equal variances. A statistical test used to determine if there are non-random associations between two categorical variables, especially when sample sizes are small. It calculates the exact probability of observing the data in a contingency table, assuming no association between the variables. A statistical test used to compare a sample mean to a known population mean when the population standard deviation is known and the data is normally distributed or the sample size is large. A variation of the independent t-test used when the variances of the two groups being compared are not assumed to be equal. It adjusts the degrees of freedom and provides a more accurate test in cases of unequal variances. A non-parametric test used to compare the distributions of two independent groups. It assesses if one group tends to have larger or smaller values than the other."
Definition,Module 04 - Effect Size and Statistical Power.docx,A quantitative measure of the magnitude of a phenomenon or relationship between variables. The probability that a statistical test will correctly reject a false null hypothesis. A measure of effect size that assesses the standardized difference between two means. A measure of association between an exposure and an outcome. The ratio of the probability of an event occurring in an exposed group to the probability of the event occurring in a non-exposed group. A statistical measure that quantifies the linear relationship between two continuous variables. The incorrect acceptance of a false null hypothesis. The number of observations or participants included in a study. The probability of rejecting a true null hypothesis. A statistical method used to determine the minimum sample size needed to detect a statistically significant difference or relationship. A statistical test used to compare the means of two groups.
Degrees of Freedom,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","The number of independent pieces of information available to estimate a parameter. In the context of adjusted R-squared, degrees of freedom are used to account for the number of variables and observations in the model."
Degrees of Freedom (k),Module 03 - Distributions.docx,The degrees of freedom determine the shape of the distribution. It is related to the number of independent standard normal deviates being summed.
Delayed Rewards,Module 12 - Gradient Descent and Backpropagation.docx,The agent might need to take actions that don't yield immediate rewards but lead to higher rewards in the long run.
Dendrites,Module 12 - Gradient Descent and Backpropagation.docx,These are branch-like structures that receive signals (inputs) from other neurons.
Dep. Varialble,Module 08 - Coefficients and Metrics.docx,"the response variable, dependent, outcome, etc."
Derivatives,Module 10 - Logarithms.docx,"Measure the rate of change of a function. The first derivative is like velocity, and the second derivative is like acceleration."
Derivatives,Module 12 - Gradient Descent and Backpropagation.docx,The result of differentiation is called a derivative. The derivative of a function gives you a new function that tells you the slope of the tangent line at any point on the original function's curve. A Measure of Change
"Descending the ""Hill""",Module 12 - Gradient Descent and Backpropagation.docx,"To minimize MSE, we want to move in the opposite direction of the gradient, i.e., the direction of the steepest descent. This is where the ""gradient descent"" name comes from."
Descending the Loss Landscape,Module 12 - Gradient Descent and Backpropagation.docx,Imagine the cost function as a landscape with hills and valleys. Gradient descent starts at a random point on this landscape and takes steps in the direction of the steepest descent to reach a valley (minimum).
Descriptive Statistics,Module 02 - Descriptive Statistics.docx,"The document ""Module 02 - Statistics Complete.ipynb - Colab.pdf"" provides a comprehensive overview of essential statistical concepts and their applications in data science. It covers a wide range of topics, from basic descriptive statistics to more advanced concepts like skewness, kurtosis, and correlation. Descriptive Statistics Correlation Other Important Concepts The document also includes various examples and visualizations to illustrate these concepts and their applications in data analysis. Skewness A measure of the asymmetry of a probability distribution. Indicates the direction and degree to which a distribution leans towards one side of the mean. A symmetrical distribution has a skewness of zero. A distribution skewed to the right (positive skew) has a longer tail on the right side. A distribution skewed to the left (negative skew) has a longer tail on the left side. Kurtosis A measure of the ""tailedness"" of a probability distribution. Describes how concentrated the data points are around the mean compared to the tails. A mesokurtic distribution has a kurtosis similar to a normal distribution. A leptokurtic distribution has a higher peak and fatter tails, indicating more extreme values. A platykurtic distribution has a lower peak and thinner tails, indicating fewer extreme values. "
Descriptive Statistics,Module 02 - Descriptive Statistics.docx,Here's a detailed outline and summary of key elements and terms
Determine the confidence level,Module 09 - Feature Selection.docx,"Choose a confidence level (e.g., 95%)."
Differentiability,Module 12 - Gradient Descent and Backpropagation.docx,"MSE is a smooth, continuous function that's differentiable. This property is crucial for gradient descent, as it relies on calculating the gradient (derivatives) of the cost function."
Distributed Training,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch provides tools for distributed training across multiple GPUs or machines, enabling efficient scaling for large datasets and complex models."
Distributions,Module 03 - Distributions.docx,"Outline and Summary of Key Elements and Terms Random Variables A random variable is a variable whose value is a numerical outcome of a random phenomenon. Random variables can be discrete (e.g., the outcome of a die roll) or continuous (e.g., the height of a person). Understanding random variables is essential for probability and statistical modeling. A collection of random variables is independent and identically distributed (IID) if each variable has the same probability distribution as the others and all are mutually independent. IID random variables are commonly assumed in many statistical analyses. The IID assumption simplifies calculations and allows for the use of certain statistical methods. Types of Distributions There are many different types of probability distributions, including normal, uniform, binomial, Bernoulli, Poisson, and others. Each distribution has its own unique characteristics and parameters that define its shape and behavior. Choosing the appropriate distribution is crucial for accurate statistical modeling and inference. Probability Density Function (PDF) The PDF is a function that describes the relative likelihood of a continuous random variable taking on a given value. The area under the PDF curve between two points represents the probability that the variable falls within that range. The total area under the PDF curve is always equal to 1. Cumulative Density Function (CDF) The CDF is a function that gives the probability that a random variable is less than or equal to a given value. The CDF is the integral of the PDF. The CDF is useful for calculating probabilities of events that fall within a certain range. Percent Point Function (PPF) The PPF is the inverse of the CDF. It gives the value of the random variable for which the CDF is equal to a given probability. The PPF is useful for finding quantiles and percentiles of a distribution. Kernel Density Estimation (KDE) KDE is a non-parametric method for estimating the PDF of a random variable. It uses a kernel function to smooth the observed data and estimate the underlying distribution. KDE is useful when the data does not fit a known parametric distribution. Normal Distribution The normal distribution is a bell-shaped, symmetrical distribution that is widely used in statistics. It is characterized by its mean (μ) and standard deviation (σ). Many natural phenomena follow a normal distribution. Z Distribution The Z distribution is a standard normal distribution with a mean of 0 and a standard deviation of 1. It is used to standardize normal distributions and calculate probabilities. Z-scores represent the number of standard deviations a data point is from the mean. t Distribution The t-distribution is similar to the normal distribution but has heavier tails. It is used when the sample size is small or the population standard deviation is unknown. The t-distribution approaches the normal distribution as the sample size increases. Uniform Distribution The uniform distribution is a distribution where all values within a given range are equally likely. It is often used to model random events with no clear preference for any particular outcome. The uniform distribution can be discrete or continuous. Binomial Distribution The binomial distribution is a discrete distribution that models the probability of a certain number of successes in a sequence of independent trials. Each trial has only two possible outcomes (success or failure). The binomial distribution is characterized by the number of trials (n) and the probability of success on each trial (p). Bernoulli Distribution The Bernoulli distribution is a special case of the binomial distribution with only one trial. It models the probability of a single event with two possible outcomes (success or failure). The Bernoulli distribution is characterized by the probability of success (p). Multinomial Distribution The multinomial distribution is a generalization of the binomial distribution to more than two possible outcomes. It models the probability of a certain number of occurrences of each outcome in a sequence of independent trials. The multinomial distribution is characterized by the number of trials (n) and the probabilities of each outcome. Poisson Distribution The Poisson distribution is a discrete distribution that models the probability of a certain number of events occurring in a fixed interval of time or space. It is often used to model rare events. The Poisson distribution is characterized by the average rate of events (λ). Chi-Squared Distribution The Chi-Squared distribution is a continuous probability distribution that is widely used in statistics. It is the distribution of the sum of squared standard normal deviates. Suppose you want to test whether a die is fair. You roll the die 60 times and record the number of times each face appears. You can use the Chi-Squared goodness of fit test to compare the observed frequencies with the expected frequencies for a fair die (10 for each face). If the test statistic is large, it suggests that the die may not be fair. Gamma Distribution In summary, the Gamma distribution is a versatile tool for modeling positive, skewed data and has applications in a wide range of fields due to its flexibility and relationship to other important distributions. Beta Distribution The Beta distribution is a continuous probability distribution defined on the interval [0, 1]. It is commonly used to model random variables that represent probabilities or proportions. The Beta distribution is characterized by two shape parameters, α and β, which determine the shape of the distribution. In summary, the Beta distribution is a versatile tool for modeling probabilities, proportions, and other quantities that fall within the range of 0 to 1. Its flexibility and properties make it suitable for a wide range of applications in various fields. "
Distributions,Module 10 - Logarithms.docx,Describe the probability of different outcomes in a random variable. Common types include:
Distributions,Module 03 - Distributions.docx,"Key properties Uses Example The Gamma distribution is a continuous probability distribution that is widely used in statistics and various fields to model positive, skewed data. It's characterized by two parameters Key Properties Uses Key Properties Uses"
Distributions Fitter,Module 03 - Distributions Fitter.docx,"The Python fitter library is a handy tool for figuring out which probability distribution best matches your data. It does this by fitting various distributions to your data and comparing how well they match. This can be super useful in fields like statistics, data analysis, and machine learning where understanding the underlying distribution of your data is key. Bash pip install fitter  Python from fitter import Fitter import numpy as np  # Generate some sample data data = np.random.randn(1000)  # Create a Fitter object f = Fitter(data)  # Fit the data to various distributions f.fit()  # Print the summary of the best fitting distributions f.summary()  In this example, Fitter tries to fit various distributions to the provided data. The summary() method then shows you the top distributions that best fit your data, along with their parameters. You can also specify which distributions you want fitter to try. This can save time if you have an idea of what kind of distribution your data might follow. Python f = Fitter(data, distributions=['norm', 'gamma', 't']) f.fit() f.summary()  The fitter library also lets you visualize how well the distributions fit your data. You can use the hist() and plot_pdf() methods for this. Python f.hist() f.plot_pdf()  You can get the parameters of the best fitting distribution using the get_best() method. Python best_params = f.get_best() print(best_params)  The fitter library uses the scipy.stats module under the hood, so it supports a wide range of distributions. The fitting process can take some time, especially if you have a large dataset or are trying many distributions. It's always a good idea to visually inspect the fit of the distributions to make sure they make sense for your data. "
Distributions Fitter,Module 03 - Distributions Fitter.docx,"Here's a breakdown of how it works 1. Installation First things first, you need to install the fitter library. You can do this using pip 2. Basic Usage Here's a simple example to illustrate how to use the fitter library 3. Specifying Distributions 4. Visualizing the Fit 5. Accessing Best Parameters Different Ways fitter is Used Important Notes"
Drives action and decision-making,Module 05 - EDA.docx,A good story can evoke emotions and motivate people to take action. Storytelling with data can help drive decision-making by clearly communicating the implications of the analysis and inspiring people to act on the insights.
Duck Typing,Module 02 - Data.docx,Python's approach of determining an object's type based on its behavior rather than explicit type declarations.
Durbin-Watson,Module 08 - Coefficients and Metrics.docx,"A test for the presence of autocorrelation, if the errors aren't independent"
Dynamic Typing,Module 02 - Data.docx,The data type of a variable is determined during program execution.
EDA,Module 05 - EDA.docx,"Exploratory Data Analysis (EDA) is a crucial initial step in any data science project, where the primary goal is to gain a deep understanding of the data before formal modeling or hypothesis testing. It involves a combination of summarizing the main characteristics of the data, visualizing it from different angles, and identifying potential patterns, relationships, and anomalies. Key Components of EDA Descriptive Statistics Descriptive statistics provide a concise summary of the data's main features. This includes measures of central tendency (mean, median, mode), measures of spread (variance, standard deviation, range, quartiles), and measures of shape (skewness, kurtosis). Descriptive statistics help understand the distribution of individual variables, identify potential outliers, and guide further analysis. GroupBy and CrossTab GroupBy allows you to split your data into groups based on categorical variables and then calculate summary statistics for each group. This helps identify differences or similarities between groups and understand how variables behave across different categories. CrossTab (also known as contingency table) is used to analyze the relationship between two or more categorical variables. It shows the frequency distribution of one variable, conditional on the values of other variables, helping identify potential associations or dependencies between them. Pivot Tables Pivot tables are similar to CrossTabs but provide more flexibility in summarizing and rearranging data. They allow you to aggregate data across multiple dimensions and create different views of the data to explore various relationships and patterns. Correlation Correlation measures the strength and direction of the linear relationship between two variables. A positive correlation indicates that the variables tend to move in the same direction, while a negative correlation indicates they move in opposite directions. Correlation helps identify potential predictor variables and understand the relationships between different features in the data. Multicollinearity Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated with each other. This can make it difficult to isolate the individual effects of each predictor on the response variable and can lead to unstable or unreliable estimates of the regression coefficients. Identifying and addressing multicollinearity is important for building robust and interpretable regression models. Plotting in EDA By combining these visualization techniques with the analytical tools mentioned earlier, you can gain a comprehensive understanding of the data during the EDA phase and lay a strong foundation for subsequent modeling and analysis. Storytelling Storytelling with data is the ability to effectively communicate insights from data analysis in a clear, compelling, and engaging way that resonates with the audience and drives action. It involves crafting a narrative around the data that helps people understand the context, insights, and implications of the analysis. By combining data analysis with storytelling techniques, you can create compelling narratives that inform, engage, and inspire your audience to take action. "
EDA,Module 05 - EDA.docx,"Visualizing data is a crucial aspect of EDA, as it allows for a more intuitive understanding of the data's structure and relationships. Some useful plotting techniques include Here are some key reasons why storytelling with data is important Here are some examples of how storytelling with data can be used"
ESS (Explained Sum of Squares),"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx",Measures the variability in the dependent variable that is explained by the independent variable(s). It's the sum of the squared differences between the predicted values and the mean of the dependent variable.
ETS,Module 13 - Statsmodels Time Series Complete.docx,"ETS decomposition is a time series analysis method that decomposes a time series into its error, trend, and seasonality components. It is used to understand the underlying patterns and dynamics of a time series and can be helpful for forecasting and anomaly detection."
EWMA,Module 13 - Statsmodels Time Series Complete.docx,EWMA stands for Exponentially Weighted Moving Average. It's a way to smooth out data by giving more weight to recent observations and less weight to older observations. This makes it particularly useful for analyzing time series data where the most recent data points are often the most relevant.
Early Probability,Module 03 - Probability.docx,The development of probability theory from its early roots in gambling to its modern applications in various fields.
Economics,Module 12 - Gradient Descent and Backpropagation.docx,Analyzing marginal costs and revenues.
Education,Module 05 - EDA.docx,"An educator might use storytelling to present student performance data to parents, highlighting areas of improvement and identifying areas where additional support might be needed."
Effect Size and Statistical Power,Module 04 - Effect Size and Statistical Power.docx,Let’s get this done. Here are the explanations and definitions for the terms and concepts presented in the provided document
Effect Size and Statistical Power,Module 04 - Effect Size and Statistical Power.docx, Definitions and Explanations of Terms and Concepts Effect Size Statistical Power Cohen's d Odds Ratio Relative Risk Ratio Pearson's Correlation Type II Error Sample Size Significance Level (Alpha) Power Analysis t-Test 
Efficiency,Module 12 - Basic Artificial Neural Net (ANN).docx,Biases can improve the learning efficiency of the network by allowing for faster convergence during training.
Efficiency,Module 12 - Gradient Descent and Backpropagation.docx,"It provides a quick and easy way to find derivatives without having to use the limit definition of a derivative. Training on n-grams can be computationally efficient, especially for smaller datasets. Adam often converges faster than standard gradient descent, especially in complex or noisy optimization landscapes."
Efficient Computations,Module 12 - Basic Artificial Neural Net (ANN).docx,"PyTorch tensors are optimized for efficient numerical computation. They provide a wide range of built-in functions for tensor manipulation, linear algebra, and other mathematical operations commonly used in deep learning."
Embedding Layer,Module 12 - Gradient Descent and Backpropagation.docx,The embedding layer converts each word into a dense vector representation. These vectors capture some semantic information about the words.
Endog(enous),Module 08 - Coefficients and Metrics.docx,Similar to the dependent variable
Engineering,Module 12 - Gradient Descent and Backpropagation.docx,Optimizing designs and analyzing systems.
Enhances understanding and retention,Module 05 - EDA.docx,"Stories are easier to remember than isolated facts and figures. By framing data insights in a narrative, people are more likely to understand and retain the information."
Error Matrix,Module 11 - Confusion Matrix Binary.docx,Another name for a confusion matrix.
Error Term,Module 12 - Basic Artificial Neural Net (ANN).docx,(y_i - ŷ_i) represents the error between the target output and the predicted output for the i-th example.
Error Term,Module 13 - Statsmodels Time Series Complete.docx,The model also includes an error term to account for random fluctuations and unpredictable factors.
Escaping Local Minima,Module 12 - Gradient Descent and Backpropagation.docx,"The accumulated velocity can help the optimizer ""power through"" shallow local minima and reach a better solution."
Estimate the sampling distribution,Module 09 - Feature Selection.docx,The distribution of the calculated statistics across all the resampled datasets approximates the sampling distribution of the statistic.
Estimating pi,Module 09 - Feature Selection.docx,Randomly generate points within a square and count how many fall inside a circle inscribed within the square. The ratio of points inside the circle to the total points can be used to estimate pi.
Estimating probabilities,Module 09 - Feature Selection.docx,"Simulate events with known probabilities (e.g., coin flips, dice rolls) to estimate the likelihood of specific outcomes."
Ethics and Human Bias,Module 01 - Introduction.docx,This section highlights the ethical considerations and potential biases that can arise in data science projects.
Euclid Elements,Module 01 - Introduction.docx,"The foundational text of geometry, introducing basic concepts like points, lines, planes, and angles."
Euler's Number (e),Module 10 - Logarithms.docx,"Approximately 2.71828. It's used in calculations involving compound interest, exponential growth, and the natural logarithm."
Example,Module 12 - Gradient Descent and Backpropagation.docx,y = x² as x approaches 2 y = x² y = x²
Example,Module 12 - Basic Artificial Neural Net (ANN).docx,"In this case, X has two samples, each with four features. These features could represent anything, such as the measurements of different attributes of an object or the pixel values of an image."
Exog(enous),Module 08 - Coefficients and Metrics.docx,Similar to the independent variable
Expected Error,Module 11 - Bias Variance Tradeoff.docx,"The expected error of a model on unseen data can be decomposed into three components: bias squared, variance, and irreducible error."
Expected Value,Module 01 - Introduction.docx,The average value that is expected to be obtained in a random experiment over a large number of trials.
Explanation,Module 04 - Effect Size and Statistical Power.docx,"Effect size helps researchers understand the practical significance of their findings, going beyond just statistical significance. It indicates how much difference or relationship exists between groups or variables. For example, Cohen's d is used to measure the standardized difference between two means, while Pearson's correlation coefficient measures the strength and direction of a linear relationship between two variables. In other words, it's the likelihood of detecting a real effect if one exists. A higher statistical power means a lower chance of making a Type II error (failing to reject a false null hypothesis). Power is influenced by factors like sample size, effect size, and significance level. It tells us how many standard deviations apart two means are. A larger d value indicates a greater difference between the groups. It represents the odds of an outcome occurring in an exposed group compared to the odds of the same outcome occurring in an unexposed group. An odds ratio of 1 indicates no association, while values greater than 1 suggest a positive association and values less than 1 suggest a negative association. It's used to assess the risk of an outcome in one group compared to another. A relative risk ratio of 1 means no difference in risk between groups. Values greater than 1 indicate increased risk in the exposed group, while values less than 1 indicate decreased risk. It ranges from -1 to +1, where -1 indicates a perfect negative linear relationship, +1 indicates a perfect positive linear relationship, and 0 indicates no linear relationship. [cite: 4] It means failing to detect a real effect when one exists. The probability of a Type II error is denoted by the Greek letter beta (β). A larger sample size generally increases the power of a statistical test and improves the precision of estimates. It's the threshold set by researchers to determine statistical significance. A commonly used alpha level is 0.05, meaning there's a 5% chance of rejecting the null hypothesis when it's actually true. It helps researchers design studies with adequate power to avoid Type II errors. Power analysis can also be used to calculate the power of a test given the sample size, effect size, and significance level. It determines if there's a statistically significant difference between the means of two independent groups or between paired observations."
Exploration vs. Exploitation,Module 12 - Gradient Descent and Backpropagation.docx,The agent needs to balance exploring new actions and exploiting actions that are known to be good.
F statistic,Module 08 - Coefficients and Metrics.docx,a measure of how significant the fit is
F1 Score,Module 11 - Precision Recall.docx,The harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.
F1 Score,Module 11 - Confusion Matrix Binary.docx,The harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.
F1-Score,Module 11 - Confusion Matrix Multilabel.docx,"The harmonic mean of precision and recall, providing a balanced measure of a model's performance for a specific class."
FN (False Negative),Module 11 - Confusion Matrix Multilabel.docx,The number of instances incorrectly predicted as not belonging to a specific class.
FP (False Positive),Module 11 - Confusion Matrix Multilabel.docx,The number of instances incorrectly predicted as belonging to a specific class.
False Negative (FN),Module 11 - Confusion Matrix Binary.docx,An instance where the model incorrectly predicts the negative class when the actual class is positive.
False Positive (FP),Module 11 - Confusion Matrix Binary.docx,An instance where the model incorrectly predicts the positive class when the actual class is negative.
Faster Convergence,Module 12 - Gradient Descent and Backpropagation.docx,"Momentum can help the optimization process converge faster, especially in situations with noisy gradients or oscillating updates."
Feature Extraction,Module 12 - Gradient Descent and Backpropagation.docx,"Each hidden layer learns to extract relevant features from the input sequence. In the context of word prediction, these features could be things like:"
Feature Importance,Module 12 - Basic Artificial Neural Net (ANN).docx,The magnitude of the weights can indicate the importance of different features. Larger weights suggest that the corresponding features have a stronger influence on the network's output.
Feature Importance,Module 12 - Gradient Descent and Backpropagation.docx,Highly activated neurons indicate that the corresponding input features are important for the task the network is learning.
Feature Selection,Module 09 - Feature Selection.docx,"The document primarily explores feature selection techniques using the Titanic dataset. It aims to identify the most relevant features for predicting survival outcomes. Let's break down some of the key terms and concepts Example Example The impact on feature selection Important Note Benefits In the context of the Titanic example Here are the basic steps involved in cross-validation Benefits of using cross-validation Bootstrapping is a statistical method that involves repeatedly resampling data from a given dataset to estimate the sampling distribution of a statistic. This technique is particularly useful when dealing with small sample sizes or when the underlying distribution of the data is unknown. Here's how it works Bootstrapping can be used to construct confidence intervals, which provide a range of plausible values for a population parameter. Here's how Bootstrapping can also be used to estimate p-values, which indicate the strength of evidence against a null hypothesis. Here's a simplified approach Here are the general steps involved in a Monte Carlo simulation"
Feature Selection,Module 09 - Feature Selection.docx,"Exploratory Data Analysis (EDA) EDA is used to understand and summarize the main characteristics of a dataset. In this context, EDA is used to find the features that are most useful in predicting the dependent variable (survived). Feature Selection Selecting the most relevant features for a machine-learning model. It aims to improve model performance. Filter Methods Uses statistical metrics to rank features. Examples include correlation, chi-square, and ANOVA. Wrapper Methods Use algorithms to select optimal features. Examples include forward selection, backward selection, and stepwise selection. Embedded Methods Selects features during the model-building process. Examples include Lasso, Ridge, and Elastic Net. Correlation Measures the linear relationship between two variables. It helps in identifying features that are strongly related to the target variable. Chi-Square Test Used to determine relationships between categorical features. A higher chi-square statistic indicates a stronger association. ANOVA (Analysis of Variance) A statistical test used to compare means across two or more groups. It is used in feature selection to identify features that have significantly different means between groups. Information Gain Measures how much information a feature provides about the target variable. It is often used in decision tree-based models. Mutual Information Measures the mutual dependence between two variables. It can capture both linear and non-linear relationships. Variance Inflation Factor (VIF) Measures how much a predictor is influenced by the presence of other predictors. It helps in identifying multicollinearity (high correlation between predictors). Scalers Used to transform features to a specific range or distribution. Examples include MinMaxScaler, StandardScaler, and RobustScaler. Outliers Data points that differ significantly from other observations. They can be handled by dropping, marking, or rescaling. One-Hot Encoding Transforms categorical features into numerical representations. It is used to ensure that categorical data is compatible with machine learning algorithms. Label Encoder Transforms categorical labels into numerical representations. It is used for the target variable in classification tasks. Confusion Matrix A table used to evaluate the performance of a classification model. It provides insights into true positives, true negatives, false positives, and false negatives. Overfitting and Underfitting in Feature Selection Overfitting Overfitting happens when a model learns the training data too well, including the noise and outliers. This makes the model perform very well on the training data but poorly on unseen data. In feature selection, overfitting can occur when you select too many features, especially irrelevant ones. The model becomes overly complex and starts to fit the specific nuances of the training set that don't generalize to new data. Imagine using the Titanic dataset and including features like 'Passenger Name' or 'Ticket Number'. These features might have unique variations in the training set that don't hold any predictive power on new data. The model might wrongly associate specific names or ticket numbers with survival, leading to overfitting. Underfitting Underfitting occurs when the model is too simple to capture the underlying patterns in the data. This happens when you select too few features, or the selected features don't have enough predictive power. The model fails to adequately learn the relationships between the features and the target variable, resulting in poor performance on both training and unseen data. If you only select 'Passenger Class' as a feature for predicting survival on the Titanic, you might miss out on crucial information like 'Sex' or 'Age', which significantly influenced survival rates. This oversimplification can lead to underfitting. The Goal The goal of feature selection is to find the right balance – selecting enough relevant features to capture the important patterns in the data without including irrelevant features that lead to overfitting. Techniques like those discussed in the Titanic example (correlation, chi-square, mutual information, etc.) help identify the most informative features and achieve that balance. Regularization is a technique used to prevent overfitting in machine learning models. It does this by adding a penalty to the complexity of the model. Regularization In terms of feature selection, regularization essentially forces the model to reduce the influence of less important features. It can even shrink the coefficients of some features to zero, effectively removing them from the model. Regularization helps in identifying the most important features. It simplifies the model and improves its ability to generalize to new data. It can make the model more stable and less sensitive to small changes in the data. Lasso (L1) Regularization Adds a penalty to the sum of the absolute values of the coefficients. It can shrink some coefficients to zero, effectively performing feature selection. Ridge (L2) Regularization Adds a penalty to the sum of the squared values of the coefficients. It forces the coefficients of less important features to be small but not necessarily zero. The choice between Lasso and Ridge depends on the specific dataset and problem. Regularization is just one of many techniques used for feature selection. It is often used with other methods to identify the most relevant features. Elastic Net Elastic Net is a regularization and variable selection technique that combines the penalties of Lasso and Ridge regression. It's particularly useful when dealing with datasets that have a large number of correlated features. Handles correlated features better than Lasso. Can select more features than Lasso, especially when the number of predictors is greater than the number of observations. Provides a balance between feature selection and coefficient shrinkage. While the document doesn't explicitly apply Elastic Net, it could be used to explore feature selection and potentially improve model performance. It might be particularly helpful if there were strong correlations between certain features (e.g., age and passenger class). Cross-validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It helps to understand how well the model will generalize to new data. Cross Validation Provides a more reliable estimate of model performance. Helps to identify overfitting or underfitting. Can be used with small datasets. Bootstrapping Confidence Intervals P-values Key Advantages of Bootstrapping Monte Carlo Simulation  Monte Carlo Simulation A Monte Carlo simulation is a computational technique that uses random sampling to conduct statistical analysis and solve problems. It is based on the idea of repeatedly generating random inputs for a model or system and observing the outputs. By analyzing the distribution of outputs, one can gain insights into the behavior of the system and make predictions or estimates. Examples in Statistical Analysis "
Features,Module 12 - Basic Artificial Neural Net (ANN).docx,"X contains the input features that the neural network will use to learn and make predictions. Each row in X represents a different sample or data point, and each column represents a different feature."
Few assumptions,Module 09 - Feature Selection.docx,Bootstrapping makes fewer assumptions about the underlying data distribution compared to traditional statistical methods.
Finance,Module 12 - Gradient Descent and Backpropagation.docx,"Portfolio optimization, trading"
Finance,Module 10 - Logarithms.docx,"Compound interest calculations rely heavily on logarithms. Additionally, logarithmic transformations help analyze stock price movements and assess investment risks."
Finance,Module 03 - Distributions.docx,Modeling the size of insurance claims or the time until default on a loan.
Find the percentiles,Module 09 - Feature Selection.docx,"Identify the appropriate percentiles from the distribution of the calculated statistics. For a 95% confidence interval, you would typically find the 2.5th and 97.5th percentiles."
Finding Optimal Solutions,Module 12 - Gradient Descent and Backpropagation.docx,"Convergence indicates that the algorithm has found a set of parameters that (at least locally) minimize the cost function, leading to a well-trained model."
First Hidden Layer,Module 12 - Gradient Descent and Backpropagation.docx,"This layer might focus on extracting basic features like short-term dependencies, part-of-speech tags, or simple word combinations."
First Layer,Module 12 - Gradient Descent and Backpropagation.docx,"You might focus on individual words, sentences, and basic plot points. Identifies ""the"" as an article, ""quick"" as an adjective, ""brown"" as another adjective."
Flexibility,Module 03 - Distributions.docx,The Gamma distribution is very flexible and can take on a variety of shapes depending on the values of its parameters. This makes it suitable for modeling a wide range of phenomena. The Beta distribution is very flexible and can take on a variety of shapes depending on the values of its parameters. This makes it suitable for modeling a wide range of phenomena.
Flexibility,Module 12 - Basic Artificial Neural Net (ANN).docx,"Biases provide additional flexibility to the neural network. They allow the activation function to shift, enabling the neuron to activate even when the weighted sum of inputs is zero or negative."
For example,Module 12 - Basic Artificial Neural Net (ANN).docx,self.weights3 += self.learning_rate * self.hidden_layer2.T.dot(d_output) updates the weights connecting the second hidden layer to the output layer.
Forecasting,Module 13 - Statsmodels Time Series Complete.docx,Predicting future values based on historical trends.
Forward Pass,Module 12 - Basic Artificial Neural Net (ANN).docx,The input data is passed through the network to generate predictions.
Forward Pass,Module 12 - Gradient Descent and Backpropagation.docx,"The input data flows through the network, layer by layer, until it reaches the output layer. Each neuron performs a weighted sum of its inputs, adds a bias, and applies an activation function. outputs = net(X) calculates the network's predictions."
Forward Pass / Signal Propagation,Module 12 - Gradient Descent and Backpropagation.docx,"In both biological and artificial neurons, there's a concept of a ""forward pass"" where information flows from inputs to outputs. In biological neurons, this involves the transmission of electrical signals through dendrites, soma, and axons. In artificial neurons, it involves the weighted sum of inputs and the application of an activation function."
Foundation,Module 12 - Gradient Descent and Backpropagation.docx,"It's a building block for differentiating more complex functions, especially when combined with other rules like the chain rule and the product rule."
Francis Galton (1822-1911),Module 04 - Hypothesis Testing.docx,"Developed concepts of regression and correlation, and his work influenced the field of eugenics."
Frequentist approach,Module 03 - Miscellaneous.docx,"You would flip the coin a large number of times (e.g., 1000) and count the number of times it lands heads up. The proportion of heads in your experiment would be your estimate of the probability of the coin landing heads up."
Frequentists,Module 03 - Miscellaneous.docx,View probability as the long-run frequency of an event occurring. They estimate probabilities based on repeated observations or experiments. Generally avoid using prior information or subjective beliefs in their analysis. They rely solely on the observed data to draw conclusions. Use hypothesis testing and confidence intervals to draw inferences about population parameters. They focus on rejecting or failing to reject a null hypothesis based on the observed data.
Function must be a 1,Module 12 - Gradient Descent and Backpropagation.docx,1 mapping
GLM vs OLS,Module 10 - GLM vs OLS.docx, Fundamentals Key Statistics Libraries 
GLMs,Module 10 - GLMs.docx,Fundamentals Regression and Modeling Link Functions Families Other Important Concepts 
Game Playing,Module 12 - Gradient Descent and Backpropagation.docx,"AlphaGo, AlphaZero (chess, Go), game AI"
Gamma,Module 10 - GLMs.docx,"Used for continuous positive data that is skewed (e.g., income, waiting times)."
Gamma Distribution,Module 10 - Logarithms.docx,Used for time-to-failure or reliability analysis.
"Garbage In, Garbage Out (GIGO)",Module 02 - Data.docx,The quality of the output depends on the quality of the input data.
Gaussian (Normal),Module 10 - GLMs.docx,Used for continuous data that is normally distributed.
Generalized Linear Models (GLMs),Module 10 - GLMs.docx,GLMs are a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.
Generalized Linear Models (GLMs),Module 10 - GLM vs OLS.docx,GLMs are a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.
Generalized Linear Models (GLMs),Module 10 - Logarithms.docx,A flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.
Generate random inputs,Module 09 - Feature Selection.docx,Draw random samples from the specified probability distributions for each input variable.
Global Minimum,Module 10 - Logarithms.docx,The absolute lowest point in the entire function.
Global Minimum,Module 10 - GLMs.docx,The absolute lowest point in the entire function.
Global Optimization,Module 12 - Gradient Descent and Backpropagation.docx,"Backpropagation and gradient descent in artificial neural networks aim to find a global minimum of the cost function. In biological neurons, learning is often more local and distributed, focusing on adapting to specific stimuli and tasks."
Goal,Module 12 - Basic Artificial Neural Net (ANN).docx,Training the Neural Network
Goal,Module 12 - Gradient Descent and Backpropagation.docx,"Minimize the cost function J(w, b) with respect to the weight w and bias b using gradient descent."
Goodness of Fit Test,Module 03 - Distributions.docx,To test how well a theoretical distribution fits a set of observed data.
Goodness-of-Fit Testing,Module 03 - Distributions Fitter.docx,You can use fitter to formally test how well a particular distribution fits your data.
Google's AI Specialist,Module 12 - Basic Artificial Neural Net (ANN).docx,TPUs are custom-designed processors developed by Google specifically for machine learning and AI workloads.
Gradient Calculation,Module 12 - Basic Artificial Neural Net (ANN).docx,"The term (∂/∂θ) J(θ₀, θ₁) represents the gradient of the cost function. This gradient is calculated using backpropagation, which involves the sigmoid derivative (as explained above)."
Gradient Calculation,Module 12 - Gradient Descent and Backpropagation.docx,"Deep learning relies heavily on calculating gradients (derivatives) of the loss function with respect to the model's parameters. In vanilla Python, you'd have to manually implement the backpropagation algorithm to compute these gradients, which can be complex and error-prone."
Gradient Descent,Module 12 - Basic Artificial Neural Net (ANN).docx,The gradients calculated during backpropagation are used to update the weights and biases in the direction that minimizes the error. This is typically done using an optimization algorithm like gradient descent.
Gradient Descent,Module 12 - Gradient Descent and Backpropagation.docx,"Gradient descent is an optimization algorithm used to update the weights and biases in artificial neural networks. It relies on the gradients calculated during backpropagation. While biological neurons adjust their connections based on experience, the mechanism is different from gradient descent. The gradients calculated in the backward pass are used to update the weights and biases in the direction that minimizes the cost function. This is typically done using an optimization algorithm like gradient descent."
Gradient Descent Update,Module 12 - Basic Artificial Neural Net (ANN).docx,"The gradient descent update forumula represents the gradient descent update rule. It describes how the parameters (θ_j) are adjusted based on the gradient of the cost function (J(θ₀, θ₁))."
Gradient Descent and Backpropagation,Module 12 - Gradient Descent and Backpropagation.docx,"Gradient Descent and Backpropagation  Overview Neuron - many inputs one output Activation Functions Cost Function Limits Indeterminate Forms Derivatives Chain Rule Composition Power Rule  Neurons and Neural Nets (a) Biological Neuron This part shows a simplified diagram of a biological neuron, the fundamental building block of our brains. (b) Artificial Neuron This part shows a model of an artificial neuron, which is inspired by the biological neuron but simplified for computational purposes.  The Network The artificial neuron model represents a simple neural network with one layer. In more complex networks, multiple layers of interconnected neurons are used to process information and learn complex patterns. Key Concepts Analogy to Biological Neuron This simplified model of a neuron forms the basis of artificial neural networks, which are powerful tools for machine learning and artificial intelligence.  sigmoid(x) defines the sigmoid activation function, which squashes the input value between 0 and 1. weighted_sum = np.dot(inputs, weights) + bias calculates the weighted sum of inputs and adds the bias. output = sigmoid(weighted_sum) applies the sigmoid activation function to the weighted sum, producing the final output of the neuron. This fundamental building block is used in various combinations and architectures to create complex neural networks capable of learning from data and making predictions.  Weighted Sum The weighted sum is a crucial step in the artificial neuron's computation. It combines the input values with their corresponding weights to determine the overall input to the neuron. Output The output of the neuron is the result of applying the activation function to the weighted sum. The activation function introduces non-linearity and determines whether the neuron should ""fire"" (activate). Example Output The weighted sum (1.0400) represents the combined effect of the inputs based on their weights. The output (0.7391) represents the neuron's activation level after applying the sigmoid function. In this case, the neuron is ""firing"" with a relatively high activation. Key Points The weighted sum combines the inputs with their weights, determining the overall input to the neuron. The activation function transforms the weighted sum into the final output, introducing non-linearity and determining the neuron's activation level. This simple computation forms the basis of artificial neurons, which are the building blocks of neural networks.  Activation Function The activation function is a crucial component of an artificial neuron. It introduces non-linearity and determines the neuron's output based on the weighted sum of its inputs. Common Activation Functions Output Values and Their Implications What does this imply? The activation level of a neuron influences how the information flows through the network. Example with Sigmoid Choosing the Right Activation Function The choice of activation function depends on the specific task and the characteristics of the data. Different activation functions have different strengths and weaknesses, and it often requires experimentation to find the best one for a particular problem.  Relationship to Gradient Descent and Backpropagation While artificial neural networks are inspired by biological neurons, the analogy breaks down when we get to the specifics of gradient descent and backpropagation. These are algorithms used to train artificial neural networks, and they don't have direct biological counterparts. Similarities Differences Key Takeaways Artificial neural networks draw inspiration from biological neurons, but they are simplified models. The concepts of forward pass and learning have some parallels in both types of neurons. Backpropagation and gradient descent are specific to artificial neural networks and don't have direct biological counterparts. Biological learning mechanisms are more complex and less understood than the algorithms used to train artificial neural networks.  Reinforcement Learning Reinforcement learning (RL) is a type of machine learning where an agent learns to interact with an environment by taking actions and receiving feedback in the form of rewards or penalties. It's like teaching a dog a new trick with treats and corrections—the dog learns which actions lead to tasty rewards and which ones don't. 1. Agent The learner and decision-maker. This is the entity that interacts with the environment, like a robot, a game-playing AI, or a software program. 2. Environment Everything the agent interacts with. This could be a physical world (like a maze for a robot) or a virtual world (like a game environment). 3. State A specific situation or configuration of the environment. It provides information about the current context to the agent. 4. Action What the agent can do in a given state. The agent chooses an action from a set of possible actions. 5. Reward Feedback from the environment after the agent takes an action. It's a scalar value that tells the agent how good or bad the action was in that particular state. 6. Policy The agent's strategy for choosing actions in different states. It maps states to actions, essentially defining the agent's behavior. 7. Value Function Estimates the long-term value of being in a particular state or taking a specific action. It helps the agent make decisions that lead to the highest cumulative reward over time. 8. Model (Optional) A representation of the environment. It can be used to predict the next state and reward given the current state and action. This allows the agent to plan ahead and make more informed decisions. How RL Works The goal of the agent is to learn a policy that maximizes the cumulative reward over time. Key Concepts Applications of RL Reinforcement learning is a powerful paradigm for solving problems that involve sequential decision-making in complex environments. It has the potential to create intelligent agents that can learn to perform a wide range of tasks autonomously.  Gradient Descent Machine Learning and the Matrix Machine learning uses matrices To find the parameters of our equation we can use np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) Where X has a bias (OLS requires manual addition of bias or constant) Regression uses several analytical methods such as This can overwhelm a machine computing abilities if X is too large So we use gradient descent to incrementally find our parameters (  Gradient An increase or decrease in the magnitude of a property (e.g. temperature, pressure, or concentration) observed in passing from one point or moment to another.  Minimizing the Cost Function Gradient Descent Algorithm and the Derivative Partial Derivative In mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry. In this case we find the partial derivative with respect to x and hold y as a constant The derivative of a constant is 0 Our Final Formula  Convergence Convergence, in the context of machine learning and optimization algorithms like gradient descent, refers to the process of approaching a stable solution. It means that as the algorithm iterates, the parameters of the model gradually settle towards values that minimize the error or loss function. Convergence in Gradient Descent Indicators of Convergence Importance of Convergence Challenges in Convergence Convergence is a crucial concept in machine learning, signifying that an optimization algorithm has found a stable solution that (locally) minimizes the error. It's an indicator of successful training and provides a stopping criterion for the algorithm. However, achieving convergence can be challenging, and various factors can influence the speed and stability of the process.  Python Example Imagine you're standing on a hillside and want to find the lowest point. You might take a step in the direction that slopes downward most steeply. You'd repeat this process, taking smaller steps as you get closer to the bottom, until you reach a point where you can't go any lower. That's essentially how gradient descent works. In machine learning, our ""hillside"" is the cost function (denoted as J(theta0, theta1) in your case). This function measures the error between our model's predictions and the actual data. Our goal is to find the values of theta0 (intercept) and theta1 (feature weights) that minimize this error. The Process theta0 = theta0 - alpha * (partial derivative of J with respect to theta0) theta1 = theta1 - alpha * (partial derivative of J with respect to theta1) alpha is the learning rate, which controls the size of the steps we take. Simple Python Example  Explanation This code implements gradient descent for linear regression with one feature. It uses mean squared error as the cost function. It iteratively updates the parameters theta0 and theta1 to minimize the cost. The learning rate alpha controls how big the steps are in each iteration. After the gradient descent process, it prints the final values of theta0, theta1, and the cost.  In the code examplepro vided, the cost function being used is the Mean Squared Error (MSE). MSE = (1 / n) * Σ(yi - ŷi)² n is the number of data points yi is the actual value of the target variable for the i-th data point ŷi is the predicted value of the target variable for the i-th data point While MSE is frequently used, it's not the only option. Other cost functions, like Mean Absolute Error (MAE) or Huber loss, might be more suitable depending on the specific problem and the characteristics of the data.  Key Takeaways Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In machine learning, it's used to find the parameters of a model that minimize the error on the training data. The learning rate is a crucial hyperparameter that needs to be tuned carefully. This explanation and example provide a basic understanding of gradient descent. In real-world scenarios, you'll often work with more complex models, datasets, and optimization algorithms. However, the underlying principle of iteratively adjusting parameters to minimize a cost function remains the same.  Problems with the Gradient and Momentum  Imagine you're rolling a ball down a hill. Gradient descent is like letting the ball roll freely, following the steepest slope downwards. Momentum, in this analogy, is like giving the ball an initial push. How Momentum Works In standard gradient descent, the parameter updates are solely based on the current gradient. With momentum, we introduce a ""velocity"" term that accumulates the gradients from previous iterations. This velocity influences the direction and speed of the parameter updates. Benefits of Momentum Mathematical Formulation v_t = β * v_{t-1} + (1 - β) * ∇J(θ_t) θ_{t+1} = θ_t - α * v_t Explanation The velocity v_t is a weighted average of the previous velocity v_{t-1} and the current gradient ∇J(θ_t). The momentum parameter β controls how much weight is given to the previous velocity. A higher β means more weight is given to the past, leading to smoother updates. The learning rate α scales the velocity to determine the step size. Intuition The velocity term accumulates the gradients over time, giving the optimization process ""inertia."" This inertia helps the optimizer move faster in consistent directions and overcome small obstacles in the loss landscape. PyTorch Example optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) This creates an SGD optimizer with a learning rate of 0.01 and a momentum parameter of 0.9. Key Takeaways Momentum is a technique to accelerate and stabilize gradient descent. It introduces a velocity term that accumulates gradients from previous iterations. Momentum can lead to faster convergence, smoother updates, and better solutions. PyTorch provides built-in support for momentum in various optimizers.  Limits Calculus is about rates of change, or the study of continuous change; derivatives lim  Approaching Indeterminacy Getting Both Sides of Asymptote A line that continually approaches a given curve but does not meet it at any finite distance In analytic geometry, an asymptote of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity. In projective geometry and related contexts, an asymptote of a curve is a line which is tangent to the curve at a point at infinity.  Indeterminate Forms In calculus and other branches of mathematical analysis, limits involving an algebraic combination of functions in an independent variable may often be evaluated by replacing these functions by their limits; if the expression obtained after this substitution does not provide sufficient information to determine the original limit, then the expression is called an indeterminate form.  Indeterminate forms are expressions in calculus where the limit cannot be determined simply by evaluating the function at the limiting value. They often arise when we encounter situations like dividing by zero or evaluating expressions that tend towards infinity. 0/0 Indeterminate Form f(x) = (x² - 4) / (x - 2) f(2) = (2² - 4) / (2 - 2) = 0 / 0 This is an indeterminate form because 0/0 doesn't have a definite value. We can't determine the limit just by plugging in the value. Why is it indeterminate? These conflicting tendencies create ambiguity, making it necessary to use other techniques to evaluate the limit. How to resolve it f(x) = (x² - 4) / (x - 2) = (x + 2)(x - 2) / (x - 2) = x + 2  (for x ≠ 2) lim (x → 2) f(x) = lim (x → 2) (x + 2) = 2 + 2 = 4 Other Indeterminate Forms ∞/∞ ∞ - ∞ 0 × ∞ 1∞ 00 ∞0 Key Takeaways Indeterminate forms arise when the limit cannot be determined by direct substitution. They often involve conflicting tendencies or expressions that tend towards zero or infinity. Techniques like factoring, L'Hôpital's rule, or algebraic manipulation are used to resolve indeterminate forms and evaluate the limits. Understanding indeterminate forms is crucial for analyzing the behavior of functions and solving various calculus problems.  In calculus, a limit describes the behavior of a function as its input approaches a certain value. It's about what happens near a point, not necessarily at the point itself. Let's consider the function y = x² and see what happens as x gets closer and closer to 2. If we plug in values of x slightly smaller than 2 (like 1.9, 1.99, 1.999), we see that y gets closer and closer to 4. Similarly, if we plug in values of x slightly larger than 2 (like 2.1, 2.01, 2.001), y also gets closer and closer to 4. lim (x → 2) x² = 4 Indeterminacy Indeterminacy arises when we can't directly determine the limit by simply plugging in the value. This often happens when we encounter expressions like 0/0, ∞/∞, ∞ - ∞, etc. These are called indeterminate forms because they don't have a definite value. Asymptotes An asymptote is a line that a curve approaches as it heads towards infinity. Limits and Minimizing the Cost Function In the context of minimizing a cost function, limits play a role in understanding the behavior of the function as we adjust the parameters. Indeterminate Forms in Cost Function Minimization While not as common with simple cost functions like MSE for linear regression, indeterminate forms can arise in more complex optimization scenarios. For example, if the cost function involves ratios or exponentials, you might encounter situations where the gradient becomes indeterminate at certain points. Techniques like L'Hôpital's rule can sometimes be used to evaluate these limits and continue the optimization process. Key Takeaways Limits help us understand the behavior of functions as their inputs approach certain values. Indeterminate forms arise when we can't directly determine the limit by substitution. Asymptotes describe the behavior of functions as they approach infinity. In minimizing cost functions, limits play a role in understanding convergence, identifying minima, and adjusting step sizes.  Differentiation Key Ideas in Differentiation Why is Differentiation Important? Notation dy/dx (Leibniz's notation) f'(x) (Lagrange's notation) y' (Newton's notation) Using the power rule (a shortcut for differentiating power functions), we get dy/dx = 2x. This means that at any point x, the slope of the line tangent to the curve y = x² is 2x. For instance, at x = 3, the slope of the tangent line is 2 * 3 = 6. Key Takeaways Differentiation finds the instantaneous rate of change of a function. It gives you the slope of the tangent line at any point on the curve. Derivatives have wide-ranging applications in many fields. The power rule is a useful shortcut for differentiating power functions.  Derivatives Slopes, Secant and Tangent Lines, and Derivatives  The derivative of a function tells you how much the output of that function changes with respect to an infinitesimally small change in its input. It essentially measures the instantaneous rate of change. Let's take the function y = x². The derivative of this function is dy/dx = 2x. What does this mean? It means that at any point x, the slope of the line tangent to the curve y = x² is 2x. For instance, at x = 3, the slope of the tangent line is 2 * 3 = 6. This tells us that if we increase x slightly from 3, y will increase approximately 6 times that amount. Relationship with MSE and Minimizing the Cost Function In the context of gradient descent and MSE, derivatives are crucial for finding the direction of steepest descent. This code uses the sympy library for symbolic calculations. It defines the function y = x², calculates its derivative, and then evaluates the derivative at x = 3. Derivatives help us understand how a function changes. In machine learning, we use derivatives (gradients) of the cost function (like MSE) to find the direction to adjust our model parameters in order to minimize the error. Gradient descent is an iterative process that repeatedly uses derivatives to update the parameters and move towards the minimum of the cost function.  m = rise/run  Slope of Tangent Line m= lim x→α f(x)−f(a) x−a Alternative Equation m= lim h→0 f(a+h)−f(h) h  The Derivative  1. Limits 2. Secant Lines 3. Tangent Lines 4. Derivatives 5. Slopes The slope of a secant line gives the average rate of change of the function over an interval. As the two points on the secant line get closer and closer together (taking a limit), the secant line approaches the tangent line. The slope of the tangent line gives the instantaneous rate of change of the function at a specific point. The derivative of a function gives us a way to calculate the slope of the tangent line at any point. This code will generate a graph of y = x² along with a secant line and a tangent line at x=2, helping you visualize the concepts.   The Power Rule We find the derivative of a function using the Power Rule In this case we find the partial derivative with respect to x and hold y as a constant The derivative of a constant is 0  The power rule is a fundamental rule in calculus that provides a shortcut for finding the derivative of functions that involve powers of x (like x², x³, x⁵, etc.). The Power Rule Formula dy/dx = n * x^(n-1) Applying the Power Rule to y = x² Therefore, the derivative of y = x² is dy/dx = 2x. Explanation The power rule essentially tells us that to find the derivative of a power function, we multiply the function by the original exponent and then reduce the exponent by 1. Why is the Power Rule Useful? Examples The power rule is a powerful tool that simplifies the process of differentiation, making it essential for anyone studying calculus and its applications in various fields.  Chain Rule Let's imagine a scenario where we're training a neural network with a single neuron. This neuron takes an input x, multiplies it by a weight w, adds a bias b, and then applies a sigmoid activation function to produce an output y. The Model The Cost Function J(w, b) = (1/2) * (y - t)² (where t is the target value) Applying the Chain Rule To update w and b using gradient descent, we need to calculate the partial derivatives of J with respect to w and b. This is where the chain rule comes in. ∂J/∂w = ∂J/∂y * ∂y/∂z * ∂z/∂w ∂J/∂y = (y - t) ∂y/∂z = σ(z) * (1 - σ(z)) (derivative of sigmoid) ∂z/∂w = x ∂J/∂b = ∂J/∂y * ∂y/∂z * ∂z/∂b ∂J/∂y = (y - t) ∂y/∂z = σ(z) * (1 - σ(z)) ∂z/∂b = 1 Gradient Descent Update w = w - α * ∂J/∂w b = b - α * ∂J/∂b (where α is the learning rate) We used the chain rule to break down the differentiation of the cost function into smaller, manageable steps. This allowed us to calculate the gradients needed for updating the neuron's weights and bias during gradient descent. This example demonstrates how the chain rule plays a fundamental role in training neural networks, enabling us to efficiently compute gradients and minimize the cost function, even with complex, layered architectures.  Backpropagation   Minimizing the Cost Function The core goal of backpropagation is to minimize the cost function. By repeatedly adjusting the weights and biases based on the calculated gradients, the network gradually learns to make more accurate predictions.  Relationship with Gradient Descent Backpropagation and gradient descent work hand-in-hand. Backpropagation calculates the gradients (direction and magnitude of change) of the cost function with respect to the weights and biases. Gradient descent then uses these gradients to update the parameters, taking steps in the direction that minimizes the cost.  Python Example This example demonstrates a simple neural network with one hidden layer trained using backpropagation and gradient descent. It showcases the key steps involved in the process, including the forward pass, error calculation, backward pass, and parameter updates.  The final output in the Python example I provided represents the neural network's predictions after it has been trained using backpropagation and gradient descent. [[0], [1], [1], [0]] [[0.02], [0.97], [0.98], [0.03]] This indicates that the network has learned to approximate the XOR function reasonably well, with the output values close to the expected 0s and 1s. Interpreting the Output Each value in the final output corresponds to the network's prediction for one of the four input combinations. Values close to 0 indicate that the network predicts the XOR output to be 0. Values close to 1 indicate that the network predicts the XOR output to be 1. The actual output values may vary slightly each time you run the code due to the random initialization of weights and biases. The accuracy of the network can be improved by increasing the number of epochs, adding more layers or neurons, or using a more sophisticated optimization algorithm. In essence, the final output shows how well the neural network has learned to solve the XOR problem after being trained with backpropagation and gradient descent.  Next Word Prediction  A simple vocabulary of words is defined. Word-to-index and index-to-word mappings are created for converting between words and numerical representations. Training data (sentences) is provided. create_training_examples function generates input-target pairs from the sentences. SimpleRNN class implements a basic recurrent neural network. An instance of SimpleRNN is created. The network is trained for a specified number of epochs. In each epoch, the training data is processed, loss is calculated, and parameters are updated. predict_next_word function takes seed text as input and predicts the next word(s). It performs a forward pass through the trained network and returns the most likely word(s) based on the output probabilities.  Weights and Biases Hidden Layer Variables In Summary These variables and parameters work together to define the structure and behavior of the RNN. The weights and biases determine the strength of connections between layers, while the hidden layer variables capture the internal state and transformations of the network as it processes the input sequence. The forward pass calculates the hidden states (hs) and output probabilities (ps), while the backward pass calculates the gradients of the cost function with respect to the weights and biases (Wxh, Whh, Why, bh, by), allowing the network to learn and improve its predictions over time.  What are n-grams? N-grams are contiguous sequences of n items from a given sequence of text or speech. In the context of word prediction, these items are words. And so on... The provided Python code implicitly uses bigrams for training. The provided code implicitly uses bigrams for training the word prediction model. You can extend it to use higher-order n-grams to capture more context and potentially improve the prediction accuracy. However, be mindful of the potential limitations of n-gram models, especially data sparsity.  PyTorch  1. Ease of Use 2. Flexibility and Dynamic Computation Graphs 3. Strong GPU Support 4. Extensive Ecosystem and Community 5. Wide Adoption in Research and Industry  PyTorch is built on top of Python and it offers significant advantages over ""vanilla"" Python when it comes to building and training machine learning models, especially deep learning models. 1. Automatic Differentiation 2. GPU Acceleration 3. Optimized Tensor Operations 4. Neural Network Modules 5. Ecosystem and Community While you could technically implement deep learning models in vanilla Python, it would be significantly more complex, less efficient, and prone to errors. PyTorch provides the tools and infrastructure to streamline the development and training of deep learning models, making it a preferred choice for researchers and practitioners.  PyTorch Example  Net class defines a simple feedforward neural network with one hidden layer. torch.randn creates random input data (X) and target values (y) for demonstration. net = Net() creates an instance of the neural network. criterion = nn.MSELoss() defines the loss function (MSE for regression). optimizer = optim.SGD(net.parameters(), lr=0.01) creates an optimizer (SGD) to update the network's parameters with a specified learning rate. Iterates for a specified number of epochs. optimizer.zero_grad() resets the gradients from the previous iteration. loss.backward() performs backpropagation, calculating gradients of the loss with respect to the parameters. optimizer.step() updates the weights and biases based on the calculated gradients. This example demonstrates the core principles of gradient descent and backpropagation in PyTorch, showcasing how to define, train, and optimize a neural network for a regression task. You can modify this code to experiment with different network architectures, datasets, and optimization algorithms.  This code defines an RNN model in PyTorch to predict the next word in the sentence ""the quick brown fox jumped over the lazy dog"". It uses nn.RNN, nn.Embedding, and nn.Linear layers to process the input sequence and generate predictions. The model is trained using the Adam optimizer and CrossEntropyLoss. After training, the predict function can be used to generate text based on a given seed text. This example demonstrates a basic approach to word prediction with RNNs in PyTorch. You can experiment with different RNN architectures, hyperparameters, and datasets to improve the model's performance.  Hidden Layers  General Interpretation Specific to the Example ""the"" is often followed by an adjective or noun. ""quick"" is often followed by ""brown"" in this specific sentence. ""over"" is often followed by ""the"". Important Notes In Summary The hidden layers in an RNN for word prediction learn to extract relevant features, build a sequential representation of the text, and capture increasingly abstract relationships between words. While it's challenging to fully understand the inner workings of these layers, they play a crucial role in enabling the network to make accurate predictions.  Analogy Imagine you're trying to understand a story. The two layers work together to create a deeper understanding of the story. Example Interpretation Important Considerations In Summary Adding a second hidden layer to the RNN can enhance its ability to learn complex patterns and build deeper representations of the input sequence. This can lead to improved performance in word prediction and other sequence modeling tasks, but it also introduces challenges that need to be addressed during training.  Yes, there are activation functions used during training, but they might not be explicitly defined in the code you provided because they are built into the PyTorch modules. 1. Within the RNN layers (nn.RNN) The nn.RNN module in PyTorch uses the tanh activation function by default within each RNN cell. This activation function is applied to the hidden state at each time step to introduce non-linearity and help the network learn complex patterns in the sequence. 2. Implicitly in the loss function (nn.CrossEntropyLoss) The nn.CrossEntropyLoss loss function combines the softmax activation function with the cross-entropy loss calculation. The softmax function takes the raw logits (unnormalized scores) produced by the model and converts them into probabilities for each word in the vocabulary. The cross-entropy loss then measures the difference between these predicted probabilities and the true target word. Why no explicit activation in the output layer? As mentioned earlier, there's no explicit activation function in the output layer (self.fc) because the nn.CrossEntropyLoss already handles the softmax activation. Adding another activation function on top of softmax would be redundant and might interfere with the learning process. Even though you don't see activation functions explicitly defined in every part of the code, they are still crucial for the training process. The tanh activation is used within the RNN cells, and the softmax activation is implicitly applied within the nn.CrossEntropyLoss function. These activations introduce non-linearity and enable the network to learn complex relationships in the sequential data.  Testing (Predicting) Activation Function There is no explicit activation function being applied within the predict function itself. The predict function focuses on generating the next word based on the model's output. The necessary activations are already applied within the model's forward method, so there's no need to apply them again in the prediction function.  Loss Functions  MSE  Cross Entropy What is Cross-Entropy? How it Works Let's say you have a binary classification problem (like the one in your code). Cross-Entropy = - (y * log(ŷ) + (1 - y) * log(1 - ŷ)) Why is it Used as a Loss Function? Relationship with Activation Functions These activation functions produce output values that can be interpreted as probabilities, which are then used in the cross-entropy calculation. In Your Code In your code, you used nn.CrossEntropyLoss. This loss function combines the softmax activation with the cross-entropy loss. So, while you don't explicitly apply softmax in your output layer, it's handled internally by the loss function. Key Takeaway Cross-entropy is a loss function, not an activation function. It measures the difference between probability distributions and is commonly used in classification tasks to train neural networks. It's often paired with activation functions like sigmoid or softmax to ensure the model outputs probabilities.  Optimization Functions  Gradient Descent  Stochastic Gradient Descent  Adam Adam is a popular optimization algorithm used in training neural networks. It stands for Adaptive Moment Estimation. It's an extension of gradient descent that combines the ideas of momentum and adaptive learning rates. How Adam Works Benefits of Adam Mathematical Formulation m_t = β_1 * m_{t-1} + (1 - β_1) * g_t  # First moment (momentum) v_t = β_2 * v_{t-1} + (1 - β_2) * g_t²  # Second moment (adaptive learning rate) m_t_hat = m_t / (1 - β_1^t)  # Bias correction for first moment v_t_hat = v_t / (1 - β_2^t)  # Bias correction for second moment θ_{t+1} = θ_t - α * m_t_hat / (√v_t_hat + ε)  # Parameter update PyTorch Example optimizer = optim.Adam(model.parameters(), lr=0.001) This creates an Adam optimizer with a learning rate of 0.001 and default values for the other hyperparameters. Key Takeaways Adam is an adaptive optimization algorithm that combines momentum and adaptive learning rates. It often converges faster and is more robust than standard gradient descent. Adam is widely used in deep learning due to its efficiency and effectiveness. PyTorch provides a convenient implementation of Adam in the optim.Adam optimizer.   "
Gradient Descent and Backpropagation,Module 12 - Gradient Descent and Backpropagation.docx,"This image illustrates the concept of an artificial neuron and draws a comparison to its biological counterpart. Let's break down each part While the artificial neuron is a simplified model, it captures some essential aspects of biological neurons Explanation Sigmoid Function Inputs, Weights, and Bias Weighted Sum Activation Output The code will print the calculated weighted sum and the output of the neuron after applying the sigmoid activation. For example This simple example demonstrates the basic computation within an artificial neuron In the code In the code There are many different activation functions, each with its own characteristics. Here are a few common ones The output of the activation function determines the neuron's activation level. Here's how to interpret the output values In the previous Python example, we used the sigmoid activation function. If the output is Here's a breakdown of the similarities and differences Here's a breakdown of the key components and concepts in reinforcement learning The agent interacts with the environment in a loop Here's a more detailed breakdown In summary Here's why MSE is a common choice for the cost function, especially in regression problems The Formula The MSE formula is where In the code, this is calculated within the cost_function The update rule for gradient descent with momentum is where In PyTorch, you can easily add momentum to optimizers like SGD Here's a classic example Consider the function If we try to find the limit as x approaches 2 by directly substituting x = 2, we get In this case, we can factor the numerator and simplify the expression Now, we can find the limit There are several other indeterminate forms, including Therefore, we say that the limit of y = x² as x approaches 2 is 4. Mathematically, we write this as Differentiation is a fundamental concept in calculus that deals with finding the instantaneous rate of change of a function. Think of it like this Differentiation has numerous applications in various fields If you have a function y = f(x), its derivative can be denoted in several ways Let's differentiate y = x² In essence Let's break down these calculus concepts using the example of y = x² Connecting it all If y = xⁿ (where n is any real number), then the derivative dy/dx is Apply the formula We'll use the Mean Squared Error (MSE) as our cost function 1. Partial Derivative of J with respect to w 2. Partial Derivative of J with respect to b Now that we have the partial derivatives, we can update w and b using gradient descent In essence Backpropagation is the backbone of neural network training. It's the algorithm that allows networks to learn from data and improve their accuracy over time. Here's how it works Let's break it down Ideally, the final output should be very close to the actual XOR outputs However, due to the simplicity of the network and the limited training, the output might not be perfect. You might see values like Important Notes Explanation Data Preparation RNN Class Training Prediction Output The code will print the loss during training and the predicted words for the given seed text. For example N-gams are relevant and can be applied to the training data in the word prediction example. Here's how How n-grams apply to the training data Using Higher-Order n-grams You could modify the code to use higher-order n-grams (trigrams, 4-grams, etc.) by changing how the input-target pairs are generated. For example, to use trigrams Benefits of using n-grams Limitations In summary PyTorch has become incredibly popular in the machine learning community. Here's a breakdown of its importance and why it's widely used Here's why you'd choose PyTorch over vanilla Python for these tasks In essence Explanation Define the Neural Network Generate Sample Data Instantiate Components Training Loop Backward Pass and Optimization Key PyTorch Concepts It's challenging to give a precise, human-interpretable explanation of what's happening in each hidden layer of an RNN, especially as the network gets more complex. However, we can provide some general insights based on how RNNs and word prediction work In the provided code, the RNN has a single hidden layer with 50 neurons. Here's a possible interpretation of what might be happening With two hidden layers, the RNN can potentially learn more complex and nuanced patterns in the sequence. Here's how the interpretation might expand Hierarchical Feature Extraction In the context of ""the quick brown fox..."", the two layers might learn Here's a breakdown of where activation functions are applied during training In summary Here's why In summary Cross-entropy calculates the average number of bits needed to represent an event from the true distribution using the predicted distribution. The formula for binary cross-entropy is Cross-entropy is often used in conjunction with activation functions like Adam calculates individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Here's a breakdown The update rule for Adam is quite involved, but here's a simplified version where In PyTorch, you can use the optim.Adam optimizer"
Gradient Properties,Module 12 - Gradient Descent and Backpropagation.docx,"It has favorable gradient properties for training neural networks, leading to efficient learning."
Gradient of MSE,Module 12 - Gradient Descent and Backpropagation.docx,"To minimize MSE, we need to find where its derivative is zero. This is where the concept of the gradient comes in. The gradient is a vector that points in the direction of the greatest rate of increase of the function. In our case, it points in the direction of the steepest increase of the MSE."
Grammatical roles,Module 12 - Gradient Descent and Backpropagation.docx,"Whether a word is a noun, verb, adjective, etc."
Healthcare,Module 05 - EDA.docx,"A healthcare professional might use storytelling to communicate the effectiveness of a new treatment to patients, using data to show how it has improved outcomes for others."
Healthcare,Module 12 - Gradient Descent and Backpropagation.docx,"Personalized treatment plans, drug discovery"
Heatmaps,Module 05 - EDA.docx,To visualize the correlation matrix between multiple variables.
Hidden Layer,Module 12 - Gradient Descent and Backpropagation.docx,The neurons in the hidden layer learn to combine the information from the embedding layer and the previous hidden state. They might be detecting patterns like:
High Input,Module 12 - Basic Artificial Neural Net (ANN).docx,"If the weighted sum is a large positive value, the sigmoid function outputs a value close to 1, indicating high activation."
High Throughput,Module 12 - Basic Artificial Neural Net (ANN).docx,"They offer very high throughput for matrix operations, leading to faster training and inference of deep learning models."
Histograms,Module 05 - EDA.docx,To visualize the distribution of a single variable.
Hodrick-Prescott (HP) Filter,Module 13 - Pandas Time Series Complete.docx,A mathematical technique used to separate the trend from the cyclical component in time series data. It helps to identify the underlying long-term direction of the data. The document provides a clear explanation of this filter and its applications.
Holt-Winters,Module 13 - Statsmodels Time Series Complete.docx,"EWMA has just one smoothing factor and doesn't account for trend, seasonality, etc. HW offers three smoothing factors for level, trend, and season and can adjust for divisions per cycle (L). Offers Single, Double (Holt and Trend), and Triple (Holt-Winters and Seasonality) Exponential Smoothing."
Horizontal Asymptote,Module 12 - Gradient Descent and Backpropagation.docx,"A horizontal asymptote occurs where the function approaches a constant value as x approaches infinity (or negative infinity). For example, the function y = (2x + 1) / x has a horizontal asymptote at y = 2."
How it works,Module 09 - Feature Selection.docx,"Elastic Net adds both the L1 (Lasso) and L2 (Ridge) penalties to the loss function during model training. The L1 penalty encourages sparsity (forcing some coefficients to zero), while the L2 penalty encourages grouping (keeping correlated features together)."
Hypothesis Testing,Module 03 - Distributions.docx,To test hypotheses about the variance and standard deviation of a normal distribution.
Hypothesis Testing,Module 04 - Hypothesis Testing.docx,"The document ""Module 04 Hypothesis Testing.ipynb - Colab.pdf"" provides a comprehensive overview of hypothesis testing, a fundamental concept in inferential statistics. It covers a wide range of topics, from the historical development of statistical thinking to the practical application of various hypothesis tests. Bernoulli's Fallacy Highlights the distinction between sampling probabilities (aleatory probabilities) and inferential probabilities (epistemic probabilities). Emphasizes the importance of considering background information and assumptions in probabilistic inference. Discusses the historical context of statistics and its use in political agendas, including eugenics. Historical Figures and Concepts Hypothesis Testing The document also includes various examples and code snippets to illustrate these concepts and their applications in data analysis and decision-making.  "
Hypothesis Testing,Module 04 - Hypothesis Testing.docx,Here's a detailed outline and summary of the key elements and terms
Hypothesis Testing,Module 01 - Introduction.docx,A statistical method used to make inferences about a population based on sample data.
Hypothesis and AB Testing,Module 04 - Hypothesis and AB Testing.docx,"Hypothesis Testing A/B testing is a randomized experimentation process wherein two or more versions of a variable are shown to different segments of website visitors at the same time to determine which version drives business metrics best. Population is the entire group of individuals or objects that a study is interested in. Sample is a subset of the population that is selected for study. Sample mean is the average of the values in a sample. Sample variability is the degree to which the values in a sample differ from each other. Null hypothesis is a statement that there is no difference between two groups or that there is no relationship between two variables. Confidence interval is a range of values that is likely to contain the true population parameter. Type I error is the rejection of a true null hypothesis. Type II error is the failure to reject a false null hypothesis. P-value is the probability of obtaining a test statistic as extreme as or more extreme than the one observed, assuming that the null hypothesis is true. Statistical significance is a measure of how likely it is that an observed effect is due to chance. Statistical power is the probability that a test will correctly reject a false null hypothesis. Minimum detectable effect is the smallest effect size that a study is designed to detect. Practical significance is a measure of whether an effect is large enough to be meaningful in the real world. Probability Density Function (PDF) The PDF is a function that describes the probability of a random variable taking on a given value. Cumulative Density Function (CDF) The CDF is a function that describes the probability that a random variable will take on a value less than or equal to a given value. Percent Point Function (PPF) The PPF is the inverse of the CDF. It gives the value of the random variable for which the CDF has a given value. Parametric and Non-Parametric Tests Parametric tests are statistical tests that make assumptions about the distribution of the data. Non-parametric tests are statistical tests that do not make assumptions about the distribution of the data. t-Test Student's t-test is a statistical test used to determine if there is a significant difference between the means of two groups. One-sample t-test is used to test whether the mean of a single sample is equal to a specified value. Independent two-sample t-test is used to test whether the means of two independent samples are equal. z-Test A statistical test used to determine if there is a significant difference between the means of two groups when the variances are known. Chi-Square Test A statistical test used to determine if there is a significant association between two categorical variables. ANOVA (Analysis of Variance) A statistical test used to compare the means of two or more groups. Kruskal-Wallis Test A non-parametric test used to compare the distributions of two or more groups. A/A Testing A type of A/B testing where both groups are given the same treatment to validate the test setup. AB Testing A statistical method used to compare two or more versions of a webpage, app, or other product to see which one performs better. Overall Evaluation Criterion (OEC) A quantitative measure of the experiment's objective. Gaurdrail Metrics Business metrics designed to indirectly measure business value and provide alerts about any potentially misleading or erroneous results and analysis. Randomization Unit A who or what randomly assigned to a group. Data Leakage (Interference) The behavior of the control group is influenced by the treatment given to the test group. SUTVA Assumptions The Stable Unit Treatment Value Assumption (SUTVA) is a key assumption that is usually made in causal inference. Minimum Detectable Effect (MDE) The smallest improvement you are willing to be able to detect. Practical, or Substantive, Significance The importance, or meaningfulness, of a statistically significant result. Significance Level The probability of rejecting a null hypothesis when it is true. Statistical Power The probability of correctly identifying the effect when there is one. Effect Size A quantitative measure of the magnitude of a phenomenon. Pooled Variance A method for estimating the variance of a population when the variances of different samples from that population are assumed to be equal. Conversion Rates The percentage of users who take a desired action. Lift The percentage change in a metric between a test group and a control group. Discrete Metrics Metrics that can only take on a finite number of values. Continuous Metrics Metrics that can take on any value within a given range. Kolmogorov-Smirnov Test for Normality A statistical test used to determine if a dataset follows a normal distribution. Chi-Square Test for Independence A statistical test used to determine if there is a significant association between two categorical variables. Sample Ratio Mismatch (SRM) A phenomenon that can occur in A/B testing when the ratio of users in the control group to users in the treatment group is not what was intended. Fisher's Exact Test A statistical test used to determine if there is a significant association between two categorical variables when the sample size is small. Welch's t-test A statistical test used to compare the means of two groups when the variances of the two groups are not assumed to be equal. OOP (Object-Oriented Programming) A programming paradigm that is based on the concept of objects, which can contain data and code. Python Builtins Functions and constants that are available in Python without the need to import any modules. "
Hypothesis and AB Testing,Module 04 - Hypothesis and AB Testing.docx,Here are brief definitions and explanations for the terms and concepts presented in the document You're absolutely correct! I apologize for missing those definitions in my previous response. Here are the definitions and explanations for the terms and concepts you pointed out
Hypothesis testing,Module 09 - Feature Selection.docx,Simulate data under a null hypothesis to determine the p-value and assess statistical significance.
IID,Module 03 - Distributions.docx,Independent and Identically Distributed
Identify n,Module 12 - Gradient Descent and Backpropagation.docx,"In our example, y = x², the exponent n is 2."
Identifying Contributions,Module 12 - Basic Artificial Neural Net (ANN).docx,Each team analyzes how their work contributed to the error.
Imbalanced Binary Classification,Module 11 - Precision Recall.docx,"A classification problem where one class (the majority class) has significantly more instances than the other class (the minority class). In the context of the document, it refers to credit card fraud detection, where fraudulent transactions are much rarer than legitimate ones."
Imperative Programming,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch uses an imperative programming style, which means you can execute code and see the results immediately. This makes debugging and experimentation much easier compared to frameworks with static computation graphs."
Importance of Statistics,Module 02 - Data.docx,"Statistics is used to summarize data, make informed decisions, answer research questions, recognize patterns, and evaluate the effectiveness of interventions."
In our example,Module 12 - Gradient Descent and Backpropagation.docx,"As x gets closer and closer to 2, y = x² gets closer and closer to 4. We write this as: lim (x → 2) x² = 4 Imagine drawing a line that cuts through the curve of y = x² at two points, say x = 1 and x = 3. This is a secant line. Imagine zooming in really close to the curve y = x² at the point x = 2. The tangent line at that point would just graze the curve, having the same slope as the curve at x = 2. The derivative of y = x² is dy/dx = 2x. This means that at any point x, the slope of the tangent line to the curve is 2x. For example, at x = 2, the slope of the tangent line is 2 * 2 = 4."
In simpler terms,Module 03 - Miscellaneous.docx,"It's mistakenly thinking that just because a piece of evidence is more likely to be found if someone is guilty, finding that evidence automatically means they are highly likely to be guilty."
In summary,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch's combination of ease of use, flexibility, GPU support, a rich ecosystem, and a strong community has made it a leading deep learning framework for both research and production. It empowers users to build and train complex models efficiently while providing the flexibility to explore new frontiers in AI."
Increased Capacity,Module 12 - Gradient Descent and Backpropagation.docx,Two hidden layers give the network more capacity to learn and store information. This can be beneficial for longer sequences or more complex language tasks.
Independence of Observations,Module 08 - Assumptions.docx,The observations in the dataset should be independent of each other.
Independent Two-Sample t-Test,Module 04 - The Tests.docx,To test if there is a significant difference between the means of two independent groups.
Industry,Module 12 - Gradient Descent and Backpropagation.docx,"Companies like Facebook, Tesla, and Uber utilize PyTorch for various applications, from natural language processing and computer vision to self-driving cars and robotics."
Inference,Module 02 - Descriptive Statistics.docx,The process of drawing conclusions about a population based on a sample of data.
Inference,Module 02 - Data.docx,The process of drawing conclusions about a population based on a sample of data. It distinguishes between a parameter (characteristic of a population) and a statistic (characteristic of a sample).
Initialization,Module 12 - Gradient Descent and Backpropagation.docx,Start with initial guesses for theta0 and theta1.
Input (X),Module 12 - Basic Artificial Neural Net (ANN).docx,The input data is passed as an argument to the forward method. This data represents the features of the input samples.
Input-Target Pairs,Module 12 - Gradient Descent and Backpropagation.docx,"The create_training_examples function generates pairs of (input word, target word). This is essentially creating bigrams from the training sentences. For example, the sentence ""the quick brown fox"" generates the bigrams (""the"", ""quick""), (""quick"", ""brown""), (""brown"", ""fox"")."
"Inputs (x₁, x₂, ..., xₙ)",Module 12 - Gradient Descent and Backpropagation.docx,"These are the input values to the neuron, analogous to the signals received by the dendrites of a biological neuron. Each input has an associated weight (w₁, w₂, ..., wₙ)."
Inputs and Weights,Module 12 - Gradient Descent and Backpropagation.docx,"Similar to how dendrites receive signals with varying strengths, the artificial neuron receives inputs with associated weights."
Interpretability,Module 12 - Gradient Descent and Backpropagation.docx,"MSE is easy to understand. It directly measures the average squared difference between the predicted and actual values, giving a clear indication of how well the model is fitting the data."
Interpretability,Module 12 - Basic Artificial Neural Net (ANN).docx,The output can be interpreted as an activation level or a probability.
Interpretation,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","While R-squared and related measures are valuable tools, it's important to interpret them in the context of the specific data and research question. A high R-squared doesn't necessarily mean that the model is a good one, and a low R-squared doesn't necessarily mean that the model is bad."
"Intersections, Unions, and Conditional Probability",Module 03 - Probability.docx,Concepts for calculating probabilities involving multiple events.
Introduction,Module 01 - Introduction.docx, 1. Introduction 2. A Brief History 3. Data 
Introduction,Module 03 - Probability.docx,The Monty Hall Problem and Sample Space
Intuition,Module 12 - Gradient Descent and Backpropagation.docx,"A lower cross-entropy value indicates a higher similarity between the predicted and true distributions, meaning the model is making better predictions."
Inverse Gaussian,Module 10 - GLMs.docx,Used for continuous positive data with a specific type of skewness.
Irreducible Error,Module 11 - Bias Variance Tradeoff.docx,"This is the inherent uncertainty or noise in the data itself, which cannot be eliminated regardless of the model used."
Iterative Process,Module 12 - Gradient Descent and Backpropagation.docx,Gradient descent is an iterative algorithm that repeatedly updates the model's parameters to find the values that minimize the cost function.
Jacob Bernoulli (1655-1705),Module 04 - Hypothesis Testing.docx,Introduced the Law of Large Numbers and used urn drawing examples to illustrate probability concepts.
Jarque-Bera,Module 08 - Coefficients and Metrics.docx,Another test for skewness and kurtosis
John Arbuthnot (1667-1735),Module 04 - Hypothesis Testing.docx,Investigated the sex ratio at birth and argued for divine providence based on observed patterns.
K-fold Cross-Validation,Module 11 - Cross Validation.docx,"A specific cross-validation technique where data is divided into k equal-sized folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold used as the test set once."
Karl Pearson (1857-1936),Module 04 - Hypothesis Testing.docx,Developed Pearson's Correlation Coefficient (r) and contributed to the concept of the p-value.
Key takeaway,Module 03 - Miscellaneous.docx,"The Prosecutor's Fallacy highlights the importance of considering all relevant evidence and probabilities, including prior probabilities, when assessing guilt or innocence."
Kurtosis,Module 08 - Coefficients and Metrics.docx,A measure of the shape of the data
Labels,Module 12 - Basic Artificial Neural Net (ANN).docx,y contains the corresponding target output or labels for each sample in X. These labels represent the desired outcome or prediction that the neural network should learn to produce.
Lagged Value,Module 13 - Statsmodels Time Series Complete.docx,A lagged value is simply a previous value in the series.
Laplace’s Saturn Data with residuals - https,Module 06 - Astronomy and Statistics.docx,//www.britannica.com/biography/Pierre-Simon-marquis-de-Laplace
Large Sample Size,Module 08 - Assumptions.docx,Logistic regression generally requires a larger sample size compared to linear regression for reliable results.
Large Steps,Module 12 - Basic Artificial Neural Net (ANN).docx,"A large learning rate means you take big steps, which can be faster but might cause you to overshoot the bottom and bounce around."
Law of Large Numbers,Module 01 - Introduction.docx,"A statistical principle stating that as the number of trials increases, the sample average converges to the expected value."
Leaf/Terminal Node,Module 11  - Trees.docx,A node with no children or sub-nodes.
Learn,Module 12 - Gradient Descent and Backpropagation.docx,The agent updates its policy and/or value function based on the reward received.
Learn Anything,Module 01 - Introduction.docx,"This section emphasizes the importance of continuous learning in data science, including consuming content, seeking mentorship, practicing skills, and teaching others."
Learning,Module 12 - Basic Artificial Neural Net (ANN).docx,"The weights are the primary parameters that the neural network learns during training. By adjusting these weights through backpropagation and gradient descent, the network learns to map the input features to the desired output."
Learning / Synaptic Plasticity,Module 12 - Gradient Descent and Backpropagation.docx,"Both types of neurons exhibit a form of learning. In biological neurons, learning happens through changes in the strength of synapses (synaptic plasticity). In artificial neurons, learning happens through adjustments of weights and biases during training."
Learning Rate,Module 12 - Basic Artificial Neural Net (ANN).docx,The learning rate controls the size of the steps taken during the parameter updates.
Learning Word Relationships,Module 12 - Gradient Descent and Backpropagation.docx,"The RNN learns the relationships between these bigrams during training. It learns to predict the next word (""target"") given the current word (""input""). This is a form of n-gram language modeling where the model learns the probability of a word given its preceding word(s)."
Learning rate schedules,Module 12 - Basic Artificial Neural Net (ANN).docx,Gradually decrease the learning rate over time as the training progresses.
Limited Context,Module 12 - Gradient Descent and Backpropagation.docx,N-grams only capture a limited local context. They don't capture long-range dependencies in language.
Limited Cores,Module 12 - Basic Artificial Neural Net (ANN).docx,"CPUs have a relatively small number of cores (processing units), typically ranging from 4 to 64 in consumer-grade processors."
Limits,Module 12 - Gradient Descent and Backpropagation.docx,Approaching the Value Differentiation relies on the concept of limits. A limit describes the behavior of a function as its input approaches a certain value. Derivatives are found by taking the limit of the slope of secant lines as the two points on the secant line get closer and closer together.
Linear,Module 10 - GLMs.docx,η(μ)=μ.
Linear Regression,Module 01 - Introduction.docx,A type of regression model that assumes a linear relationship between the variables.
Linear Regression,Module 08 - Linear Regression.docx,"Linear Regression Concepts Correlation vs Simple Linear Regression Correlation analysis shows the strength and direction of the linear relationship between two variables. Simple linear regression analysis estimates parameters in a linear equation to predict values of one variable from another. The Dependent Variable In linear regression, the dependent variable (y) is continuous and numerical. It's the variable being predicted. Linear Regression A linear approach to modeling the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables). It's used to predict a continuous outcome based on one or more predictor variables. Scalar A physical quantity that is completely described by its magnitude (e.g., volume, density, speed, energy, mass, and time). It's a single number, as opposed to a vector, which has both magnitude and direction. Simple Linear Regression A type of linear regression where there's only one explanatory (independent) variable used to predict a single scalar response (dependent variable). Line of Best Fit A straight line drawn through a scatter plot of data points that best represents the relationship between those points. It's often determined using the least squares method. Residuals The vertical distance between a data point and the regression line (line of best fit). It represents the error in the prediction made by the regression model for that specific data point. Mean Squared Error (MSE) A measure of the average squared difference between the predicted values and the actual values in a regression model. It's commonly used to evaluate the performance of a regression model. Linear Regression with Scikit-learn Scikit-learn is a Python library that provides tools for machine learning, including linear regression. It can be used to build and train linear regression models. Assumptions of Linear Regression Linear regression models have several assumptions that should be met for the model to be reliable, including linearity, no multicollinearity, normality of residuals, homoscedasticity, and independence of residuals. Multiple Linear Regression A type of linear regression where there are two or more explanatory (independent) variables used to predict a single scalar response (dependent variable). Confidence Intervals A range of values within which it is estimated that a population parameter (like a regression coefficient) lies, with a certain level of confidence (e.g., 95%). It's used to express the uncertainty associated with an estimate. Types of Regression There are various types of regression analysis beyond simple and multiple linear regression, including polynomial regression, support vector regression, decision tree regression, and random forest regression, each with its own approach to modeling relationships between variables. "
Linearity of Independent Variables and Log Odds,Module 08 - Assumptions.docx,There should be a linear relationship between the independent variables and the log odds of the dependent variable.
Linearizing Relationships,Module 10 - Logarithms.docx,"Many natural phenomena exhibit exponential growth or decay. Logarithms can ""straighten out"" these curves, converting them into linear relationships. This simplification allows us to apply familiar linear regression techniques and gain deeper insights into the data."
Link Function,Module 10 - Logarithms.docx,A function that connects the linear predictor in a GLM to the expected value of the response variable. Examples include the identity link (for simple linear regression) and the logit link (for logistic regression).
Link Functions,Module 10 - Link Functions.docx,"Generalized Linear Models (GLMs) extend the traditional linear regression model to accommodate a wider range of response variables beyond continuous data. A key component of a GLM is the link function, which establishes a relationship between the linear predictor (a linear combination of predictor variables) and the expected value of the response variable. It's important to note that the link function is a crucial part of the GLM specification. Choosing an inappropriate link function can lead to biased estimates and incorrect inferences. "
Link Functions,Module 10 - Link Functions.docx,Here's a list of common link functions and their purposes 1. Identity Link 2. Log Link 3. Logit Link 4. Probit Link 5. Complementary Log-Log Link 6. Inverse Link 7. Square Root Link Choosing the Right Link Function The choice of link function depends on
Link Functions,Module 10 - GLMs.docx,A link function is a function that connects the linear predictor in a GLM to the expected value of the response variable. Examples include the identity link (for simple linear regression) and the logit link (for logistic regression).
Local Minima,Module 12 - Gradient Descent and Backpropagation.docx,"Gradient descent can sometimes get stuck in local minima, which are points that are minimum within a small region but not the global minimum of the cost function."
Local Minimum,Module 10 - GLMs.docx,A point in a function where the function has the lowest value in its immediate neighborhood.
Local Minimum,Module 10 - Logarithms.docx,A point in a function where the function has the lowest value in its immediate neighborhood.
Local Regression (LOESS),Module 10 - GLMs.docx,"A method that fits a separate regression model to localized subsets of the data, creating a smooth curve by combining these local models."
Local Regression (LOESS),Module 10 - Logarithms.docx,"A method that fits a separate regression model to localized subsets of the data, creating a smooth curve by combining these local models."
Local vs. Global Minima,Module 12 - Gradient Descent and Backpropagation.docx,"For complex cost functions, there might be multiple local minima. Limits help us analyze the behavior of the function around these points and determine if we've reached a true global minimum."
Log-Liklihood,Module 08 - Coefficients and Metrics.docx,"can be used to compare the fit of different coefficients, the higher value is better"
Log-likelihood,Module 10 - GLM vs OLS.docx,The probability of observing the data given a set of model parameters.
Logarithms,Module 10 - Logarithms.docx,"Unveiling the Power of Exponents At their core, logarithms are the inverse of exponentials. They help us answer the question, ""What exponent is needed to raise a certain base to get a specific value?"" For instance, the logarithm base 10 of 1000 is 3, because 10 raised to the power of 3 equals 1000. Why Logarithms Matter Real-World Applications In summary, logarithms are powerful tools that simplify complex calculations, reveal hidden patterns in data, and enable comparisons across vast scales. Their versatility makes them indispensable in a wide range of fields, from finance and computer science to music theory and psychology. Fundamentals The inverse of exponentials. They're used to express large numbers, scale data, or linearize relationships. For example, 103=1000 and log10​(1000)=3. Regression and Modeling Other Important Concepts  "
Logistic,Module 10 - GLMs.docx,η(μ)=log(μ/(1−μ)).
Logistic Regression,Module 08 - Logistic Regression.docx,"A glossary of terms and concepts from the document you provided is presented below. Logistic Regression A statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. Sigmoid A sigmoid curve is a common S-shaped curve that is often used in statistics to represent cumulative distribution functions. Sigmoid curves are bound by 0 and 1 on the y-axis and have a probability of 0.5 at the midpoint, or x=0. Threshold In logistic regression, a classification threshold, also called the decision threshold, is the value logistic regression uses to map a logistic regression value to a binary category. A logistic regression model that returns 0.9995 for a particular email message is predicting that it is very likely to be spam. Probability The probability is the number of observed outcomes divided by the possible outcomes. Odds The odds are what happened divided by what didn't happen. We take the odds to make the value continuous. Log Odds The log of the odds is used to get a range from negative infinity to infinity. We take the odds ratio to get a parameter estimate and take the log of that ratio to make the variable range from negative infinity to infinity and be symmetric around 0 instead of 1. Euler's Number Euler's number is often used as the base of an exponential. The derivative of the function y=ex is equal to itself. Euler's number is often used in machine learning. Logistic Function Logistic Model In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. Logit A Logit function, the inverse of the logistic sigmoid, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity. Sigmoid vs. Logit The inverse of the logit curve is the inverse-logit or sigmoid function. The sigmoid function transforms the numbers (-∞ to +∞) back to values between 0 and 1. "
Logits as output,Module 12 - Gradient Descent and Backpropagation.docx,The output from the model contains the raw logits (unnormalized scores) for each word in the vocabulary. These logits are sufficient for determining the most likely next word.
Loss Calculation,Module 12 - Gradient Descent and Backpropagation.docx,"loss = criterion(outputs, y) computes the loss between predictions and targets."
Loss Calculation,Module 12 - Basic Artificial Neural Net (ANN).docx,The criterion (loss function) is used to calculate the error between the predictions and the true labels.
Loss Function,Module 12 - Basic Artificial Neural Net (ANN).docx,A loss function measures the difference between the network's predictions and the actual target values. It quantifies the error that the network is making.
Loss Stabilization,Module 12 - Gradient Descent and Backpropagation.docx,The loss value stops decreasing significantly with each iteration and stabilizes around a certain value.
Low Input,Module 12 - Basic Artificial Neural Net (ANN).docx,"If the weighted sum is a large negative value, the sigmoid function outputs a value close to 0, indicating low activation."
MAR (Missing at Random),Module 02 - Miscellaneous.docx,The probability of missingness depends on observed variables but not on the missing values themselves. The missingness can be explained by other information in the data.
MCAR (Missing Completely at Random),Module 02 - Miscellaneous.docx,"The probability of data being missing is unrelated to any observed or unobserved variables. Essentially, missingness occurs entirely by chance."
MNAR (Missing Not at Random),Module 02 - Miscellaneous.docx,"The probability of missingness depends on the missing values themselves. This is the most complex type of missing data, as the missingness is related to the information we don't have."
MSE as the Cost Function,Module 12 - Gradient Descent and Backpropagation.docx,"As discussed earlier, MSE measures the average squared difference between predicted and actual values. Our goal is to find the values of our model parameters (like theta0 and theta1 in the previous example) that minimize this MSE."
Machine Learning,Module 12 - Gradient Descent and Backpropagation.docx,Training models using gradient descent.
Machine Learning,Module 03 - Distributions.docx,"Used in various machine learning algorithms, such as Bayesian networks and variational autoencoders."
Make inferences,Module 09 - Feature Selection.docx,"Based on the estimated sampling distribution, you can make inferences about the population parameter, such as calculating confidence intervals or p-values."
Makes data more accessible,Module 05 - EDA.docx,Raw data can be overwhelming and difficult to understand for many people. Storytelling helps make data more accessible and understandable by presenting it in a clear and engaging way.
Market Indices,Module 13 - Pandas Time Series Complete.docx,"These are indicators that track the performance of a group of stocks, bonds, or other assets. Common examples include the Dow Jones Industrial Average (DOW), S&amp;P 500, and NASDAQ Composite. They provide insights into overall market trends."
Markov Decision Process (MDP),Module 12 - Gradient Descent and Backpropagation.docx,A mathematical framework for modeling sequential decision-making problems in reinforcement learning.
Massive Cores,Module 12 - Basic Artificial Neural Net (ANN).docx,"GPUs contain thousands of cores, allowing them to perform massive parallel computations."
Mathematical Form,Module 10 - Link Functions.docx,"g(μ) = μ g(μ) = log(μ) g(μ) = log(μ / (1 - μ)) g(μ) = Φ⁻¹(μ), where Φ⁻¹ is the inverse of the standard normal CDF. g(μ) = log(-log(1 - μ)) g(μ) = 1 / μ g(μ) = √μ"
Matrix Multiplication Focus,Module 12 - Basic Artificial Neural Net (ANN).docx,"TPUs are optimized for matrix multiplication, a core operation in deep learning algorithms."
Maximum Likelihood Estimation (MLE),Module 10 - Logarithms.docx,A method for estimating unknown parameters (like the probability of heads in a coin flip) by finding the parameter values that make the observed data most likely.
Maximum Likelihood Estimation (MLE),Module 10 - GLMs.docx,A method for estimating unknown parameters (like the probability of heads in a coin flip) by finding the parameter values that make the observed data most likely.
Measures of Center,Module 02 - Descriptive Statistics.docx,"Mean, median, and mode are used to describe the central tendency of a dataset."
Measures of Shape,Module 02 - Descriptive Statistics.docx,Skewness and kurtosis are used to describe the shape of a distribution.
Measures of Spread,Module 02 - Descriptive Statistics.docx,"Variance, standard deviation, and quartiles are used to describe the spread or dispersion of data points around the mean."
Memorylessness,Module 03 - Distributions.docx,"For certain parameter values, the Gamma distribution exhibits a ""memoryless"" property, similar to the exponential distribution. This means that the probability of an event occurring in the future is independent of how much time has already elapsed."
Meteorology,Module 03 - Distributions.docx,Modeling rainfall amounts or the time between rainfall events.
Method,Module 08 - Coefficients and Metrics.docx,how the parameters (coefficients) were calculated
Miscellaneous,Module 02 - Miscellaneous.docx,"Box plot A graphical representation of the distribution of a dataset. Displays the median, quartiles, and potential outliers. The box represents the interquartile range (IQR), which contains the middle 50% of the data. The line inside the box represents the median. Whiskers extend from the box to indicate the range of the data within the fences. Outliers are plotted as individual points beyond the fences. Fence A cutoff value used in box plots to identify potential outliers. Calculated as 1.5 times the interquartile range (IQR) above the upper quartile and below the lower quartile. Data points outside the fences are considered potential outliers. Whiskers Lines extending from the box in a box plot. Indicate the range of the data within the fences. The upper whisker extends to the largest data point within the upper fence. The lower whisker extends to the smallest data point within the lower fence. IQR (Interquartile Range) The range between the first quartile (Q1) and the third quartile (Q3). Contains the middle 50% of the data. Calculated as Q3 - Q1. Quartiles Values that divide a dataset into four equal parts. Quantiles A generalization of quartiles that divide a dataset into any number of equal parts. Quartiles are a specific type of quantile. Percentiles are another common type of quantile. Outliers Data points that are significantly different from other observations in the dataset. Can be identified using various methods, including box plots and z-scores. May indicate errors, unusual events, or simply the natural variability of the data. Cardinality In the context of data, cardinality refers to the number of unique values in a dataset or a particular column (feature) of a dataset. High cardinality means many unique values (e.g., customer IDs, product names). Low cardinality means few unique values (e.g., binary categories like gender, boolean values). Important for understanding data types, storage efficiency, and potential issues like high-cardinality categorical features in machine learning. Features with Constant and Semi-Constant Values These features provide little to no information for predictive modeling or analysis. Often removed during feature selection to improve model efficiency and prevent overfitting. MCAR, MNAR, MAR Complete Case Analysis or Listwise Deletion A simple method for handling missing data. Involves removing any rows (observations) from the dataset that have at least one missing value. Can lead to loss of information and biased results if the missing data is not MCAR. Suitable when the proportion of missing data is small and MCAR can be reasonably assumed. Use of any and all in a Pandas DataFrame Useful for filtering data, checking for missing values, and performing logical operations on data. "
Miscellaneous,Module 03 - Miscellaneous.docx,Example Example Example
Miscellaneous,Module 02 - Miscellaneous.docx,These are types of missing data mechanisms
Miscellaneous,Module 03 - Miscellaneous.docx,"More Notes on Probability A FEW YEARS AGO a man won the Spanish national lottery with a ticket that ended in the number 48. Proud of his “accomplishment,” he revealed the theory that brought him the riches. “I dreamed of the number 7 for seven straight nights,” he said, “and 7 times 7 is 48.” Those of us with a better command of our multiplication tables might chuckle at the man’s error, but we all create our own view of the world and then employ it to filter and process our perceptions, extracting meaning from the ocean of data that washes over us in daily life. And we often make errors that, though less obvious, are just as significant as his. A Drunkard's Walk Prologue Paragraph 1  I. Introduction A man's lottery win and his faulty logic highlight how people can make significant errors in their understanding of probability. The chapter introduces the concept of probability, emphasizing its role in decision-making under uncertainty. II. Notes Deterministic programs focus on definite inputs and outputs, while probabilistic programs deal with decisions under uncertainty. Random variables represent the uncertain outcomes of random events, and their probabilities can be measured. In finance, risk involves an unknown outcome with a known probability distribution, while uncertainty involves both an unknown outcome and an unknown probability distribution. Variance measures the dispersion of a random variable, indicating how far a set of numbers is spread out from their average value. III. Data Does Not Speak for Itself Numbers need to be interpreted and given meaning, as they don't have inherent meaning themselves. IV. Probability The probability of an event is measured on a scale from 0 to 1, with 0 indicating that the event will not happen and 1 indicating that the event will always happen. It is calculated as the number of favorable outcomes divided by the total number of possible outcomes. The terms ""events"" and ""sample space"" are important in probability. Combinations, permutations, and factorials are used to calculate probabilities in different situations. Intersections, unions, and conditional probability are also important concepts in probability. V. Bayes Theorem and Bayes Inference Bayes' Theorem is a way to update the probability of an event based on new evidence. Bayesian inference is a method of statistical inference that uses Bayes' Theorem to update the probability of a hypothesis as more evidence or information becomes available. VI. Types of Probability There are several types of probability, including classical probability, enumerative probability, long-run frequency probability, propensity or chance probability, and subjective probability. VII. Rolling One Die The example of rolling a die is used to illustrate the difference between frequentist and Bayesian approaches to probability. VIII. Rolling Two Dice The probability distribution for the sum of two dice is shown. IX. Gambling The history of probability is intertwined with the history of gambling. The ancient Greeks and Romans had gambling games, but they didn't develop a systematic understanding of probability. The development of probability theory began in the 16th century, with the work of mathematicians such as Gerolamo Cardano and Galileo Galilei. Blaise Pascal and Pierre de Fermat made significant contributions to probability theory in the 17th century. X. Fraternal Twins The example of fraternal twins is used to illustrate the concept of conditional probability. XI. Probability Foundations The basic rules of probability are explained, including the addition rule and the multiplication rule. The addition rule and the multiplication rule are fundamental concepts in probability that help calculate the probability of complex events. Addition Rule Used to calculate the probability of either one event or another occurring. For mutually exclusive events (events that cannot occur at the same time), the probability of either event occurring is the sum of their individual probabilities. For non-mutually exclusive events (events that can occur at the same time), the probability of either event occurring is the sum of their individual probabilities minus the probability of both events occurring together. What is the probability of rolling a 1 or a 6 on a standard six-sided die? Since these events are mutually exclusive, we can use the addition rule for mutually exclusive events. The probability of rolling a 1 is 1/6. The probability of rolling a 6 is 1/6. Therefore, the probability of rolling a 1 or a 6 is 1/6 + 1/6 = 1/3. Multiplication Rule Used to calculate the probability of two events occurring together. For independent events (events where the occurrence of one does not affect the occurrence of the other), the probability of both events occurring is the product of their individual probabilities. For dependent events (events where the occurrence of one affects the occurrence of the other), the probability of both events occurring is the product of the probability of the first event and the conditional probability of the second event given that the first event has occurred. What is the probability of drawing two aces in a row from a standard deck of 52 cards, if the first card is not replaced? Since the events are dependent (the first card not being replaced affects the probability of drawing an ace on the second draw), we use the multiplication rule for dependent events. The probability of drawing an ace on the first draw is 4/52 (there are 4 aces in a deck of 52 cards). The probability of drawing another ace on the second draw, given that an ace was drawn on the first and not replaced, is 3/51 (there are 3 aces left in a deck of 51 cards). Therefore, the probability of drawing two aces in a row is (4/52) * (3/51) = 1/221. XII. Adding or Multiplying Probabilities See the definitions above XIII. The Conjunction Fallacy The conjunction fallacy is a common error in decision-making where people judge that a conjunction of two possible events is more likely than one or both of the conjuncts. XIV. The Prosecutor's Fallacy The prosecutor's fallacy is a common error in legal reasoning that involves misinterpreting statistical evidence. The Prosecutor's Fallacy is a common error in legal reasoning that involves misinterpreting conditional probabilities. It occurs when the probability of evidence being present given innocence is confused with the probability of innocence given the presence of that evidence. Imagine a rare blood type is found at a crime scene, and this blood type is present in only 1% of the population. The prosecutor might argue that since the defendant has this rare blood type, there's a 99% chance they are guilty. This is the Prosecutor's Fallacy. Why is it a fallacy? It ignores the prior probability of the defendant being guilty before the blood type evidence was considered. If there was little other evidence linking the defendant to the crime, the prior probability of guilt would be low. The blood type evidence increases the likelihood of guilt, but it doesn't automatically jump to 99%. The O.J. Simpson trial is used to illustrate the prosecutor's fallacy. Most abusers to do not kill their spouse Spouses that are killed by the other spouse are usually abusers XVI. Monty Hall Problem The Monty Hall Problem is a classic brain teaser that illustrates how our intuition about probability can be wrong. XVII. Formulas Formulas for factorials, permutations, and combinations are provided. XVIII. Intersections The concept of intersections in probability is explained. XIX. Unions The concept of unions in probability is explained. XX. Compliment The concept of complements in probability is explained. XXI. Conditional Probability The concept of conditional probability is explained. XXII. Bayes' Theorem Bayes' Theorem is a way to update the probability of an event based on new evidence. XXIII. Example Problems Several example problems are provided to illustrate the application of Bayes' Theorem. Frequentists vs Bayesians 1. Interpretation of Probability 2. Use of Prior Information 3. Statistical Inference Examples Let's consider a scenario where you're trying to determine the probability of a coin landing heads up. In Summary "
Model,Module 08 - Coefficients and Metrics.docx,what model are we using (ordinary least squares) for the training
Model Complexity,Module 11 - Bias Variance Tradeoff.docx,"This refers to the ability of a model to capture intricate patterns in the data. Complex models have higher variance and lower bias, while simpler models have higher bias and lower variance."
Model Selection,Module 03 - Distributions Fitter.docx,"In many statistical and machine learning models, you need to assume a certain distribution for your data. fitter can help you choose the most appropriate distribution."
Modeling Proportions,Module 03 - Distributions.docx,"Representing random variables that lie between 0 and 1, such as the proportion of defective items in a batch or the click-through rate of an advertisement."
Modify create_training_examples,Module 12 - Gradient Descent and Backpropagation.docx,"Instead of single words, the input would be a sequence of two words, and the target would be the third word."
"Module 1 covers a wide range of topics, from the history of data science to the most recent advances in machine learning. The outline is organized into five main sections",Module 01 - Introduction.docx,"Introduction, A Brief History, Data, Maths and Stats, and Examples. Here is a detailed summary of the key elements and terms in each section:"
Moments,Module 02 - Descriptive Statistics.docx,"Statistical measures that describe the shape of a distribution, including mean, variance, skewness, and kurtosis."
Momentum,Module 12 - Gradient Descent and Backpropagation.docx,"Like momentum-based gradient descent, Adam keeps track of past gradients to smooth out the updates. It calculates an exponentially decaying average of past gradients, similar to momentum."
Monotonicity,Module 12 - Basic Artificial Neural Net (ANN).docx,"The sigmoid function is monotonic, meaning it's always increasing. This can help with stability during training."
Monty Hall Problem,Module 03 - Probability.docx,A classic probability puzzle that demonstrates how counterintuitive probability can be. It illustrates the concept of conditional probability and how updating information can change the odds of an event.
Moving Average,Module 13 - Statsmodels Time Series Complete.docx,"The moving average is a simple but powerful technique used to smooth out fluctuations in time series data and highlight underlying trends. It works by calculating the average of a specified number of consecutive data points, creating a new series of averages."
Multilabel Confusion Matrix,Module 11 - Confusion Matrix Multilabel.docx,"A confusion matrix specifically designed for multilabel classification problems, where each instance can belong to multiple classes simultaneously."
Multinomial Logistic Regression,Module 11 - Confusion Matrix Multilabel.docx,A statistical model used to predict the probability of an instance belonging to one of three or more classes.
Multiplicative,Module 13 - Statsmodels Time Series Complete.docx,Used when the trend is non-linear (exponential) and/or seasonality varies proportionally to the level of the series.
Music Theory,Module 10 - Logarithms.docx,"The distance between musical notes, or intervals, can be expressed logarithmically. This helps understand the relationships between notes and create harmonious melodies."
Mutable vs. Immutable,Module 02 - Data.docx,"Whether the elements within a data structure can be changed after creation (mutable: lists, dictionaries) or not (immutable: tuples, strings)."
Myelin Sheath,Module 12 - Gradient Descent and Backpropagation.docx,This is a fatty layer that insulates the axon and speeds up signal transmission.
NVIDIA Tesla T4,Module 12 - Basic Artificial Neural Net (ANN).docx,"The T4 is a specific GPU model from NVIDIA's Tesla series, designed for high-performance computing and AI workloads. It offers a good balance of performance and power efficiency."
No Convergence,Module 12 - Gradient Descent and Backpropagation.docx,"In some cases, the algorithm might not converge at all, either due to inappropriate learning rates, noisy data, or other issues."
No Multicollinearity,Module 08 - Assumptions.docx,The independent variables should not be highly correlated with each other.
No. Observations,Module 08 - Coefficients and Metrics.docx,"the number of observations, rows... (n)"
Nominal,Module 02 - Data.docx,"Categories with no inherent order (e.g., colors, names)."
Non-linearity,Module 12 - Gradient Descent and Backpropagation.docx,"Activation functions introduce non-linearity, allowing the network to learn complex patterns and relationships in the data."
Non-linearity,Module 12 - Basic Artificial Neural Net (ANN).docx,"The weights, in combination with activation functions, introduce non-linearity into the network, allowing it to learn complex patterns and relationships in the data. The sigmoid function is non-linear, meaning its output is not a straight line. This non-linearity is crucial for neural networks to learn complex patterns and relationships in data."
Nonprofits,Module 05 - EDA.docx,"A nonprofit organization might use storytelling to communicate the impact of their work to donors, using data to show how donations have been used to make a difference in the community."
Normal Distribution,Module 10 - Logarithms.docx,"A bell-shaped curve, often seen in natural phenomena."
Normal Distribution,Module 02 - Descriptive Statistics.docx,A bell-shaped probability distribution that is widely used in statistics.
Normalization,Module 13 - Pandas Time Series Complete.docx,"The process of scaling data to a specific range, often between 0 and 1. In the context of the document, it's used to compare the growth of different market indices from a common starting point."
Not Zero-Centered,Module 12 - Basic Artificial Neural Net (ANN).docx,"The output of the sigmoid function is not zero-centered, which can sometimes hinder the learning process."
Null Hypothesis,Module 04 - Hypothesis Testing.docx,The hypothesis that there is no significant difference between two groups or no effect of a treatment.
NumPy-like Tensors,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch provides a Tensor class similar to NumPy arrays, but with added functionalities like GPU support and automatic differentiation. These tensors are optimized for efficient numerical operations, which are essential in deep learning."
Numerical,Module 02 - Data.docx,Data that represents quantities and can be either discrete (countable) or continuous (measurable).
Observe,Module 12 - Gradient Descent and Backpropagation.docx,The agent observes the current state of the environment.
Omnibus,Module 08 - Coefficients and Metrics.docx,D'Angostino's test provides a combined test for the presence of skewness and kurtosis
One-Sample t-Test,Module 04 - The Tests.docx,To test if a sample mean is significantly different from a known or hypothesized population mean.
Online Repositories,Module 02 - Data.docx,Various online platforms and resources that provide datasets for research and analysis.
Optimizer,Module 12 - Basic Artificial Neural Net (ANN).docx,An optimizer is an algorithm that adjusts the network's parameters (weights and biases) to minimize the loss function.
Optimum Model Complexity,Module 11 - Bias Variance Tradeoff.docx,"The optimal complexity of a model is the point where the combination of bias and variance is minimized, resulting in the lowest total error."
Order of Autoregression (p),Module 13 - Statsmodels Time Series Complete.docx,"The number of lagged values used in the model. For example, an autoregressive model of order 2 uses the two previous values to predict the current value."
Ordinal,Module 02 - Data.docx,"Categories with a meaningful order (e.g., rankings, ratings)."
Ordinary Least Squares (OLS),Module 10 - GLM vs OLS.docx,OLS is a statistical method used to estimate the parameters of a linear regression model. The goal of OLS is to find the line that minimizes the sum of the squared differences between the observed data points and the points on the line.
Out-of-Sample R-squared (OOS R-squared),"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Measures the model's performance on data that was not used to train the model. This is important for evaluating how well the model generalizes to new, unseen data."
Out-of-Sample Testing,Module 11 - Cross Validation.docx,"Similar to cross-validation, it involves testing a model on data not used during training to estimate its real-world performance."
Outliers,Module 02 - Descriptive Statistics.docx,Data points that are significantly different from other observations in the dataset.
Output,Module 12 - Gradient Descent and Backpropagation.docx,0.7391 0.7391
Output Layer,Module 12 - Gradient Descent and Backpropagation.docx,The output layer takes the final hidden state and produces a probability distribution over the vocabulary. The word with the highest probability is the predicted next word.
Output Layer,Module 12 - Basic Artificial Neural Net (ANN).docx,The team responsible for the final output identifies the error.
Output Range,Module 12 - Basic Artificial Neural Net (ANN).docx,The output of the sigmoid function is always between 0 and 1. This makes it useful for representing probabilities or activation levels of neurons.
Outputs,Module 12 - Gradient Descent and Backpropagation.docx,This is the final output of the neuron after the activation function is applied.
Overfitting,Module 11 - Confusion Matrix Binary.docx,"Occurs when a model is too complex and learns the training data too well, including noise. This leads to good performance on training data but poor generalization to new, unseen data."
Overfitting,Module 12 - Gradient Descent and Backpropagation.docx,"With more layers, the network has more capacity to overfit the training data. Regularization techniques can help prevent this."
Overfitting,Module 11 - Cross Validation.docx,"Occurs when a model learns the training data too well, including noise and irrelevant patterns, leading to poor performance on new data."
P-value,Module 04 - Hypothesis Testing.docx,"The probability of obtaining results as extreme as the observed results, assuming the null hypothesis is true."
P>|t|,Module 08 - Coefficients and Metrics.docx,"the p-value, indicates a statistically significant relationship to the dependent variable if less than the confidence level, usually 0.05"
PACF,Module 13 - Statsmodels Time Series Complete.docx,"PACF stands for Partial Autocorrelation Function. It's another important tool in time series analysis, and it's closely related to the ACF (Autocorrelation Function). However, there's a key difference: it measures the correlation between a time series and its lagged versions after removing the effect of intermediate lags."
Pair plots,Module 05 - EDA.docx,To visualize pairwise relationships between all variables in a dataset.
Paired t-Test,Module 04 - The Tests.docx,To test if there is a significant difference between the means of two dependent or related groups (not covered in the document).
Pandas,Module 13 - Pandas Time Series Complete.docx,This is a popular Python library specifically designed for working with and analyzing data. It provides data structures like DataFrames that are highly efficient for manipulating and exploring time series data.
Pandas,Module 13 - Statsmodels Time Series Complete.docx,This is a popular Python library specifically designed for working with and analyzing data. It provides data structures like DataFrames that are highly efficient for manipulating and exploring time series data.
Pandas Time Series Complete,Module 13 - Pandas Time Series Complete.docx,Module 13 - Pandas Time Series Complete 
Parallel Powerhouse,Module 12 - Basic Artificial Neural Net (ANN).docx,"Originally designed for graphics rendering, GPUs have evolved into powerful parallel processors. They excel at handling tasks that can be broken down into many smaller, simultaneous operations."
Parameter Stability,Module 12 - Gradient Descent and Backpropagation.docx,The changes in the model's parameters become very small with each iteration.
Parameter Update,Module 12 - Basic Artificial Neural Net (ANN).docx,The optimizer updates the network's parameters based on the calculated gradients and the learning rate.
Parent / Children,Module 11  - Trees.docx,The relationship between nodes and their sub-nodes.
Partial Derivatives,Module 12 - Gradient Descent and Backpropagation.docx,"When we have multiple parameters (like theta0 and theta1), we need to calculate the partial derivative of the MSE with respect to each parameter. This tells us how much the MSE changes when we change that specific parameter while keeping others constant."
Passing Activations,Module 12 - Basic Artificial Neural Net (ANN).docx,Passing the activations from one layer as input to the next layer.
Pearson's r,Module 02 - Descriptive Statistics.docx,"A standardized measure of the linear relationship between two continuous variables, ranging from -1 to +1."
Penalizes Incorrect Predictions,Module 12 - Gradient Descent and Backpropagation.docx,Cross-entropy heavily penalizes predictions that are confident but wrong. This encourages the model to learn more accurate probabilities.
Percent Change,Module 13 - Pandas Time Series Complete.docx,A way to express the change in value as a percentage of the original value. The document uses it to calculate the percentage decrease in the DOW from 2008 to 2009 and the increase from 2020 to 2024.
Physics,Module 12 - Gradient Descent and Backpropagation.docx,Finding velocities and accelerations of moving objects.
Pierre-Simon Laplace (1749-1827),Module 04 - Hypothesis Testing.docx,"Promoted Bayes' theorem, contributed to celestial mechanics, and proved the Central Limit Theorem."
Plotly,Module 13 - Pandas Time Series Complete.docx,A library that creates interactive and visually appealing plots and charts. It's often used to visualize time series data in an engaging way.
Poisson,Module 10 - GLMs.docx,"η(μ)=log(μ). Used for count data where the events occur independently at a constant rate (e.g., the number of customers arriving at a store per hour)."
Poisson Distribution,Module 10 - Logarithms.docx,Used for events happening in a fixed period of time.
Polynomial Regression,Module 10 - GLMs.docx,"Polynomial regression extends linear regression by adding polynomial terms (squared, cubed, etc.) to the model, allowing it to capture non-linear relationships."
Polynomial Regression,Module 10 - Logarithms.docx,"Extends linear regression by adding polynomial terms (squared, cubed, etc.) to the model, allowing it to capture non-linear relationships."
Population,Module 01 - Introduction.docx,The entire group of individuals or objects that are of interest in a study.
Pre-trained Models,Module 12 - Gradient Descent and Backpropagation.docx,PyTorch Hub offers a vast collection of pre-trained models that you can readily use or fine-tune for your tasks.
Precision,Module 11 - Confusion Matrix Binary.docx,The proportion of true positive predictions out of all positive predictions. It's calculated as TP / (TP + FP).
Precision,Module 11 - Confusion Matrix Multilabel.docx,The proportion of true positive predictions out of all positive predictions for a specific class.
Precision,Module 11 - Precision Recall.docx,The proportion of true positive predictions out of all positive predictions. It measures how often the model is correct when it predicts the positive class (fraudulent in this case).
Precision Recall,Module 11 - Precision Recall.docx,
Precision vs. Accuracy,Module 02 - Data.docx,"Precision refers to the level of detail or exactness in a measurement, while accuracy refers to how close a measurement is to the true value."
Precision-Recall Tradeoff,Module 11 - Confusion Matrix Binary.docx,"The tradeoff between precision and recall. Increasing precision often decreases recall, and vice versa. The optimal balance depends on the specific application and the relative costs of false positives and false negatives."
Precision-Recall Tradeoff,Module 11 - Precision Recall.docx,"The inverse relationship between precision and recall. Increasing precision (reducing false positives) often leads to decreased recall (increasing false negatives), and vice versa. The optimal balance depends on the specific application."
Predicted Distribution (ŷ),Module 12 - Gradient Descent and Backpropagation.docx,"Represents the probabilities predicted by your model. For example, ŷ = [[0.2, 0.8], [0.9, 0.1]] means the model predicts a 20% chance for the first sample to be in class 0 and an 80% chance for it to be in class 1 (and vice versa for the second sample)."
Presentism,Module 01 - Introduction.docx,"The tendency to interpret past events through the lens of present-day values and knowledge, which can introduce bias in historical analysis."
Primary Data Sources,Module 02 - Data.docx,"Data collected directly from firsthand experience, such as surveys, experiments, and observations."
Prob(Jarque-Bera),Module 08 - Coefficients and Metrics.docx,probability of Jarque-Bera
Prob(Omnibus),Module 08 - Coefficients and Metrics.docx,probability of Omnibus
Probability,Module 03 - Probability.docx,"The document ""Module 03 - Probability Complete.ipynb - Colab.pdf"" provides a comprehensive overview of essential probability concepts and their applications in data science. It covers a wide range of topics, from basic probability calculations to more advanced concepts like conditional probability, Bayes' theorem, and common fallacies in probability reasoning. Randomness and Randomized Controlled Trials Probability Calculations The measure of the likelihood that an event will occur. Historical Context and Biases Advanced Probability Concepts The document also includes various examples and problems to illustrate these concepts and their applications in data analysis and decision-making. "
Probability,Module 03 - Probability.docx,Here's a detailed outline and summary of key elements and terms
Probability,Module 01 - Introduction.docx,The measure of the likelihood that an event will occur.
Probability Density Function (PDF),Module 10 - Logarithms.docx,A function that describes the relative likelihood of different values of a random variable.
Probability Representation,Module 12 - Basic Artificial Neural Net (ANN).docx,The output range of 0 to 1 makes it suitable for representing probabilities.
Probability close to 0,Module 12 - Basic Artificial Neural Net (ANN).docx,Indicates a higher probability of belonging to class 0.
Probability close to 1,Module 12 - Basic Artificial Neural Net (ANN).docx,Indicates a higher probability of belonging to class 1.
Prop F statistic,Module 08 - Coefficients and Metrics.docx,the probability that you would get the F stat given the null hypothesis
Propagating Backwards,Module 12 - Basic Artificial Neural Net (ANN).docx,They communicate the error to the teams responsible for the previous steps.
Prosecutor's Fallacy,Module 03 - Probability.docx,A common error in legal reasoning that confuses the probability of a match with the probability of guilt.
Pruning,Module 11  - Trees.docx,Eliminating branches and nodes to simplify the tree.
Psychology,Module 10 - Logarithms.docx,Logarithmic scales are used in psychophysics to measure the relationship between the physical intensity of a stimulus and its perceived intensity.
Purpose,Module 12 - Basic Artificial Neural Net (ANN).docx,"The criterion (loss function) will be used during training to calculate the loss between the network's output and the true labels. This loss value guides the optimization process, indicating how well the network is performing and how the parameters should be adjusted to improve accuracy."
Purpose,Module 12 - Gradient Descent and Backpropagation.docx,Cross-entropy measures the dissimilarity between the predicted probability distribution and the true probability distribution. It's commonly used in classification tasks where you want your model to output probabilities for different classes.
Purpose,Module 10 - Link Functions.docx,"Models the expected value of the response variable directly as the linear predictor. Models the logarithm of the expected value of the response variable. This ensures the predicted values are always positive. Models the log-odds (logit) of the probability of an event. This constrains the predicted probabilities to lie between 0 and 1. Similar to the logit link, it models the inverse of the cumulative distribution function (CDF) of the standard normal distribution. Models the log of the negative log of the complementary probability (1 - probability of event). Models the inverse of the expected value of the response variable. Models the square root of the expected value of the response variable."
PyTorch,Module 12 - PyTorch.docx,"I found the following definitions and explanations for terms and concepts in this document. CPUs, T4 GPUs, and TPU v2-8s These are different types of processors with varying strengths and weaknesses. CPUs are general-purpose processors, T4 GPUs are parallel processors well-suited for deep learning, and TPU v2-8s are custom-designed processors specifically for machine learning and AI workloads. Input Layer The input layer is the first layer of the neural network. It receives the input data and passes it to the next layer. Hidden Layers Hidden layers are intermediate layers between the input and output layers. They perform computations and learn representations of the data. Output Layer The output layer produces the final result or prediction of the neural network. Neurons Neurons are the fundamental processing units of a neural network. They receive input, perform a computation, and produce an output. Training Loop The training loop is the iterative process of feeding data to the neural network, calculating the error, and updating the network's parameters to improve its performance. Forward Pass The forward pass is the process of passing the input data through the network to generate a prediction. Loss Calculation The loss calculation involves determining the difference between the predicted output and the actual target output. Backward Pass and Optimization The backward pass calculates the gradients of the loss with respect to the network's parameters. The optimizer then updates these parameters to minimize the loss. Epochs An epoch refers to one complete pass through the entire training dataset during the training process. Predictions Predictions are the outputs generated by the neural network when given new input data. torch.Tensor In PyTorch, a torch.Tensor is a multi-dimensional array that serves as the fundamental building block for all operations and models. It is similar to a NumPy array but with added functionalities for deep learning, such as GPU support and automatic differentiation. Automatic Differentiation Automatic differentiation is a technique used to automatically calculate the gradients of functions, which is crucial for training neural networks. torch.nn Module The torch.nn module in PyTorch provides a collection of pre-built layers, activation functions, and other components for constructing neural networks. Loss Function A loss function measures the difference between the network's predictions and the actual target values, quantifying the error the network makes. nn.MSELoss() This creates an instance of the MSELoss class from PyTorch's nn module, representing the Mean Squared Error Loss, commonly used for regression tasks. Optimizer An optimizer is an algorithm that adjusts the network's parameters (weights and biases) to minimize the loss function. optim.SGD(...) This creates an instance of the SGD class from PyTorch's optim module, representing Stochastic Gradient Descent, a widely used optimization algorithm. Learning Rate (lr) The learning rate controls the step size taken in the direction of the negative gradient during each iteration of the optimization process. "
PyTorch's autograd,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch provides the autograd package, which automatically computes gradients for you. This significantly simplifies the development process and reduces the risk of errors."
Pythagorean Theorem,Module 01 - Introduction.docx,A fundamental geometric theorem that relates the sides of a right triangle: a2+b2=c2.
Python Data Model,Module 02 - Data.docx,Defines the rules for how objects behave and interact in Python.
Pythonic,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch is built on Python, a language favored by many for its readability and ease of use. This makes PyTorch accessible to a broad range of users, from beginners to experienced researchers."
Q1,Module 02 - Miscellaneous.docx,The 25th percentile.
Q2,Module 02 - Miscellaneous.docx,The median (50th percentile).
Q3,Module 02 - Miscellaneous.docx,The 75th percentile.
QQ plots,Module 05 - EDA.docx,"To compare the distribution of a variable to a theoretical distribution (e.g., normal distribution)."
Queuing theory,Module 03 - Distributions.docx,Modeling the waiting times in queues or the time between events.
R,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Relationship between R-squared, R, and r Understanding TSS, ESS, and RSS Why Adjusted R-squared is Important"
R,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Core Concepts and Terms In simple linear regression, R represents the correlation coefficient. It measures the strength and direction of the linear relationship between two variables. R ranges from -1 to 1, where -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 indicates no linear relationship. Additional Important Concepts Deep Dive and Elaboration In simple linear regression (one independent variable), R is simply the square root of R-squared, and both have the same sign. They both reflect the strength of the linear relationship between the variables. The concept of 'r' (Pearson's correlation coefficient) is more general and doesn't assume a dependent vs. independent relationship between variables, just their joint variability. In multiple linear regression, the relationship between R and R-squared is more complex. R is calculated similarly to simple linear regression, but R-squared has a different formula to account for multiple predictors. These three concepts are fundamental to understanding how R-squared is calculated and what it represents. Visualizing these sums of squares graphically can be helpful. Imagine a scatter plot with the observed values of the dependent variable on the y-axis and the predicted values on the x-axis. TSS represents the total variation of the observed values around their mean. ESS represents the variation of the predicted values around the mean. RSS represents the variation of the observed values around the predicted values. R-squared can be calculated as ESS/TSS, which shows the proportion of the total variation explained by the model. R-squared tends to increase as more variables are added to the model, even if those variables don't actually improve the model's predictive power. Adjusted R-squared addresses this issue by penalizing the addition of unnecessary variables. When comparing models with different numbers of predictors, adjusted R-squared is a more reliable measure of model fit than R-squared. Key Considerations "
R-Squared,Module 08 - Coefficients and Metrics.docx,"coefficient of determination, how well the regression fits the data"
R-squared,Module 10 - GLM vs OLS.docx,A statistical measure that indicates the proportion of the variance in the dependent variable that is explained by the independent variables.
R-squared (R^2),"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","The coefficient of determination, a statistical measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Essentially, it tells us how well our model fits the data. A higher R-squared value generally indicates a better fit, with 1 being a perfect fit."
RNN Layer,Module 12 - Gradient Descent and Backpropagation.docx,"The RNN layer processes the embedded words sequentially, updating its hidden state at each time step. The hidden state acts as a ""memory"" of the network, storing information about the preceding words in the sequence."
ROC Curve,Module 11 - Confusion Matrix Binary.docx,A graphical representation of the tradeoff between true positive rate (sensitivity) and false positive rate (1-specificity) for different classification thresholds.
RSS (Residual Sum of Squares),"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Also known as the sum of squared errors (SSE), it measures the variability in the dependent variable that is not explained by the model. It's calculated as the sum of the squared differences between the observed values and the predicted values."
Randomness,Module 03 - Probability.docx,"A phenomenon where the outcome of a single event is uncertain, but a regular distribution of outcomes emerges over many repetitions."
Range,Module 03 - Distributions.docx,The range of the distribution is from 0 to infinity.
Rate parameter (β),Module 03 - Distributions.docx,Determines how spread out the distribution is. (Sometimes an inverse scale parameter (θ = 1/β) is used instead.)
ReLU (Rectified Linear Unit),Module 12 - Gradient Descent and Backpropagation.docx,"Outputs the input if it's positive, otherwise outputs 0."
Reaching a Tolerance,Module 12 - Gradient Descent and Backpropagation.docx,The algorithm stops when the loss or parameter changes fall below a predefined tolerance level.
Recall,Module 11 - Precision Recall.docx,The proportion of true positive predictions out of all actual positive instances. It measures how well the model identifies all actual positive instances. Also known as sensitivity.
Recall,Module 11 - Confusion Matrix Multilabel.docx,The proportion of true positive predictions out of all actual positive instances for a specific class. Also known as sensitivity.
Recall,Module 11 - Confusion Matrix Binary.docx,The proportion of true positive predictions out of all actual positive instances. It's calculated as TP / (TP + FN). Also known as sensitivity.
Record outputs,Module 09 - Feature Selection.docx,Store the results or outputs from each simulation run.
Regression,Module 01 - Introduction.docx,A statistical method used to model the relationship between a dependent variable and one or more independent variables.
Relationship to Other Distributions,Module 03 - Distributions.docx,"For specific values of α and β, the Beta distribution can take the same shape as other distributions, such as the uniform distribution."
Relationship to other distributions,Module 03 - Distributions.docx,"It's related to other important distributions. For example, the exponential distribution and the Chi-Squared distribution are special cases of the Gamma distribution."
Reliability,Module 02 - Data.docx,The consistency and reproducibility of data or measurements.
Reliability analysis,Module 03 - Distributions.docx,Modeling the time to failure of a system or component.
Repeat,Module 09 - Feature Selection.docx,"This process is repeated multiple times, with different folds used for testing each time."
Repeat,Module 12 - Gradient Descent and Backpropagation.docx,Repeat steps 2 and 3 until the gradient becomes very small or a set number of iterations is reached.
Repeated Random Subsampling,Module 11 - Cross Validation.docx,"A cross-validation method where data is randomly split into training and testing sets multiple times. It's less common than k-fold cross-validation because some data points might never be included in the test set, and others might be included more than once."
Representation,Module 12 - Basic Artificial Neural Net (ANN).docx,Biases help the network learn more complex patterns and representations in the data. They can capture offsets or baseline activations that are independent of the input features.
Representativeness,Module 02 - Data.docx,The sample should accurately reflect the characteristics of the population it represents.
Research,Module 12 - Gradient Descent and Backpropagation.docx,PyTorch is the go-to framework for many researchers due to its flexibility and ease of use in experimenting with new ideas and architectures.
Research Question,Module 02 - Data.docx,A clear and focused question that guides the data collection and analysis process.
Return Output,Module 12 - Basic Artificial Neural Net (ANN).docx,"The forward method returns the self.output_layer, which represents the network's prediction for the given input."
Reward,Module 12 - Gradient Descent and Backpropagation.docx,The environment provides a reward signal based on the action taken.
Risk analysis,Module 09 - Feature Selection.docx,"Model uncertain factors and their potential impact on outcomes (e.g., financial risk, project risk) to assess probabilities and make informed decisions."
Robotics,Module 12 - Gradient Descent and Backpropagation.docx,"Control of robots, navigation, manipulation"
RobustScaler,Module 11 - Precision Recall.docx,"A data preprocessing technique that scales features to a common range, making them less sensitive to outliers. It's used to normalize the 'Amount' and 'Time' features in the credit card dataset."
Robustness,Module 12 - Gradient Descent and Backpropagation.docx,Adam is generally robust to the choice of hyperparameters and can perform well with default settings.
Role,Module 12 - Basic Artificial Neural Net (ANN).docx,"Each value in this matrix represents the weight associated with the connection between a specific input feature and a specific neuron in the first hidden layer. These weights determine how much each input feature influences the activation of the neurons in the first hidden layer. Each value in this matrix represents the weight associated with the connection between a specific neuron in the first hidden layer and a specific neuron in the second hidden layer. These weights determine how the activations of the first hidden layer influence the activations of the second hidden layer. Each value in this matrix represents the weight associated with the connection between a specific neuron in the second hidden layer and the output neuron. These weights determine how the activations of the second hidden layer contribute to the final output of the network. Each value in this vector represents the bias associated with a specific neuron in the first hidden layer. The bias acts as an offset or threshold for the activation of that neuron. It allows the neuron to activate even when the weighted sum of its inputs is zero. Similar to self.bias1, this vector contains the biases for the neurons in the second hidden layer. This vector contains the bias for the output neuron."
Rolling Mean,Module 13 - Pandas Time Series Complete.docx,"Also known as a moving average, it calculates the average of data points within a specific sliding window. This smooths out short-term fluctuations in time series data, making it easier to see long-term trends."
Ronald Fisher (1890-1962),Module 04 - Hypothesis Testing.docx,Formalized the p-value concept and contributed to statistical methods in various fields.
Root Node,Module 11  - Trees.docx,The initial node in the decision tree.
Run the model or system,Module 09 - Feature Selection.docx,Use the generated random inputs to run the model or simulate the system.
SARIMAX,Module 13 - Statsmodels Time Series Complete.docx,SARIMA (Seasonal ARIMA) is an extension of ARIMA that explicitly includes seasonal components. SARIMAX (Seasonal ARIMA with exogenous regressors) is an extension of SARIMA that allows you to include external variables (exogenous variables) in your model.
Sample,Module 01 - Introduction.docx,A subset of the population that is selected for study.
Sample Space,Module 03 - Probability.docx,The set of all possible outcomes of an experiment.
Scaling Data,Module 10 - Logarithms.docx,"Logarithms excel at handling data that spans a vast range of values. By transforming data logarithmically, we bring extreme values closer together, making patterns and trends more apparent. This is crucial in fields like finance, where stock prices can vary wildly, or in audio processing, where decibels (a logarithmic unit) measure sound intensity."
Scatter plots,Module 05 - EDA.docx,To visualize the relationship between two variables.
Scikit-learn Datasets,Module 02 - Data.docx,Built-in datasets in the scikit-learn library for machine learning practice.
Seaborn Datasets,Module 02 - Data.docx,Datasets available in the Seaborn library for data visualization.
Second Hidden Layer,Module 12 - Gradient Descent and Backpropagation.docx,"This layer can build upon the features extracted by the first layer. It might learn to recognize longer-range dependencies, more complex grammatical structures, or higher-level semantic relationships between words."
Second Layer,Module 12 - Gradient Descent and Backpropagation.docx,"You start to connect the dots, recognizing themes, character development, and the overall narrative structure. Recognizes the pattern ""article + adjective + adjective + noun"" and predicts that the next word is likely a noun (like ""fox"")."
Secondary Data Sources,Module 02 - Data.docx,"Data collected from existing sources like books, journals, articles, and online databases."
Semantic relationships,Module 12 - Gradient Descent and Backpropagation.docx,"The meaning and relationships between words (e.g., ""king"" and ""queen"")."
Semi-constant features,Module 02 - Miscellaneous.docx,Have a dominant unique value and very few occurrences of other values.
Sequential Processing,Module 12 - Basic Artificial Neural Net (ANN).docx,"CPUs are designed to handle a wide variety of tasks, but they typically excel at sequential processing, executing instructions one after another."
Sequential Representation,Module 12 - Gradient Descent and Backpropagation.docx,"The hidden layers build a sequential representation of the input text. This means they capture the order and context of the words, which is crucial for predicting the next word."
Shape,Module 12 - Basic Artificial Neural Net (ANN).docx,"(input_size, hidden_size1) - In this case, (4, 5) because you have 4 input features and 5 neurons in the first hidden layer. (hidden_size1, hidden_size2) - In this case, (5, 4) because you have 5 neurons in the first hidden layer and 4 neurons in the second hidden layer. (hidden_size2, output_size) - In this case, (4, 1) because you have 4 neurons in the second hidden layer and 1 output neuron. (1, hidden_size1) - In this case, (1, 5) because there are 5 neurons in the first hidden layer. (1, hidden_size2) - In this case, (1, 4) because there are 4 neurons in the second hidden layer. (1, output_size) - In this case, (1, 1) because there is 1 output neuron."
Shape,Module 03 - Distributions.docx,"It is a right-skewed distribution. The shape of the distribution depends on the degrees of freedom (k). As k increases, the distribution becomes more symmetrical."
Shape parameter (α or k),Module 03 - Distributions.docx,Determines the shape of the distribution.
Shapiro-Wilk Test,Module 10 - Logarithms.docx,A statistical test to check if data follows a normal distribution.
Short-term dependencies,Module 12 - Gradient Descent and Backpropagation.docx,The previous few words in the sequence.
Sigmoid,Module 12 - Gradient Descent and Backpropagation.docx,Squashes the input between 0 and 1. For binary classification.
Sigmoid Derivative,Module 12 - Basic Artificial Neural Net (ANN).docx,σ'(z_i^l) is the derivative of the sigmoid function applied to the weighted sum of inputs for the i-th example in layer l. This term captures how much a small change in the weighted sum affects the activation of the neuron.
Simplicity,Module 12 - Gradient Descent and Backpropagation.docx,N-gram models are relatively simple to understand and implement.
Simplify,Module 12 - Gradient Descent and Backpropagation.docx,2 * x¹ = 2x
Simplifying Calculations,Module 10 - Logarithms.docx,"Logarithms transform multiplication and division into addition and subtraction, respectively. This property was invaluable in the days before calculators, as it made complex calculations more manageable. Even today, logarithms are used in algorithms for efficient computation."
Simulation,Module 03 - Distributions Fitter.docx,"Once you know the best fitting distribution, you can use it to generate synthetic data for simulations or modeling."
Simultaneous Zero,Module 12 - Gradient Descent and Backpropagation.docx,"However, the denominator (x - 2) is also approaching zero, suggesting the limit might be undefined or infinite."
Skewness,Module 08 - Coefficients and Metrics.docx,A measure of the symmetry of the data about the mean
Slope of a secant line,Module 12 - Gradient Descent and Backpropagation.docx,The slope of this secant line represents the average rate of change of the function between those two points. It's calculated as: (change in y) / (change in x)
Slope of a tangent line,Module 12 - Gradient Descent and Backpropagation.docx,The slope of the tangent line represents the instantaneous rate of change of the function at that specific point.
Slow Convergence,Module 12 - Gradient Descent and Backpropagation.docx,"Convergence can be slow, especially with complex models or challenging loss landscapes. Techniques like momentum or adaptive learning rates can help speed up convergence."
Small Steps,Module 12 - Basic Artificial Neural Net (ANN).docx,"A small learning rate means you take tiny steps, which can be slow but ensures you don't overstep the bottom."
Small sample sizes,Module 09 - Feature Selection.docx,It's particularly useful when dealing with small sample sizes where traditional methods might not be reliable.
Smoother Updates,Module 12 - Gradient Descent and Backpropagation.docx,"Momentum smooths out the parameter updates, reducing oscillations and leading to more stable convergence."
Smoothing Splines,Module 10 - Logarithms.docx,"Similar to splines, but they aim to approximate the data without necessarily passing through every data point, which is useful for noisy data."
Smoothing Splines,Module 10 - GLMs.docx,"Smoothing splines are similar to splines, but they aim to approximate the data without necessarily passing through every data point, which is useful for noisy data."
Smoothness,Module 12 - Basic Artificial Neural Net (ANN).docx,"The sigmoid function is smooth and differentiable, meaning it has a well-defined derivative at all points. This is important for training neural networks using gradient-based optimization algorithms. Its differentiability allows for gradient-based optimization."
Softmax,Module 12 - Gradient Descent and Backpropagation.docx,For multi-class classification.
Soma,Module 12 - Gradient Descent and Backpropagation.docx,This is the cell body of the neuron where the inputs are processed.
Some More Types of Regression,Module 08 - Some More Types of Regression.docx,"Some More Types of Regression Simple Linear Regression A type of linear regression with one explanatory or independent variable used to predict a scalar response or dependent variable. Multiple Linear Regression A type of linear regression with two or more explanatory or independent variables used to predict a scalar response or dependent variable. Polynomial Regression A type of regression used to describe how diseases spread, pandemics, or epidemics. Support Vector Regression A type of regression that uses a margin that tries to capture as many of the plots as possible. Decision Tree Regression A type of regression that uses splits to partition data points. Random Forest Regression A type of regression that is a version of ensemble learning. Random forests pick random data points from a training set. "
Sparsity,Module 12 - Gradient Descent and Backpropagation.docx,"As you increase the value of n, the number of possible n-grams grows exponentially, leading to data sparsity issues."
Special Methods,Module 02 - Data.docx,"Functions within a class that customize the behavior of objects (e.g., how they are initialized, printed, or compared)."
Specify input parameters,Module 09 - Feature Selection.docx,Identify the key input variables and their associated probability distributions.
Speedometer,Module 12 - Gradient Descent and Backpropagation.docx,"When you're driving a car, the speedometer doesn't show your average speed over the entire trip. It shows your speed at that very moment, which is your instantaneous speed. Differentiation is like a mathematical speedometer for functions."
Splines,Module 10 - GLMs.docx,Splines are piecewise polynomial functions used to create smooth curves. They're useful for interpolating data and approximating complex relationships.
Splines,Module 10 - Logarithms.docx,Piecewise polynomial functions used to create smooth curves. They're useful for interpolating data and approximating complex relationships.
Split the data,Module 09 - Feature Selection.docx,The data is divided into multiple subsets (folds).
Splitting,Module 11  - Trees.docx,Dividing a node into sub-nodes based on a decision rule.
Stages,Module 02 - Data.docx,"The data life cycle includes asking a question, obtaining data, understanding the data, understanding the context or domain, and finally using the data for reports, decisions, or solutions."
Standard Error,Module 08 - Coefficients and Metrics.docx,This measures the accuracy of your coefficient estimates. A smaller standard error means your coefficient estimate is likely more precise.
Start at the Output,Module 12 - Basic Artificial Neural Net (ANN).docx,The process begins by calculating the error at the output layer. This error represents the difference between the network's prediction and the actual target value.
Start with a moderate value,Module 12 - Basic Artificial Neural Net (ANN).docx,A common starting point is 0.1 or 0.01.
Static Typing,Module 02 - Data.docx,The data type is explicitly defined and checked before execution.
Stationary Time Series,Module 13 - Statsmodels Time Series Complete.docx,"A stationary time series is one whose statistical properties, such as mean, variance, and autocorrelation, remain constant3 over time. This means that the data looks roughly the same no matter what period you examine."
Statistical Inference,Module 02 - Data.docx,Drawing conclusions about a population based on sample data.
Statistical Model,Module 02 - Data.docx,A mathematical representation of relationships between variables used to describe or explain phenomena.
Statsmodels,Module 13 - Statsmodels Time Series Complete.docx,"Statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.1"
Statsmodels Time Series Complete,Module 13 - Statsmodels Time Series Complete.docx,Module 13 - Statsmodels Time Series Complete 
Step Function,Module 10 - Logarithms.docx,A function that increases or decreases abruptly from one constant value to another.
Step Function,Module 10 - GLMs.docx,A function that increases or decreases abruptly from one constant value to another.
Step Size,Module 12 - Gradient Descent and Backpropagation.docx,"The learning rate in gradient descent determines the step size. We can think of this as taking smaller and smaller steps as we get closer to the minimum, similar to the concept of a limit approaching a value."
Stepping Stones,Module 12 - Basic Artificial Neural Net (ANN).docx,Imagine you're trying to reach the bottom of a valley. The learning rate determines the size of the steps you take.
Steps,Module 02 - Data.docx,"The data modeling process involves gathering requirements, conceptual design, logical design, physical design, and implementation."
Stopping Criterion,Module 12 - Gradient Descent and Backpropagation.docx,"Convergence provides a stopping criterion for the algorithm, preventing unnecessary computations when further iterations don't bring significant improvements."
Structural Equation Modeling,Module 01 - Introduction.docx,"A statistical technique used to analyze complex relationships between multiple variables, including latent variables that cannot be directly observed."
Subtract 1 from the exponent,Module 12 - Gradient Descent and Backpropagation.docx,2 * x^(2-1)
Sum (Σxᵢwᵢ),Module 12 - Gradient Descent and Backpropagation.docx,The neuron calculates a weighted sum of its inputs. This is done by multiplying each input by its corresponding weight and adding up the results.
Sum of Squares,Module 01 - Introduction.docx,A mathematical concept used to calculate the total squared deviation of data points from their mean.
Summation,Module 12 - Gradient Descent and Backpropagation.docx,The weighted sum in the artificial neuron is analogous to the integration of signals in the soma of a biological neuron.
Summation,Module 12 - Basic Artificial Neural Net (ANN).docx,Σ_i sums up these contributions over all training examples to get the overall gradient for the weight θ_j.
Supervised Learning,Module 11 - Cross Validation.docx,"A type of machine learning where models learn from labeled data (input-output pairs) to predict outcomes for new, unseen data."
Supervised Learning,Module 12 - Gradient Descent and Backpropagation.docx,"Training artificial neural networks often involves supervised learning, where the network is given labeled data and learns to map inputs to outputs. Biological learning is often unsupervised or reinforcement-based, where the neuron learns through exploration and feedback from the environment."
Support,Module 11 - Confusion Matrix Multilabel.docx,The number of actual instances belonging to a specific class in the test set.
Synapses,Module 12 - Gradient Descent and Backpropagation.docx,These are the connections between neurons where signals are transmitted.
T4 GPU,Module 12 - Basic Artificial Neural Net (ANN).docx,"For deep learning, scientific simulations, graphics rendering, and other tasks that benefit from parallel processing."
TN (True Negative),Module 11 - Confusion Matrix Multilabel.docx,The number of instances correctly predicted as not belonging to a specific class.
TP (True Positive),Module 11 - Confusion Matrix Multilabel.docx,The number of instances correctly predicted as belonging to a specific class.
TPU v2-8,Module 12 - Basic Artificial Neural Net (ANN).docx,"This refers to a specific generation and configuration of TPUs. It typically consists of multiple TPU chips interconnected to provide massive computational power. For large-scale deep learning training and inference, especially with TensorFlow models."
TSS (Total Sum of Squares),"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx",Measures the total variability in the dependent variable. It's calculated as the sum of the squared differences between each observed data point and the mean of the dependent variable.
Tangent Lines,Module 12 - Gradient Descent and Backpropagation.docx,"A tangent line is a line that just grazes a curve at a single point, having the same slope as the curve at that point. The slope of the tangent line represents the instantaneous rate of change of the function at that specific point."
Tanh (Hyperbolic Tangent),Module 12 - Gradient Descent and Backpropagation.docx,Squashes the input between -1 and 1.
Task Duration Modeling,Module 03 - Distributions.docx,"In project management, the Beta distribution can be used to model the probability distribution of the duration of a task."
TensorFlow Integration,Module 12 - Basic Artificial Neural Net (ANN).docx,"TPUs are tightly integrated with Google's TensorFlow framework, providing optimized performance for TensorFlow models."
Terms,Module 06 - Terms.docx,"Terms and Concepts Bessel's Correction Bessel's correction is the use of n−1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance. It also partially corrects the bias in the estimation of the population standard deviation. Bias Bias is a systematic error that can occur in a statistical analysis, leading to inaccurate or misleading results. It can be caused by various factors, such as sampling bias, measurement bias, or human bias. Degrees of Freedom Degrees of freedom are the number of independent values that a statistical analysis can estimate. You can also think of it as the number of values that are free to vary as you estimate parameters. Degrees of freedom is a combination of how much data you have and how many parameters you need to estimate. Standard Deviation The standard deviation is a measure of the spread or dispersion of data points around the mean. It is calculated as the square root of the variance. A higher standard deviation indicates that the data is more spread out, while a lower standard deviation indicates that the data is more clustered around the mean. Boosting In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). Bootstrapping Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples. This process allows you to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics. These resamples are known as sampling distributions of estimates. Central Limit Theorem The central limit theorem (CLT) is a statistical concept that states that the distribution of sample means approximates a normal distribution as the sample size gets larger, regardless of the population's distribution. Sample means will be normally distributed, even if the population isn't normally distributed. The central limit theorem is important in hypothesis testing and confidence intervals. Law of Large Numbers In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. Guarantees stable long-term results for the averages of some random events. Distribution of Sample Means Imagine taking many random samples from a population and calculating the mean of each sample. The distribution of these sample means is called the ""distribution of sample means."" Crucially, even if the original population isn't normally distributed, the distribution of sample means tends towards a normal distribution as the sample size increases. This is the foundation of the Central Limit Theorem. This distribution has its own mean (close to the population mean) and standard deviation (called the standard error). Confidence Intervals A confidence interval is a range of values around a sample statistic (like the mean) that is likely to contain the true population parameter with a certain level of confidence (e.g., 95%). It essentially gives you a plausible range for where the true population value lies, based on your sample data. Confidence intervals are affected by sample size and variability. Larger samples and lower variability lead to narrower confidence intervals, indicating more precise estimates. Uncertainty In the statistical sense, uncertainty refers to the lack of complete knowledge about a population or phenomenon. We use samples to estimate population characteristics, but there's always some level of uncertainty because we haven't measured the entire population. Uncertainty can be quantified using concepts like confidence intervals, standard errors, and margins of error. Margin of Error The margin of error is the range of values above and below the sample statistic in a confidence interval. It essentially tells you how much your sample estimate might differ from the true population value due to random sampling error. A larger margin of error indicates greater uncertainty in the estimate. Standard Error The standard error is the standard deviation of the distribution of sample means. It measures how much the sample means vary from the true population mean. A smaller standard error indicates that the sample means are clustered more closely around the population mean, suggesting a more precise estimate. Relationships and Comparisons The distribution of sample means is the foundation for understanding confidence intervals and standard errors. Confidence intervals are constructed using the margin of error, which is calculated using the standard error. All these concepts help quantify and manage uncertainty in statistical analysis. By understanding these concepts and their relationships, you can better interpret statistical results, estimate population parameters, and make informed decisions based on data analysis. Natural Language Processing (NLP) Natural language processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. It involves developing algorithms and models that can analyze text and speech, extract meaning, and perform tasks such as translation, summarization, and question answering. NLP is used in various applications, including chatbots, machine translation, sentiment analysis, and text classification. One-Hot Encoding In machine learning, one-hot encoding is a frequently used method to deal with categorical data. Because many machine learning models need their input variables to be numeric, categorical variables need to be transformed in the pre-processing part. One-hot encoding creates new binary columns for each unique value in a categorical variable, indicating the presence or absence of that value for each observation. Natural Language Processing (NLP) is a branch of artificial intelligence that deals with enabling computers to understand, interpret, and generate human language. Word Tokens Tokens are the total number of words in a corpus regardless of whether they are repeated. Word tokenization splits text into words. Tokens are the basic building blocks of natural language understanding. CountVectorizer CountVectorizer is a tool used in NLP to convert a collection of text documents into numerical feature vectors. It works by counting the occurrences of each word in each document and creating a matrix representation of the data. This matrix can then be used as input for machine learning algorithms. Bag of Words The bag-of-words model is a simplifying representation used in NLP and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. Keeping multiplicity means that the number of times a word appears in the text is recorded. Stemming Stemming is the process of reducing a word to its base or root form, called the stem. This is done by removing suffixes and prefixes from the word. Stemming is used in NLP to normalize words and reduce the number of unique words in a corpus. Lemmatization Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item. This is done by using a dictionary or a set of rules to identify the base form of a word, called the lemma. Lemmatization is used in NLP to improve the accuracy of text analysis. TF-IDF (Term Frequency Inverse Document Frequency) TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It is used to score words in the context of the document as well as in the context of the corpus. The higher the TF-IDF score, the more useful the word is in the analysis of the document. Stop Words Stop words are commonly used words (such as ""the"", ""a"", ""is"", ""to"") that are often removed from text during NLP tasks. This is because stop words typically do not carry much meaning and can clutter the analysis. Removing stop words can improve the efficiency and accuracy of NLP tasks. spaCy spaCy is an open-source library for advanced NLP tasks. It provides tools for various NLP tasks, such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing. It is designed to be fast, efficient, and easy to use. Language Models A language model is a probability distribution over sequences of words. It assigns a probability to the whole sequence. Language models are used in various NLP tasks, such as machine translation, speech recognition, and text generation. N-grams N-grams are contiguous sequences of n items from a given sample of text or speech. They are used to model the probability of a sequence of words, which is useful in various NLP tasks. Examples of n-grams include unigrams (single words), bigrams (two words), and trigrams (three words). Markov Chains A Markov chain is a mathematical system that undergoes transitions from one state to another, between a finite or countable number of possible states. It is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Markov chains are used in NLP to model the probability of a sequence of words, which is useful in various NLP tasks. Word Embeddings In natural language processing (NLP), a word embedding is a learned representation for text where words that have the same meaning have a similar representation. Word embeddings are used to capture the semantic relationships between words. They are used in various NLP tasks, such as machine translation, sentiment analysis, and text classification. Similarity Measures Similarity measures are used to quantify the similarity between two pieces of text. There are various similarity measures, such as cosine similarity, Jaccard distance, and Euclidean distance. Similarity measures are used in NLP tasks such as information retrieval, document clustering, and text classification. Part of Speech (POS) Part-of-speech (POS) tagging is the process of assigning a part-of-speech tag to each word in a text. POS tags indicate the grammatical role of a word in a sentence, such as noun, verb, adjective, or adverb. POS tagging is used in various NLP tasks, such as parsing, machine translation, and information retrieval. Named Entity Recognition (NER) Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in various NLP applications, such as information retrieval, question answering, and machine translation. Sentence Segmentation Sentence segmentation, also known as sentence boundary disambiguation, is the problem in natural language processing of deciding where sentences begin and end. Sentence segmentation is used to divide a text into individual sentences, which is a necessary step for many NLP tasks. Topic Modeling Topic modeling is a statistical method for discovering abstract ""topics"" that occur in a collection of documents. It is used to uncover hidden themes or topics in a corpus of text data. Topic modeling is used in various NLP applications, such as document summarization, information retrieval, and text classification. Latent Dirichlet Allocation (LDA) Latent Dirichlet Allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. Non-Negative Matrix Factorization Non-Negative Matrix Factorization is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence. Performs dimensionality reduction and clustering Used with TF-IDF Similarity Measures Longest Common Substring India and Indiana would return 5 Levenshtein Edit Distance Finds the minimum number of single-character edits such as replacement, deletion, and insertion, needed to convert 1 text into another India and Indiana would return 2 Hamming Distance Finds the number replacements needed to change one text into another of equal size Indians and Indiana returns 2 Jaccard Distance Finds how disimilar two words are by distance and the lower the distance, the more similar Euclidean Distance Finds the length between two points l2 norm Dot Product Considers orientation, the direction, that Euclidean lacks Uses magnitude with orientation Cosine Similarity In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval -1, 1. For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. For example, in information retrieval and text mining, each word is assigned a different coordinate and a document is represented by the vector of the numbers of occurrences of each word in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be, in terms of their subject matter, and independently of the length of the documents. The technique is also used to measure cohesion within clusters in the field of data mining. "
Test Statistic,Module 04 - Hypothesis Testing.docx,A numerical value calculated from sample data that is used to test a hypothesis.
Test Statistics,Module 11 - Confusion Matrix Multilabel.docx,Evaluation metrics used to assess the performance of a classification model.
Test of Independence,Module 03 - Distributions.docx,To test whether there is a significant association between two categorical variables.
The Brain,Module 12 - Basic Artificial Neural Net (ANN).docx,"This is the general-purpose processor found in every computer. It handles all the basic operations of the system, from running your operating system and applications to executing calculations and managing data."
The Error Curve of 1777 - https,Module 06 - Astronomy and Statistics.docx,//www.qualitydigest.com/inside/six-sigma-article/distribution-measurement-error-040323.html
The Final Output,Module 12 - Gradient Descent and Backpropagation.docx,"After 10,000 epochs (iterations) of training, the output_layer_activation variable holds the network's predictions for the four input combinations."
The Function,Module 01 - Introduction.docx,"The concept of a function, represented as f(x), is introduced as a fundamental building block in mathematics and data science."
The History of Statistics,Module 06 - Astronomy and Statistics.docx,The Measurement of Uncertainty before 1900 by Stephen M. Stigler
The Task,Module 12 - Gradient Descent and Backpropagation.docx,"The neural network is learning to solve the XOR problem, which is a classic task in machine learning. The XOR function outputs 1 if the two inputs are different and 0 if they are the same."
The Tests,Module 04 - The Tests.docx,"The document ""Module 04 - The Tests.ipynb - Colab.pdf"" provides a comprehensive overview of various statistical tests used in hypothesis testing. These tests help determine if there are significant differences between groups or if observed patterns are likely due to chance. 1. T-Test 2. Levene's Test 3. Shapiro-Wilk Test 4. Kolmogorov-Smirnov Test 5. Chi-Square Test for Independence 6. ANOVA (Analysis of Variance) 7. Kruskal-Wallis Test 8. Fisher's Exact Test 9. Z-Test 10. Welch's t-Test 11. Mann-Whitney U Test "
The Tests,Module 04 - The Tests.docx,"Here's a breakdown of the tests covered in the document, along with their definitions and use cases Use Case"
The Training,Module 12 - Gradient Descent and Backpropagation.docx,"The network is trained on the four possible input combinations ([0, 0], [0, 1], [1, 0], [1, 1]) and their corresponding XOR outputs ([0], [1], [1], [0]). During training, the network adjusts its weights and biases to minimize the difference between its predictions and the actual XOR outputs."
The confidence interval,Module 09 - Feature Selection.docx,The range between these percentiles forms the bootstrap confidence interval.
The desired interpretation of the model coefficients,Module 10 - Link Functions.docx,How changes in the predictor variables affect the response variable.
The distribution of the response variable,Module 10 - Link Functions.docx,"Normal, Poisson, binomial, gamma, etc."
The nature of the response variable,Module 10 - Link Functions.docx,"Whether it's continuous, binary, count, etc."
Think of it like this,Module 12 - Gradient Descent and Backpropagation.docx,"the weighted sum gives you a raw score, and the activation function decides what to do with that score."
Thomas Bayes (1702-1761),Module 04 - Hypothesis Testing.docx,"Developed Bayes' theorem, a fundamental concept for conditional probability and updating beliefs based on new evidence."
Time Series Data,Module 13 - Pandas Time Series Complete.docx,"This refers to data that is collected and organized in chronological order, typically over a period of time. This type of data is common in finance (stock prices, economic indicators), environmental monitoring (temperature, pollution levels), and various other fields."
Time Series Data,Module 13 - Statsmodels Time Series Complete.docx,"This refers to data that is collected and organized in chronological order, typically over a period of time. This type of data is common in finance (stock prices, economic indicators), environmental monitoring (temperature, pollution levels), and various other fields."
Too Large,Module 12 - Basic Artificial Neural Net (ANN).docx,"The network might overshoot the optimal solution, leading to oscillations and instability, or even preventing convergence."
Too Small,Module 12 - Basic Artificial Neural Net (ANN).docx,The network might learn very slowly and get stuck in local minima.
Torch Hub,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch Hub provides a collection of pre-trained models readily available for use, simplifying tasks like image classification, object detection, and natural language processing."
Train and test,Module 09 - Feature Selection.docx,The model is trained on a subset of the data and tested on the remaining fold.
Train-Test Split,Module 02 - Descriptive Statistics.docx,A technique used to split a dataset into training and testing sets for machine learning model evaluation.
Trees,Module 11  - Trees.docx,"Intercept In linear regression, the intercept is the value of the dependent variable when all independent variables are zero. It can be thought of as the starting point of the regression line. Classification (Logistic Regression) A statistical method used to predict the probability of a binary outcome (yes or no, true or false). Decision Tree A decision support tool that uses a tree-like model to map decisions and their possible consequences. Decision Tree Terms Splitting Trees The process of dividing a node in a decision tree into two or more sub-nodes based on a feature, aiming to increase information gain or reduce uncertainty. Information Gain A metric used to measure the reduction in entropy (uncertainty) achieved by splitting a dataset based on a specific feature. Gini Impurity A measure of the probability of misclassifying a randomly chosen element from a set. Entropy A measure of uncertainty or disorder in a dataset. In decision trees, it is used to determine the optimal splits. Gain Ratio A modification of information gain used in decision trees to select the best feature to split on. Random Forests An ensemble learning method that combines multiple decision trees to improve prediction accuracy and reduce overfitting. Ensemble Learning A machine-learning approach where multiple models are combined to solve a problem. Hyperparameters Parameters used to control the learning process of a machine learning model, typically set before training begins. Grid Search A method for hyperparameter tuning where different combinations of hyperparameters are tested, and the combination with the best performance is selected. Cross-Validation A technique used to evaluate the performance of a machine learning model on unseen data by partitioning the data into subsets for training and testing. "
Trees,Module 11  - Trees.docx,There are several terms and concepts explained throughout the document you provided. Here is a brief definition and explanation for each
Trigrams,Module 12 - Gradient Descent and Backpropagation.docx,"Three-word sequences (e.g., ""the quick brown"")."
True Distribution (y),Module 12 - Gradient Descent and Backpropagation.docx,"Represents the actual class labels. For example, y = [0, 1] means the first sample belongs to class 0 and the second to class 1."
True Negative (TN),Module 11 - Confusion Matrix Binary.docx,An instance where the model correctly predicts the negative class.
True Positive (TP),Module 11 - Confusion Matrix Binary.docx,An instance where the model correctly predicts the positive class.
Tuning,Module 09 - Feature Selection.docx,The balance between L1 and L2 penalties is controlled by a hyperparameter (often denoted as 'alpha' or 'lambda'). Tuning this parameter is crucial for optimal performance.
Type I Error,Module 11 - Confusion Matrix Binary.docx,Incorrectly rejecting the null hypothesis when it is actually true. Equivalent to a false positive.
Type I Error,Module 04 - Hypothesis Testing.docx,Rejecting the null hypothesis when it is actually true (false positive).
Type I Error,Module 11 - Precision Recall.docx,Incorrectly rejecting the null hypothesis when it is actually true. Equivalent to a false positive.
Type II Error,Module 11 - Precision Recall.docx,Incorrectly failing to reject the null hypothesis when it is actually false. Equivalent to a false negative.
Type II Error,Module 11 - Confusion Matrix Binary.docx,Incorrectly failing to reject the null hypothesis when it is actually false. Equivalent to a false negative.
Type II Error,Module 04 - Hypothesis Testing.docx,Failing to reject the null hypothesis when it is actually false (false negative).
Underfitting,Module 11 - Cross Validation.docx,"Occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data."
Underfitting,Module 11 - Confusion Matrix Binary.docx,"Occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data."
Unigrams,Module 12 - Gradient Descent and Backpropagation.docx,"Single words (e.g., ""the"", ""quick"", ""brown"")."
Unsupervised Learning,Module 11 - Cross Validation.docx,"A type of machine learning where models learn from unlabeled data to discover patterns, structures, or relationships in the data."
Update Parameters,Module 12 - Gradient Descent and Backpropagation.docx,Update theta0 and theta1 using the following formulas:
Updating Parameters,Module 12 - Basic Artificial Neural Net (ANN).docx,"Finally, the gradients are used to update the weights and biases in a way that reduces the error. This is typically done using an optimization algorithm like gradient descent."
Use Case,Module 04 - The Tests.docx,"To check the assumption of equal variances before performing tests like ANOVA or t-tests that assume equal variances. To check the normality assumption before performing tests like t-tests or ANOVA that assume normality. To test if a dataset follows a specific distribution or if two datasets come from the same distribution, especially when normality assumptions may not be met. To test relationships between categorical variables, such as the association between gender and political affiliation or between education level and product preference. To compare the effectiveness of different treatments, the performance of different groups on a test, or the impact of different factors on a response variable. To compare groups when the data is ordinal or when the assumptions of ANOVA are violated. To analyze contingency tables with small sample sizes where the chi-square test may not be appropriate. To test hypotheses about population means when the population standard deviation is known. To compare the means of two independent groups when the assumption of equal variances is violated. To compare groups when the data is ordinal or when the assumptions of the t-test are violated."
Using the Chain Rule,Module 12 - Basic Artificial Neural Net (ANN).docx,The chain rule from calculus is used to calculate these error attributions. It essentially tells us how much a small change in a neuron's activation affects the overall error.
Validity,Module 02 - Data.docx,The accuracy of data or measurements in reflecting the true values or concepts.
Vanishing Gradients,Module 12 - Gradient Descent and Backpropagation.docx,"Deeper networks can sometimes suffer from vanishing gradients, making it difficult to train effectively. Techniques like LSTMs or GRUs can help address this issue."
Vanishing Gradients,Module 12 - Basic Artificial Neural Net (ANN).docx,"For very large positive or negative inputs, the derivative of the sigmoid function becomes very small. This can lead to the vanishing gradient problem, making it difficult to train deep networks."
Variable,Module 02 - Descriptive Statistics.docx,Any characteristic or attribute that can be measured or counted.
Variable,Module 02 - Data.docx,Any characteristic or attribute that can be measured or counted.
Variance,Module 11 - Bias Variance Tradeoff.docx,"Variance measures how much the model's predictions would change if different training data were used. A model with high variance is overly sensitive to the specifics of the training data, leading to overfitting and poor generalization to new, unseen data."
Variance,Module 01 - Introduction.docx,A statistical measure that quantifies the spread or dispersion of data points around the mean. A measure of the spread or dispersion of data points around the mean.
Versatility,Module 09 - Feature Selection.docx,It can be applied to various statistics and situations.
Versatility,Module 12 - Gradient Descent and Backpropagation.docx,"It works for any real number exponent, including positive, negative, and fractional exponents."
Vertical Asymptote,Module 12 - Gradient Descent and Backpropagation.docx,"A vertical asymptote occurs where the function approaches infinity (or negative infinity) as x approaches a specific value. For example, the function y = 1/x has a vertical asymptote at x = 0."
Violin plots,Module 05 - EDA.docx,Similar to box plots but show the density of the data at different values.
Visualization and Interpretability,Module 12 - Gradient Descent and Backpropagation.docx,"There are techniques to visualize and interpret the learned representations in hidden layers, but they can still be challenging to fully decipher."
Web Scraping,Module 02 - Data.docx,Extracting data from websites using automated tools or scripts.
Weighted Sum,Module 12 - Gradient Descent and Backpropagation.docx,The weighted sum combines the inputs with their corresponding weights to determine the overall input to the neuron. 1.0400 Combines the inputs with their corresponding weights. 1.0400 z = wx + b
Weighted Sum,Module 12 - Basic Artificial Neural Net (ANN).docx,Calculating the weighted sum of inputs and biases for each layer.
"Weights (w₁, w₂, ..., wₙ)",Module 12 - Gradient Descent and Backpropagation.docx,These are numerical values that determine the strength or importance of each input. They are analogous to the strength of the synapses in a biological neuron.
What they are,Module 12 - Gradient Descent and Backpropagation.docx,A limit describes the behavior of a function as its input approaches a specific value. A secant line is a line that intersects a curve at two points. A tangent line is a line that touches a curve at a single point and has the same slope as the curve at that point. The derivative of a function gives you a formula to find the slope of the tangent line at any point on the curve.
Whh (Hidden to Hidden),Module 12 - Gradient Descent and Backpropagation.docx,"This is a weight matrix that connects the hidden layer to itself. It captures the recurrent connections in the RNN, allowing information from previous time steps to influence the current hidden state."
Why (Hidden to Output),Module 12 - Gradient Descent and Backpropagation.docx,This is a weight matrix that connects the hidden layer to the output layer. It determines how much each hidden neuron contributes to the final prediction of the next word.
Wxh (Input to Hidden),Module 12 - Gradient Descent and Backpropagation.docx,This is a weight matrix that connects the input layer to the hidden layer. It determines how much each input word influences the activations of the neurons in the hidden layer.
X,Module 12 - Basic Artificial Neural Net (ANN).docx,Input data. Input data.
X (Input),Module 12 - Basic Artificial Neural Net (ANN).docx,Represents two different input samples. Each sample has four features.
XV. O.J. Simpson,Module 03 - Miscellaneous.docx,The Other Side of Probability
__init__,Module 12 - Gradient Descent and Backpropagation.docx,Initializes the network's weights and biases. Initializes the layers (nn.Linear) with specified input and output sizes.
``` Epoch 0,Module 12 - Gradient Descent and Backpropagation.docx,"Loss = 13.815510557964274 Epoch 100: Loss = 5.466041528718078 Epoch 200: Loss = 2.377595868318954 ... Epoch 900: Loss = 0.03156524335835228 Seed text: the quick brown Predicted words: ['fox', 'dog"
a_j^{l-1},Module 12 - Basic Artificial Neural Net (ANN).docx,Activation of the j-th neuron in layer l-1
all,Module 02 - Miscellaneous.docx,Returns True only if all values in a Series or DataFrame meet a certain condition.
any,Module 02 - Miscellaneous.docx,Returns True if at least one value in a Series or DataFrame meets a certain condition.
autograd Package,Module 12 - Basic Artificial Neural Net (ANN).docx,"PyTorch tensors are integrated with the autograd package, which enables automatic differentiation. This means PyTorch can automatically calculate gradients (derivatives) of operations performed on tensors, which is crucial for training neural networks using gradient-based optimization algorithms."
backward,Module 12 - Gradient Descent and Backpropagation.docx,"Performs the backward pass (backpropagation), calculating gradients for each parameter."
bh (Hidden Bias),Module 12 - Gradient Descent and Backpropagation.docx,"This is a bias vector for the hidden layer. It adds a constant offset to the weighted sum of inputs in the hidden layer, providing flexibility in the activation function."
bias,Module 12 - Gradient Descent and Backpropagation.docx,"Represents the bias term, which adds an offset to the weighted sum."
by (Output Bias),Module 12 - Gradient Descent and Backpropagation.docx,"This is a bias vector for the output layer. It adds a constant offset to the weighted sum of hidden activations, influencing the final prediction."
class_weight='balanced',Module 11 - Precision Recall.docx,A parameter in machine learning models that adjusts the weights of different classes to address class imbalance. It gives more weight to the minority class during training.
coef,Module 08 - Coefficients and Metrics.docx,the estimated value of the coefficient
criterion,Module 12 - Basic Artificial Neural Net (ANN).docx,Defines the loss function (Mean Squared Error) to measure the network's prediction error.
d_hidden2 = hidden_error2 * sigmoid_derivative(self.hidden_layer2),Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the gradient of the error with respect to the second hidden layer's activations.
d_output = output_error * sigmoid_derivative(output),Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the gradient of the error with respect to the output layer's activations. It uses the derivative of the sigmoid function because that was the activation function used in the forward pass.
forward,Module 12 - Gradient Descent and Backpropagation.docx,"Performs the forward pass, calculating activations and probabilities for each word in the input sequence. Defines the forward pass, including the activation function (torch.relu)."
g_t,Module 12 - Gradient Descent and Backpropagation.docx,Gradient at time step t
hidden_error2 = d_output.dot(self.weights3.T),Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates how much the error in the output layer is attributed to the activations in the second hidden layer.
hs,Module 12 - Gradient Descent and Backpropagation.docx,This is a dictionary that stores the hidden state vectors at each time step. Each hs[t] represents the activation of the hidden layer at time step t. It captures the information from the current input and the previous hidden state.
http,Module 06 - Astronomy and Statistics.docx,//www.sites.hps.cam.ac.uk/starry/logarithms.html //www.sites.hps.cam.ac.uk/starry/keplertables.html
https,Module 06 - Astronomy and Statistics.docx,"//nso.edu/for-public/eclipse-map-2024/ //eclipse2024.org/eclipse_cities/statemap.php //stellarium-web.org/ //www.solarsystemscope.com/ //earthobservatory.nasa.gov/features/OrbitsHistory //www.britannica.com/science/astronomy/History-of-astronomy //imagine.gsfc.nasa.gov/science/toolbox/timing_history.html //en.wikipedia.org/wiki/Least_squares#History //en.wikipedia.org/wiki/Tycho_Brahe //observablehq.com/@christophe-yamahata/visualizing-tycho-brahe-s-astronomical-observations-mars //library.si.edu/digital-library/book/tychonisbraheas00braha //library.si.edu/digital-library/book/tychonisbrahedan00brah //www.sciencephoto.com/media/720385/view/table-from-the-rudolphine-tables-1627- //early-astronomy.classics.lsa.umich.edu/western_rudolphine.php //www.loc.gov/item/85194777/ //www.loc.gov/resource/rbc0001.2013gen94777/?c=40&sp=3&st=gallery //telescoper.blog/2015/08/29/statistics-in-astronomy/ //en.wikipedia.org/wiki/Least_squares //www.cuemath.com/data/least-squares/ //en.wikipedia.org/wiki/System_of_linear_equations 


Row reduction, Gaussian Elimination //www.actuaries.digital/2021/03/31/gauss-least-squares-and-the-missing-planet/"
https,Module 06 - Terms.docx,//www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/ //flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55 //en.wikipedia.org/wiki/Cosine_similarity
https,Module 08 - Coefficients and Metrics.docx,//www.statisticshowto.com/endogenous-variable/ //medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a
"input_size, hidden_size1, hidden_size2, output_size",Module 12 - Basic Artificial Neural Net (ANN).docx,Define the number of neurons in each layer.
inputs,Module 12 - Gradient Descent and Backpropagation.docx,"Represents the input values to the neuron (e.g., features of a data point)."
"inputs = np.array([0.5, 0.3, 0.2])",Module 12 - Gradient Descent and Backpropagation.docx,"These are the ""amounts"" of each ingredient (input values)."
loss.backward(),Module 12 - Gradient Descent and Backpropagation.docx,Performs backpropagation to compute gradients.
lr=0.1,Module 12 - Basic Artificial Neural Net (ANN).docx,This sets the learning rate for the optimizer. The learning rate controls the step size taken in the direction of the negative gradient during each iteration of the optimization process.
m_t,Module 12 - Gradient Descent and Backpropagation.docx,First moment estimate at time step t
matplotlib.pyplot,Module 10 - GLM vs OLS.docx,A Python plotting library that produces high-quality figures in a variety of formats.
net.parameters(),Module 12 - Basic Artificial Neural Net (ANN).docx,This provides the optimizer with the parameters of your neural network (net) that need to be updated during training.
nn.Linear,Module 12 - Gradient Descent and Backpropagation.docx,Implements a fully connected layer.
nn.MSELoss,Module 12 - Gradient Descent and Backpropagation.docx,Calculates the Mean Squared Error loss.
nn.MSELoss(),Module 12 - Basic Artificial Neural Net (ANN).docx,"This creates an instance of the MSELoss class from PyTorch's nn module. MSELoss stands for Mean Squared Error Loss, a common loss function for regression tasks. It calculates the average of the squared differences between the predicted and target values."
nn.Module,Module 12 - Gradient Descent and Backpropagation.docx,Base class for all neural network modules in PyTorch.
"np.dot(X, self.weights1) + self.bias1",Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the weighted sum of the inputs and the bias for the first hidden layer. Each neuron in this layer receives a weighted sum of all the input features.
"np.dot(self.hidden_layer1, self.weights2) + self.bias2",Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the weighted sum of the activations from the first hidden layer and the bias for the second hidden layer.
"np.dot(self.hidden_layer2, self.weights3) + self.bias3",Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the weighted sum of the activations from the second hidden layer and the bias for the output layer.
optim.SGD,Module 12 - Gradient Descent and Backpropagation.docx,Implements the Stochastic Gradient Descent optimizer.
optim.SGD(...),Module 12 - Basic Artificial Neural Net (ANN).docx,"This creates an instance of the SGD class from PyTorch's optim module. SGD stands for Stochastic Gradient Descent, a widely used optimization algorithm."
optimizer,Module 12 - Basic Artificial Neural Net (ANN).docx,Defines the optimization algorithm (Stochastic Gradient Descent) to update the network's parameters and minimize the loss.
optimizer.step(),Module 12 - Gradient Descent and Backpropagation.docx,Updates model parameters based on gradients.
output,Module 12 - Basic Artificial Neural Net (ANN).docx,Output from the forward pass.
output_error = y - output,Module 12 - Basic Artificial Neural Net (ANN).docx,This calculates the difference between the predicted output (output) from the forward pass and the actual target output (y). This difference represents the error of the network.
p-value,Module 08 - Coefficients and Metrics.docx,This indicates the probability of observing the given t-value if there were no real relationship between the feature and the outcome. A small p-value (typically < 0.05) suggests that the feature is statistically significant in predicting the outcome.
ps,Module 12 - Gradient Descent and Backpropagation.docx,This is a dictionary that stores the probability distributions over the vocabulary at each time step. Each ps[t] represents the probabilities of the next word given the current input and hidden state.
r,"Module 08 - R-Squared, R, r, TSS, ESS, and RSS.docx","Pearson's correlation coefficient, measuring the linear correlation between two sets of data. It's calculated by dividing the covariance of the two variables by the product of their standard deviations. Like R, it ranges from -1 to 1, with the same interpretations."
scikit-learn,Module 10 - GLM vs OLS.docx,"A Python library that provides a wide range of algorithms for machine learning tasks, including regression, classification, clustering, and dimensionality reduction."
"self.bias1, self.bias2, self.bias3",Module 12 - Basic Artificial Neural Net (ANN).docx,"Bias vectors for each layer, initialized with zeros."
"self.weights1, self.weights2, self.weights3",Module 12 - Basic Artificial Neural Net (ANN).docx,"Weight matrices for connections between layers, initialized with random values."
sigmoid(...),Module 12 - Basic Artificial Neural Net (ANN).docx,The sigmoid activation function is applied to the weighted sum. This introduces non-linearity and produces the activations of the neurons in the first hidden layer. These activations are stored in self.hidden_layer1. The sigmoid activation function is applied again to produce the activations of the neurons in the second hidden layer. These activations are stored in self.hidden_layer2. The sigmoid activation function is applied one last time to produce the final output of the network. This output is stored in self.output_layer.
sigmoid(weighted_sum),Module 12 - Gradient Descent and Backpropagation.docx,"This is the ""baking"" process. The sigmoid function takes the weighted sum and squashes it between 0 and 1, representing the neuron's activation level."
solver='liblinear',Module 11 - Precision Recall.docx,An algorithm used for optimization in machine learning models. It's suitable for smaller datasets and binary classification problems.
statsmodels,Module 10 - GLM vs OLS.docx,"A Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration."
statsmodels,Module 13 - Pandas Time Series Complete.docx,A Python library for statistical modeling and analysis. The document uses it to apply the HP filter to the DOW Jones Industrial Average data.
std error,Module 08 - Coefficients and Metrics.docx,the basic standard error of the estimate of the coefficient
t,Module 08 - Coefficients and Metrics.docx,"the t-statistic value, how significant the coefficient is"
t-Test,Module 04 - Hypothesis Testing.docx,A statistical test used to compare the means of two groups.
t-statistic,Module 08 - Coefficients and Metrics.docx,This value helps determine the significance of your coefficients. It's calculated by dividing the coefficient by its standard error. A larger absolute t-value suggests the feature is a more significant predictor.
torch.argmax,Module 12 - Gradient Descent and Backpropagation.docx,"The torch.argmax(output) function finds the index of the highest logit, which corresponds to the predicted word. There's no need for an additional activation function here, as we're simply selecting the word with the highest score."
torch.nn,Module 12 - Gradient Descent and Backpropagation.docx,"PyTorch provides the torch.nn module, which contains a collection of pre-built layers, activation functions, loss functions, and other components for building neural networks. This saves you from writing these components from scratch in vanilla Python."
torch.nn Module,Module 12 - Basic Artificial Neural Net (ANN).docx,"PyTorch tensors are used extensively in the torch.nn module, which provides a collection of pre-built layers, activation functions, and other components for building neural networks."
torch.relu,Module 12 - Gradient Descent and Backpropagation.docx,Applies the ReLU activation function.
update_parameters,Module 12 - Gradient Descent and Backpropagation.docx,Updates the network's parameters using gradient descent.
v_t,Module 12 - Gradient Descent and Backpropagation.docx,Velocity at time step t Second moment estimate at time step t
"weighted_sum = np.dot(inputs, weights) + bias",Module 12 - Gradient Descent and Backpropagation.docx,This calculates the combined effect of the ingredients based on their amounts and importance.
weights,Module 12 - Gradient Descent and Backpropagation.docx,"Represents the weights associated with each input, determining its importance."
"weights = np.array([0.4, 0.7, 0.1])",Module 12 - Gradient Descent and Backpropagation.docx,"These are the ""importance"" of each ingredient (weights)."
xs,Module 12 - Gradient Descent and Backpropagation.docx,This is a dictionary that stores the one-hot encoded input vectors at each time step. Each xs[t] represents the input word at time step t.
y,Module 12 - Basic Artificial Neural Net (ANN).docx,Target output.
y (Target),Module 12 - Basic Artificial Neural Net (ANN).docx,"Provides the corresponding class labels for the two samples in X. The first sample belongs to class 0, and the second sample belongs to class 1."
y = x,Module 12 - Gradient Descent and Backpropagation.docx,dy/dx = 1x⁰ = 1
y = x³,Module 12 - Gradient Descent and Backpropagation.docx,dy/dx = 3x²
y = x⁵,Module 12 - Gradient Descent and Backpropagation.docx,dy/dx = 5x⁴
y = √x = x^(1/2),Module 12 - Gradient Descent and Backpropagation.docx,dy/dx = (1/2)x^(-1/2) = 1 / (2√x)
y_i,Module 12 - Basic Artificial Neural Net (ANN).docx,Target output for the i-th training example
yfinance,Module 13 - Pandas Time Series Complete.docx,"A Python library that allows you to download historical market data from Yahoo Finance. This is commonly used to get stock prices, index values, and other financial data for analysis."
ys,Module 12 - Gradient Descent and Backpropagation.docx,This is a dictionary that stores the unnormalized output vectors at each time step. Each ys[t] represents the raw output of the network before applying the softmax function.
ŷ_i,Module 12 - Basic Artificial Neural Net (ANN).docx,Predicted output for the i-th training example
Σ_i,Module 12 - Basic Artificial Neural Net (ANN).docx,Summation over all training examples
α,Module 12 - Basic Artificial Neural Net (ANN).docx,Learning rate
α,Module 12 - Gradient Descent and Backpropagation.docx,Learning rate Learning rate
β,Module 12 - Gradient Descent and Backpropagation.docx,Momentum parameter (typically between 0 and 1)
"β_1, β_2",Module 12 - Gradient Descent and Backpropagation.docx,Exponential decay rates for the first and second moments
ε,Module 12 - Gradient Descent and Backpropagation.docx,Small constant to prevent division by zero
θ_j,Module 12 - Basic Artificial Neural Net (ANN).docx,"= θ_j - α * (∂/∂θ) J(θ₀, θ₁) = θ_j - α * Σ_i [ (y_i - ŷ_i) * σ'(z_i^l) * a_j^{l-1} ] = θ_j - α * (y - ŷ) * σ'(z^l) * a_j^{l-1}"
θ_t,Module 12 - Gradient Descent and Backpropagation.docx,Parameters at time step t
σ'(z_i^l),Module 12 - Basic Artificial Neural Net (ANN).docx,Derivative of the sigmoid function applied to the weighted sum of inputs (z_i^l) for the i-th example in layer l
∇J(θ_t),Module 12 - Gradient Descent and Backpropagation.docx,Gradient of the cost function at time step t
